{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fdd0d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (0.0.240)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from langchain) (1.4.39)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from langchain) (3.8.3)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from langchain) (0.5.13)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.11 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from langchain) (0.0.14)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from langchain) (2.8.4)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from langchain) (1.24.3)\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from langchain) (1.2.4)\n",
      "Requirement already satisfied: pydantic<2,>=1 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from langchain) (1.10.12)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from langchain) (2.29.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from pydantic<2,>=1->langchain) (4.6.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.1)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (0.4.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9166d2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unstructured in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (0.8.1)\n",
      "Requirement already satisfied: chardet in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from unstructured) (4.0.0)\n",
      "Requirement already satisfied: filetype in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from unstructured) (1.2.0)\n",
      "Requirement already satisfied: lxml in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from unstructured) (4.9.2)\n",
      "Requirement already satisfied: msg-parser in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from unstructured) (1.2.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from unstructured) (3.7)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from unstructured) (3.0.10)\n",
      "Requirement already satisfied: pandas in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from unstructured) (1.5.3)\n",
      "Requirement already satisfied: pdf2image in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from unstructured) (1.16.3)\n",
      "Requirement already satisfied: pdfminer.six in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from unstructured) (20221105)\n",
      "Requirement already satisfied: pillow in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from unstructured) (9.4.0)\n",
      "Requirement already satisfied: pypandoc in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from unstructured) (1.11)\n",
      "Requirement already satisfied: python-docx in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from unstructured) (0.8.11)\n",
      "Requirement already satisfied: python-pptx in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from unstructured) (0.6.21)\n",
      "Requirement already satisfied: python-magic in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from unstructured) (0.4.27)\n",
      "Requirement already satisfied: markdown in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from unstructured) (3.4.1)\n",
      "Requirement already satisfied: requests in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from unstructured) (2.29.0)\n",
      "Requirement already satisfied: tabulate in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from unstructured) (0.8.10)\n",
      "Requirement already satisfied: xlrd in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from unstructured) (2.0.1)\n",
      "Requirement already satisfied: olefile>=0.46 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from msg-parser->unstructured) (0.46)\n",
      "Requirement already satisfied: click in c:\\users\\student.ms-02\\appdata\\roaming\\python\\python311\\site-packages (from nltk->unstructured) (8.1.6)\n",
      "Requirement already satisfied: joblib in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from nltk->unstructured) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from nltk->unstructured) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from nltk->unstructured) (4.65.0)\n",
      "Requirement already satisfied: et_xmlfile in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from openpyxl->unstructured) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from pandas->unstructured) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from pandas->unstructured) (2022.7)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from pandas->unstructured) (1.24.3)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from pdfminer.six->unstructured) (2.0.4)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from pdfminer.six->unstructured) (39.0.1)\n",
      "Requirement already satisfied: XlsxWriter>=0.5.7 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from python-pptx->unstructured) (3.1.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from requests->unstructured) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from requests->unstructured) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from requests->unstructured) (2023.7.22)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six->unstructured) (1.15.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->unstructured) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\student.ms-02\\appdata\\roaming\\python\\python311\\site-packages (from click->nltk->unstructured) (0.4.6)\n",
      "Requirement already satisfied: pycparser in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->unstructured) (2.21)\n"
     ]
    }
   ],
   "source": [
    "#For reading pdf files\n",
    "!pip install unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55dec7d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (0.27.8)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from openai) (2.29.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from openai) (3.8.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (2023.7.22)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from aiohttp->openai) (22.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from aiohttp->openai) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\student.ms-02\\appdata\\roaming\\python\\python311\\site-packages (from tqdm->openai) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852a9f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92c83bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Cython in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (3.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53467e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (0.4.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from tiktoken) (2022.7.9)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from tiktoken) (2.29.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9832044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain\n",
      "Version: 0.0.240\n",
      "Summary: Building applications with LLMs through composability\n",
      "Home-page: https://www.github.com/hwchase17/langchain\n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: C:\\Users\\Student.MS-02\\anaconda3\\Lib\\site-packages\n",
      "Requires: aiohttp, dataclasses-json, langsmith, numexpr, numpy, openapi-schema-pydantic, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4484742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                       Version\n",
      "----------------------------- ---------------\n",
      "aiobotocore                   2.4.2\n",
      "aiofiles                      22.1.0\n",
      "aiohttp                       3.8.3\n",
      "aioitertools                  0.7.1\n",
      "aiosignal                     1.2.0\n",
      "aiosqlite                     0.18.0\n",
      "alabaster                     0.7.12\n",
      "anaconda-catalogs             0.2.0\n",
      "anaconda-client               1.11.3\n",
      "anaconda-navigator            2.4.2\n",
      "anaconda-project              0.11.1\n",
      "anyio                         3.5.0\n",
      "appdirs                       1.4.4\n",
      "argon2-cffi                   21.3.0\n",
      "argon2-cffi-bindings          21.2.0\n",
      "arrow                         1.2.3\n",
      "asgiref                       3.7.2\n",
      "astroid                       2.14.2\n",
      "astropy                       5.1\n",
      "asttokens                     2.0.5\n",
      "async-timeout                 4.0.2\n",
      "atomicwrites                  1.4.0\n",
      "attrs                         22.1.0\n",
      "Automat                       20.2.0\n",
      "autopep8                      1.6.0\n",
      "Babel                         2.11.0\n",
      "backcall                      0.2.0\n",
      "backports.functools-lru-cache 1.6.4\n",
      "backports.tempfile            1.0\n",
      "backports.weakref             1.0.post1\n",
      "bcrypt                        3.2.0\n",
      "beautifulsoup4                4.12.2\n",
      "binaryornot                   0.4.4\n",
      "black                         0.0\n",
      "bleach                        4.1.0\n",
      "blinker                       1.6.2\n",
      "bokeh                         3.1.1\n",
      "boltons                       23.0.0\n",
      "boto3                         1.24.28\n",
      "botocore                      1.27.59\n",
      "Bottleneck                    1.3.5\n",
      "brotlipy                      0.7.0\n",
      "cachetools                    5.3.1\n",
      "camelot-py                    0.9.0\n",
      "certifi                       2023.7.22\n",
      "cffi                          1.15.1\n",
      "chardet                       4.0.0\n",
      "charset-normalizer            2.0.4\n",
      "click                         8.1.6\n",
      "cloudpickle                   2.2.1\n",
      "clyent                        1.2.2\n",
      "colorama                      0.4.6\n",
      "colorcet                      3.0.1\n",
      "comm                          0.1.2\n",
      "conda                         23.5.2\n",
      "conda-build                   3.25.0\n",
      "conda-content-trust           0.1.3\n",
      "conda_index                   0.2.3\n",
      "conda-libmamba-solver         23.5.0\n",
      "conda-pack                    0.6.0\n",
      "conda-package-handling        2.1.0\n",
      "conda_package_streaming       0.8.0\n",
      "conda-repo-cli                1.0.41\n",
      "conda-token                   0.4.0\n",
      "conda-verify                  3.4.2\n",
      "constantly                    15.1.0\n",
      "contourpy                     1.0.5\n",
      "cookiecutter                  1.7.3\n",
      "cryptography                  39.0.1\n",
      "cssselect                     1.1.0\n",
      "cycler                        0.11.0\n",
      "Cython                        3.0.0\n",
      "cytoolz                       0.12.0\n",
      "daal4py                       2023.1.1\n",
      "dask                          2023.6.0\n",
      "dataclasses-json              0.5.13\n",
      "datashader                    0.15.0\n",
      "datashape                     0.5.4\n",
      "debugpy                       1.5.1\n",
      "decorator                     5.1.1\n",
      "defusedxml                    0.7.1\n",
      "diff-match-patch              20200713\n",
      "dill                          0.3.6\n",
      "distributed                   2023.6.0\n",
      "distro                        1.8.0\n",
      "docstring-to-markdown         0.11\n",
      "docutils                      0.18.1\n",
      "entrypoints                   0.4\n",
      "et-xmlfile                    1.1.0\n",
      "executing                     0.8.3\n",
      "fastapi                       0.100.0\n",
      "fastjsonschema                2.16.2\n",
      "filelock                      3.9.0\n",
      "filetype                      1.2.0\n",
      "flake8                        6.0.0\n",
      "Flask                         2.3.2\n",
      "fonttools                     4.25.0\n",
      "frozenlist                    1.3.3\n",
      "fsspec                        2023.3.0\n",
      "future                        0.18.3\n",
      "gensim                        4.3.0\n",
      "glob2                         0.7\n",
      "google-api-core               2.11.1\n",
      "google-api-python-client      2.94.0\n",
      "google-auth                   2.22.0\n",
      "google-auth-httplib2          0.1.0\n",
      "google-auth-oauthlib          1.0.0\n",
      "googleapis-common-protos      1.59.1\n",
      "greenlet                      2.0.1\n",
      "h11                           0.14.0\n",
      "h5py                          3.7.0\n",
      "HeapDict                      1.0.1\n",
      "holoviews                     1.16.2\n",
      "httplib2                      0.22.0\n",
      "hvplot                        0.8.4\n",
      "hyperlink                     21.0.0\n",
      "idna                          3.4\n",
      "imagecodecs                   2021.8.26\n",
      "imageio                       2.26.0\n",
      "imagesize                     1.4.1\n",
      "imbalanced-learn              0.10.1\n",
      "importlib-metadata            6.0.0\n",
      "incremental                   21.3.0\n",
      "inflection                    0.5.1\n",
      "iniconfig                     1.1.1\n",
      "intake                        0.6.8\n",
      "intervaltree                  3.1.0\n",
      "ipykernel                     6.19.2\n",
      "ipython                       8.12.0\n",
      "ipython-genutils              0.2.0\n",
      "ipywidgets                    8.0.4\n",
      "isort                         5.9.3\n",
      "itemadapter                   0.3.0\n",
      "itemloaders                   1.0.4\n",
      "itsdangerous                  2.1.2\n",
      "jaraco.classes                3.2.1\n",
      "jedi                          0.18.1\n",
      "jellyfish                     0.9.0\n",
      "Jinja2                        3.1.2\n",
      "jinja2-time                   0.2.0\n",
      "jmespath                      0.10.0\n",
      "joblib                        1.2.0\n",
      "json5                         0.9.6\n",
      "jsonpatch                     1.32\n",
      "jsonpointer                   2.1\n",
      "jsonschema                    4.17.3\n",
      "jupyter                       1.0.0\n",
      "jupyter_client                8.1.0\n",
      "jupyter-console               6.6.3\n",
      "jupyter_core                  5.3.0\n",
      "jupyter-events                0.6.3\n",
      "jupyter_server                2.5.0\n",
      "jupyter_server_fileid         0.9.0\n",
      "jupyter_server_terminals      0.4.4\n",
      "jupyter_server_ydoc           0.8.0\n",
      "jupyter-ydoc                  0.2.4\n",
      "jupyterlab                    3.6.3\n",
      "jupyterlab-pygments           0.1.2\n",
      "jupyterlab_server             2.22.0\n",
      "jupyterlab-widgets            3.0.5\n",
      "keyring                       23.13.1\n",
      "kiwisolver                    1.4.4\n",
      "langchain                     0.0.240\n",
      "langsmith                     0.0.14\n",
      "lazy_loader                   0.2\n",
      "lazy-object-proxy             1.6.0\n",
      "libarchive-c                  2.9\n",
      "libmambapy                    1.4.1\n",
      "linkify-it-py                 2.0.0\n",
      "llvmlite                      0.40.0\n",
      "lmdb                          1.4.1\n",
      "locket                        1.0.0\n",
      "lxml                          4.9.2\n",
      "lz4                           4.3.2\n",
      "Markdown                      3.4.1\n",
      "markdown-it-py                2.2.0\n",
      "MarkupSafe                    2.1.3\n",
      "marshmallow                   3.20.1\n",
      "matplotlib                    3.7.1\n",
      "matplotlib-inline             0.1.6\n",
      "mccabe                        0.7.0\n",
      "mdit-py-plugins               0.3.0\n",
      "mdurl                         0.1.0\n",
      "menuinst                      1.4.19\n",
      "mistune                       0.8.4\n",
      "mkl-fft                       1.3.6\n",
      "mkl-random                    1.2.2\n",
      "mkl-service                   2.4.0\n",
      "more-itertools                8.12.0\n",
      "mpmath                        1.2.1\n",
      "msg-parser                    1.2.0\n",
      "msgpack                       1.0.3\n",
      "multidict                     6.0.2\n",
      "multipledispatch              0.6.0\n",
      "munkres                       1.1.4\n",
      "mypy-extensions               0.4.3\n",
      "navigator-updater             0.4.0\n",
      "nbclassic                     0.5.5\n",
      "nbclient                      0.5.13\n",
      "nbconvert                     6.5.4\n",
      "nbformat                      5.7.0\n",
      "nest-asyncio                  1.5.6\n",
      "networkx                      2.8.4\n",
      "nltk                          3.7\n",
      "notebook                      6.5.4\n",
      "notebook_shim                 0.2.2\n",
      "numba                         0.57.0\n",
      "numexpr                       2.8.4\n",
      "numpy                         1.24.3\n",
      "numpydoc                      1.5.0\n",
      "oauth2client                  4.1.3\n",
      "oauthlib                      3.2.2\n",
      "olefile                       0.46\n",
      "openai                        0.27.8\n",
      "openapi-schema-pydantic       1.2.4\n",
      "opencv-python                 4.8.0.74\n",
      "openpyxl                      3.0.10\n",
      "packaging                     23.0\n",
      "pandas                        1.5.3\n",
      "pandocfilters                 1.5.0\n",
      "panel                         1.1.0\n",
      "param                         1.13.0\n",
      "paramiko                      2.8.1\n",
      "parsel                        1.6.0\n",
      "parso                         0.8.3\n",
      "partd                         1.2.0\n",
      "pathlib                       1.0.1\n",
      "pathspec                      0.10.3\n",
      "patsy                         0.5.3\n",
      "pdf2image                     1.16.3\n",
      "pdfminer.six                  20221105\n",
      "pep8                          1.7.1\n",
      "pexpect                       4.8.0\n",
      "pickleshare                   0.7.5\n",
      "Pillow                        9.4.0\n",
      "pip                           23.2.1\n",
      "pkginfo                       1.9.6\n",
      "platformdirs                  2.5.2\n",
      "plotly                        5.9.0\n",
      "pluggy                        1.0.0\n",
      "ply                           3.11\n",
      "pooch                         1.4.0\n",
      "poyo                          0.5.0\n",
      "prometheus-client             0.14.1\n",
      "prompt-toolkit                3.0.36\n",
      "Protego                       0.1.16\n",
      "protobuf                      4.23.4\n",
      "psutil                        5.9.0\n",
      "ptyprocess                    0.7.0\n",
      "pure-eval                     0.2.2\n",
      "py-cpuinfo                    8.0.0\n",
      "pyarrow                       11.0.0\n",
      "pyasn1                        0.4.8\n",
      "pyasn1-modules                0.2.8\n",
      "pycodestyle                   2.10.0\n",
      "pycosat                       0.6.4\n",
      "pycparser                     2.21\n",
      "pyct                          0.5.0\n",
      "pycurl                        7.45.2\n",
      "pydantic                      1.10.12\n",
      "PyDispatcher                  2.0.5\n",
      "pydocstyle                    6.3.0\n",
      "PyDrive                       1.3.1\n",
      "pyerfa                        2.0.0\n",
      "pyflakes                      3.0.1\n",
      "Pygments                      2.15.1\n",
      "PyJWT                         2.4.0\n",
      "pylint                        2.16.2\n",
      "pylint-venv                   2.3.0\n",
      "pyls-spyder                   0.4.0\n",
      "PyMuPDF                       1.22.5\n",
      "PyNaCl                        1.5.0\n",
      "pyodbc                        4.0.34\n",
      "pyOpenSSL                     23.0.0\n",
      "pypandoc                      1.11\n",
      "pyparsing                     3.0.9\n",
      "PyPDF2                        3.0.1\n",
      "PyQt5                         5.15.7\n",
      "PyQt5-sip                     12.11.0\n",
      "PyQtWebEngine                 5.15.4\n",
      "pyrsistent                    0.18.0\n",
      "PySocks                       1.7.1\n",
      "pytest                        7.3.1\n",
      "python-dateutil               2.8.2\n",
      "python-docx                   0.8.11\n",
      "python-json-logger            2.0.7\n",
      "python-lsp-black              1.2.1\n",
      "python-lsp-jsonrpc            1.0.0\n",
      "python-lsp-server             1.7.2\n",
      "python-magic                  0.4.27\n",
      "python-pptx                   0.6.21\n",
      "python-slugify                5.0.2\n",
      "python-snappy                 0.6.1\n",
      "pytoolconfig                  1.2.5\n",
      "pytz                          2022.7\n",
      "pyviz-comms                   2.3.0\n",
      "PyWavelets                    1.4.1\n",
      "pywin32                       305.1\n",
      "pywin32-ctypes                0.2.0\n",
      "pywinpty                      2.0.10\n",
      "PyYAML                        6.0\n",
      "pyzmq                         25.1.0\n",
      "QDarkStyle                    3.0.2\n",
      "qstylizer                     0.2.2\n",
      "QtAwesome                     1.2.2\n",
      "qtconsole                     5.4.2\n",
      "QtPy                          2.2.0\n",
      "queuelib                      1.5.0\n",
      "regex                         2022.7.9\n",
      "requests                      2.29.0\n",
      "requests-file                 1.5.1\n",
      "requests-oauthlib             1.3.1\n",
      "requests-toolbelt             0.9.1\n",
      "rfc3339-validator             0.1.4\n",
      "rfc3986-validator             0.1.1\n",
      "rope                          1.7.0\n",
      "rsa                           4.9\n",
      "Rtree                         1.0.1\n",
      "ruamel.yaml                   0.17.21\n",
      "ruamel-yaml-conda             0.17.21\n",
      "s3fs                          2023.3.0\n",
      "s3transfer                    0.6.0\n",
      "sacremoses                    0.0.43\n",
      "scikit-image                  0.20.0\n",
      "scikit-learn                  1.2.2\n",
      "scikit-learn-intelex          20230426.121932\n",
      "scipy                         1.10.1\n",
      "Scrapy                        2.8.0\n",
      "seaborn                       0.12.2\n",
      "Send2Trash                    1.8.0\n",
      "sentencepiece                 0.1.99\n",
      "service-identity              18.1.0\n",
      "setuptools                    67.8.0\n",
      "sip                           6.6.2\n",
      "six                           1.16.0\n",
      "smart-open                    5.2.1\n",
      "sniffio                       1.2.0\n",
      "snowballstemmer               2.2.0\n",
      "sortedcontainers              2.4.0\n",
      "soupsieve                     2.4\n",
      "Sphinx                        5.0.2\n",
      "sphinxcontrib-applehelp       1.0.2\n",
      "sphinxcontrib-devhelp         1.0.2\n",
      "sphinxcontrib-htmlhelp        2.0.0\n",
      "sphinxcontrib-jsmath          1.0.1\n",
      "sphinxcontrib-qthelp          1.0.3\n",
      "sphinxcontrib-serializinghtml 1.1.5\n",
      "spyder                        5.4.3\n",
      "spyder-kernels                2.4.3\n",
      "SQLAlchemy                    1.4.39\n",
      "sqlparse                      0.4.4\n",
      "stack-data                    0.2.0\n",
      "starlette                     0.27.0\n",
      "statsmodels                   0.13.5\n",
      "sympy                         1.11.1\n",
      "tables                        3.8.0\n",
      "tabula-py                     2.7.0\n",
      "tabulate                      0.8.10\n",
      "TBB                           0.2\n",
      "tblib                         1.7.0\n",
      "tenacity                      8.2.2\n",
      "terminado                     0.17.1\n",
      "text-unidecode                1.3\n",
      "textdistance                  4.2.1\n",
      "threadpoolctl                 2.2.0\n",
      "three-merge                   0.1.1\n",
      "tifffile                      2021.7.2\n",
      "tiktoken                      0.4.0\n",
      "tinycss2                      1.2.1\n",
      "tldextract                    3.2.0\n",
      "toml                          0.10.2\n",
      "tomli                         2.0.1\n",
      "tomlkit                       0.11.1\n",
      "toolz                         0.12.0\n",
      "tornado                       6.2\n",
      "tqdm                          4.65.0\n",
      "traitlets                     5.7.1\n",
      "transformers                  2.1.1\n",
      "Twisted                       22.10.0\n",
      "twisted-iocpsupport           1.0.2\n",
      "typing_extensions             4.6.3\n",
      "typing-inspect                0.9.0\n",
      "tzdata                        2023.3\n",
      "uc-micro-py                   1.0.1\n",
      "ujson                         5.4.0\n",
      "Unidecode                     1.2.0\n",
      "unstructured                  0.8.1\n",
      "uritemplate                   4.1.1\n",
      "urllib3                       1.26.16\n",
      "uvicorn                       0.23.1\n",
      "w3lib                         1.21.0\n",
      "watchdog                      2.1.6\n",
      "wcwidth                       0.2.5\n",
      "webencodings                  0.5.1\n",
      "websocket-client              0.58.0\n",
      "Werkzeug                      2.3.6\n",
      "whatthepatch                  1.0.2\n",
      "wheel                         0.38.4\n",
      "widgetsnbextension            4.0.5\n",
      "win-inet-pton                 1.1.0\n",
      "wrapt                         1.14.1\n",
      "xarray                        2022.11.0\n",
      "xlrd                          2.0.1\n",
      "XlsxWriter                    3.1.2\n",
      "xlwings                       0.29.1\n",
      "xyzservices                   2022.9.0\n",
      "y-py                          0.5.9\n",
      "yapf                          0.31.0\n",
      "yarl                          1.8.1\n",
      "ypy-websocket                 0.8.2\n",
      "zict                          2.2.0\n",
      "zipp                          3.11.0\n",
      "zope.interface                5.4.0\n",
      "zstandard                     0.19.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f0ab977",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26625f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get your API keys from openai, you will need to create an account.\n",
    "#Here is the link to get the keys : https://platform.openai.com/billing/overview\n",
    "import os\n",
    "os.environ[\"Open_API_Key\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc4bdbc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (3.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94c4396e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        pdf_reader = PyPDF2.PdfFileReader(file)\n",
    "        text = \"\"\n",
    "        for page_num in range(pdf_reader.numPages):\n",
    "            page = pdf_reader.getPage(page_num)\n",
    "            text += page.extract_text()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce800e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_files = [\"Reference1.pdf\", \"Reference2.pdf\", \"Reference3.pdf\", \"Reference4.pdf\", \"Reference5.pdf\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5711efca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd028493",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pdf_file in pdf_files:\n",
    "    pdf_path = os.path.abspath(pdf_file)\n",
    "    text_content = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "    # Remove the '.pdf' extension from the file name to use it as the text file name\n",
    "    text_file_name = os.path.splitext(pdf_file)[0] + \".txt\"\n",
    "\n",
    "    # Write the text content to the text file\n",
    "    with open(text_file_name, \"w\", encoding=\"utf-8\") as text_file:\n",
    "        text_file.write(text_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b33043c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# List of PDF files to extract text from\n",
    "pdf_files = [\"Reference1.pdf\", \"Reference2.pdf\", \"Reference3.pdf\", \"Reference4.pdf\", \"Reference5.pdf\"]\n",
    "\n",
    "# Output directory to save the text files\n",
    "output_directory = \"output_text_files\"\n",
    "\n",
    "# Create the output directory if it does not exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Extract text from each PDF file and save it into separate text files\n",
    "for pdf_file in pdf_files:\n",
    "    pdf_path = os.path.abspath(pdf_file)\n",
    "    text_content = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "    # Remove the '.pdf' extension from the file name to use it as the text file name\n",
    "    text_file_name = os.path.splitext(pdf_file)[0] + \".txt\"\n",
    "\n",
    "    # Create the full path to the output text file\n",
    "    output_file_path = os.path.join(output_directory, text_file_name)\n",
    "\n",
    "    # Write the text content to the text file\n",
    "    with open(output_file_path, \"w\", encoding=\"utf-8\") as text_file:\n",
    "        text_file.write(text_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "450e8cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text from 'Reference1.pdf':\n",
      "A Few Brief Notes on DeepImpact, COIL, and a Conceptual\n",
      "Framework for Information Retrieval Techniques\n",
      "Jimmy Lin andXueguang Ma\n",
      "David R. Cheriton School of Computer Science\n",
      "University of Waterloo\n",
      "Abstract\n",
      "Recent developments in representational learn-\n",
      "ing for information retrieval can be organized\n",
      "in a conceptual framework that establishes two\n",
      "pairs of contrasts: sparse vs. dense representa-\n",
      "tions and unsupervised vs. learned representa-\n",
      "tions. Sparse learned representations can fur-\n",
      "ther be decomposed into expansion and term\n",
      "weighting components. This framework al-\n",
      "lows us to understand the relationship between\n",
      "recently proposed techniques such as DPR,\n",
      "ANCE, DeepCT, DeepImpact, and COIL, and\n",
      "furthermore, gaps revealed by our analysis\n",
      "point to “low hanging fruit” in terms of tech-\n",
      "niques that have yet to be explored. We present\n",
      "a novel technique dubbed “uniCOIL”, a simple\n",
      "extension of COIL that achieves to our knowl-\n",
      "edge the current state-of-the-art in sparse re-\n",
      "trieval on the popular MS MARCO passage\n",
      "ranking dataset. Our implementation using\n",
      "the Anserini IR toolkit is built on the Lucene\n",
      "search library and thus fully compatible with\n",
      "standard inverted indexes.\n",
      "1 Introduction\n",
      "We present a novel conceptual framework for un-\n",
      "derstanding recent developments in information re-\n",
      "trieval that organizes techniques along two dimen-\n",
      "sions. The ﬁrst dimension establishes the contrast\n",
      "between sparse and dense vector representations\n",
      "for queries and documents.1The second dimen-\n",
      "sion establishes the contrast between unsupervised\n",
      "and learned (supervised) representations. Figure 1\n",
      "illustrates our framework.\n",
      "Recent proposals for dense retrieval, exempliﬁed\n",
      "by DPR (Karpukhin et al., 2020) and ANCE (Xiong\n",
      "et al., 2021), but also encompassing many other\n",
      "techniques (Gao et al., 2021b; Hofstätter et al.,\n",
      "2020; Qu et al., 2021; Hofstätter et al., 2021; Lin\n",
      "1Consistent with parlance in information retrieval, we use\n",
      "“document” throughout this paper in a generic sense to refer to\n",
      "the unit of retrieved text. To be more precise, our experiments\n",
      "are in fact focused on passage retrieval.Dense Sparse\n",
      "Supervised DPR, ANCE DeepImpact, COIL\n",
      "Unsupervised LSI, LDA BM25, tf–idf\n",
      "Table 1: Our conceptual framework for organizing re-\n",
      "cent developments in information retrieval.\n",
      "et al., 2021), can be understood as learned dense\n",
      "representations for retrieval. This is formulated\n",
      "as a representational learning problem where the\n",
      "task is to learn (transformer-based) encoders that\n",
      "map queries and documents into dense ﬁxed-width\n",
      "vectors (768 dimensions is typical) in which inner\n",
      "products between queries and relevant documents\n",
      "are maximized, based on supervision signals from\n",
      "a large dataset such as the MS MARCO passage\n",
      "ranking test collection (Bajaj et al., 2018). See Lin\n",
      "et al. (2020) for a survey.\n",
      "Dense retrieval techniques are typically com-\n",
      "pared against a bag-of-words exact match ranking\n",
      "model such as BM25, which in this context can be\n",
      "understood as unsupervised sparse retrieval. Al-\n",
      "though it may be unnatural to describe BM25 in\n",
      "this way, it is technically accurate: each document\n",
      "is represented by a sparse vector where each dimen-\n",
      "sion corresponds to a unique term in the vocabulary,\n",
      "and the scoring function assigns a weight to each di-\n",
      "mension. As with dense retrieval, query–document\n",
      "scores are computed via inner products.\n",
      "What about learned sparse retrieval? The most\n",
      "prominent recent example of this in the literature\n",
      "is DeepCT (Dai and Callan, 2019), which uses\n",
      "a transformer to learn term weights based on a re-\n",
      "gression model, with the supervision signal coming\n",
      "from the MS MARCO passage ranking test collec-\n",
      "tion.2DeepCT has an interesting “quirk”: in truth,\n",
      "it only learns the term frequency (tf) component\n",
      "of term weights, but still relies on the remaining\n",
      "2Learning sparse representations is by no means a new idea.\n",
      "The earliest example we are aware of is Wilbur (2001), who\n",
      "attempted to learn global term weights using TREC data, but\n",
      "the idea likely dates back even further.arXiv:2106.14807v1  [cs.IR]  28 Jun 2021parts of the BM25 scoring function via the gen-\n",
      "eration of pseudo-documents. This approach also\n",
      "has a weakness: it only assigns weights to terms\n",
      "that are already present in the document, which\n",
      "limits retrieval to exact match. This is an impor-\n",
      "tant limitation that is addressed by the use of dense\n",
      "representations, which are capable of capturing se-\n",
      "mantic matches.\n",
      "These two issues were resolved by the recently\n",
      "proposed DeepImpact model (Mallia et al., 2021),\n",
      "which also belongs in the family of learned sparse\n",
      "representations. DeepImpact brought together two\n",
      "key ideas: the use of document expansion to iden-\n",
      "tify dimensions in the sparse vector that should\n",
      "have non-zero weights and a term weighting model\n",
      "based on a pairwise loss between relevant and non-\n",
      "relevant texts with respect to a query. Expansion\n",
      "terms were identiﬁed by doc2query–T5 (Nogueira\n",
      "and Lin, 2019), a sequence-to-sequence model for\n",
      "document expansion that predicts queries for which\n",
      "a text would be relevant. Since the DeepImpact\n",
      "scoring model directly predicts term weights that\n",
      "are then quantized, it would be more accurate to\n",
      "call these weights learned impacts, since query–\n",
      "document scores are simply the sum of weights of\n",
      "document terms that are found in the query. Calling\n",
      "these impact scores draws an explicit connection to\n",
      "a thread of research in information retrieval dating\n",
      "back two decades (Anh et al., 2001).\n",
      "The recently proposed COIL architecture (Gao\n",
      "et al., 2021a) presents an interesting case for this\n",
      "conceptual framework. Where does it belong? The\n",
      "authors themselves describe COIL as “a new ex-\n",
      "act lexical match retrieval architecture armed with\n",
      "deep LM representations”. COIL produces repre-\n",
      "sentations for each document token that are then\n",
      "directly stored in the inverted index, where the\n",
      "term frequency usually goes in an inverted list.\n",
      "Although COIL is perhaps best described as the\n",
      "intellectual descendant of ColBERT (Khattab and\n",
      "Zaharia, 2020), another way to think about it within\n",
      "our conceptual framework is that instead of assign-\n",
      "ingscalar weights to terms in a query, the “scoring”\n",
      "model assigns each term a vector “weight”. Query\n",
      "evaluation in COIL involves accumulating inner\n",
      "products instead of scalar weights.\n",
      "Our conceptual framework highlights a ﬁnal\n",
      "class of techniques: unsupervised dense represen-\n",
      "tations. While there is little work in this space of\n",
      "late, it does describe techniques such as LSI (Deer-\n",
      "wester et al., 1990; Atreya and Elkan, 2010) andLDA (Wei and Croft, 2006), which have been previ-\n",
      "ously explored. Thus, all quadrants in our proposed\n",
      "conceptual framework are populated with known\n",
      "examples from the literature.\n",
      "2 Comments and Observations\n",
      "Based on this framework, we can make a number of\n",
      "interesting observations that highlight obvious next\n",
      "steps in the development of retrieval techniques.\n",
      "We discuss as follows:\n",
      "Choice of bases. Retrieval techniques using learned\n",
      "dense representations and learned sparse represen-\n",
      "tations present an interesting contrast. Nearly all\n",
      "recent proposals take advantage of transformers, so\n",
      "that aspect of the design is not a salient difference.\n",
      "The critical contrast is the basis of the vector rep-\n",
      "resentations: In sparse approaches, the basis of the\n",
      "vector space remains ﬁxed to the corpus vocabulary,\n",
      "and thus techniques such as DeepCT, COIL, and\n",
      "DeepImpact can be understood as term weighting\n",
      "models. In dense approaches, the model is given\n",
      "the freedom to choose a new basis derived from\n",
      "transformer representations. This change in basis\n",
      "allows the encoder to represent the “meaning” of\n",
      "texts in relatively small ﬁxed-width vectors (com-\n",
      "pared to sparse vectors that may have millions of\n",
      "dimensions). This leads us to the next important\n",
      "observation:\n",
      "Expansions for sparse representation. Without\n",
      "some form of expansion, learned sparse represen-\n",
      "tations remain limited to (better) exact matching\n",
      "between queries and documents. The nature of\n",
      "sparse representations means that it is impractical\n",
      "to consider non-zero weights for allelements in\n",
      "the vector (i.e., the vocabulary space). Thus, docu-\n",
      "ment expansion serves the critical role of proposing\n",
      "a set of candidate terms that should receive non-\n",
      "zero weights; since the number of candidate terms\n",
      "is small compared to the vocabulary size, the re-\n",
      "sulting vector remains sparse. Without expansion,\n",
      "learned sparse representations cannot address the\n",
      "vocabulary mismatch problem (Furnas et al., 1987),\n",
      "because document terms not present in the query\n",
      "cannot contribute any score. For DeepImpact, this\n",
      "expansion is performed by doc2query–T5, but in\n",
      "principle we can imagine other methods also. This\n",
      "leads us to the next important observation:\n",
      "Relating DeepCT, DeepImpact, and COIL. The up-\n",
      "shot of the above analysis is that retrieval tech-\n",
      "niques based on learned sparse representations\n",
      "should be divided into an expansion model andSparse Representations MRR@10 Notes\n",
      "Term Weighting Expansion\n",
      "(1a) BM25 None 0.184 copied from (Nogueira and Lin, 2019)\n",
      "(1b) BM25 doc2query–T5 0.277 copied from (Nogueira and Lin, 2019)\n",
      "(2a) DeepCT None 0.243 copied from (Dai and Callan, 2019)\n",
      "(2b) DeepCT doc2query–T5 ? no publicly reported ﬁgure\n",
      "(2c) DeepImpact None ? no publicly reported ﬁgure\n",
      "(2d) DeepImpact doc2query–T5 0.326 copied from (Mallia et al., 2021)\n",
      "(2e) COIL-tok ( d= 32 ) None 0.341 copied from (Gao et al., 2021a)\n",
      "(2f) COIL-tok ( d= 32 ) doc2query–T5 0.361 our experiment\n",
      "(2g) uniCOIL None 0.315 our experiment\n",
      "(2h) uniCOIL doc2query–T5 0.352 our experiment\n",
      "Dense Representations MRR@10 Notes\n",
      "(3a) ColBERT 0.360 copied from (Khattab and Zaharia, 2020)\n",
      "(3b) ANCE 0.330 copied from (Xiong et al., 2021)\n",
      "(3c) DistillBERT 0.323 copied from (Hofstätter et al., 2020)\n",
      "(3d) RocketQA 0.370 copied from (Qu et al., 2021)\n",
      "(3e) TAS-B 0.347 copied from (Hofstätter et al., 2021)\n",
      "(3f) TCT-ColBERTv2 0.359 copied from (Lin et al., 2021)\n",
      "Dense–Sparse Hybrids MRR@10 Notes\n",
      "(4a) CLEAR 0.338 copied from (Gao et al., 2021b)\n",
      "(4b) COIL-full 0.355 copied from (Gao et al., 2021a)\n",
      "(4c) TCT-ColBERTv2 + BM25 (1a) 0.369 copied from (Lin et al., 2021)\n",
      "(4d) TCT-ColBERTv2 + doc2query–T5 (1b) 0.375 copied from (Lin et al., 2021)\n",
      "(4e) TCT-ColBERTv2 + DeepImpact (2d) 0.378 our experiment\n",
      "(4f) TCT-ColBERTv2 + uniCOIL (2h) 0.378 our experiment\n",
      "(4g) TCT-ColBERTv2 + COIL (2f) 0.382 our experiment\n",
      "Table 2: Results on the development queries of the MS MARCO passage ranking task.\n",
      "a term weighting model. For example, DeepCT\n",
      "performs no expansion and uses a regression-based\n",
      "scoring model. DeepImpact performs document ex-\n",
      "pansion and uses a pairwise scoring model. COIL\n",
      "performs no expansion and uses a “scoring” model\n",
      "that generates a contextualized “weight vector” (in-\n",
      "stead of a scalar weight). This breakdown suggests\n",
      "a number of obvious experiments that help us un-\n",
      "derstand the contributions of these components,\n",
      "which we report next.\n",
      "3 Experiments\n",
      "Our proposed conceptual framework can be used\n",
      "to organize results from the literature, which are\n",
      "shown in Table 2 on the development queries of\n",
      "the MS MARCO passage ranking task (Bajaj et al.,\n",
      "2018). Some of these entries represent ﬁgures di-\n",
      "rectly copied from previous papers (with references\n",
      "shown), while others are novel experimental condi-\n",
      "tions that we report.\n",
      "The ﬁrst main block of the table shows retrieval\n",
      "with sparse representations. Row (1a) shows the\n",
      "BM25 baseline, and row (1b) provides the effective-\n",
      "ness of doc2query–T5 expansion. In both cases, the\n",
      "term weights are from the BM25 scoring function,and hence unsupervised. Learned sparse retrieval\n",
      "techniques are shown in row group (2). Separat-\n",
      "ing the term weighting component from the ex-\n",
      "pansion component allows us to identify gaps in\n",
      "model conﬁgurations that would be interesting to\n",
      "explore. For example, in row (2a), DeepCT pro-\n",
      "posed a regression-based term weighting model,\n",
      "but performed no expansion. However, the term\n",
      "weighting model can be applied to expanded doc-\n",
      "uments, as in row (2b); to our knowledge, this\n",
      "conﬁguration has not been publicly reported.\n",
      "Similarly, DeepImpact combined doc2query–T5\n",
      "as an expansion model and a term weighting model\n",
      "trained with pairwise loss. To better understand\n",
      "the contributions of each component, we could\n",
      "run the term weighting model without document\n",
      "expansion, as outlined in row (2c). This ablation\n",
      "experiment was not reported in Mallia et al. (2021),\n",
      "but would be interesting to conduct.\n",
      "In row (2e) we report the published results of\n",
      "COIL-tok (token dimension d= 32 ), which is the\n",
      "sparse component in the full COIL model (which\n",
      "is a dense–sparse hybrid). Through the lens of\n",
      "our conceptual framework, a number of extensions\n",
      "become immediately obvious. COIL can be com-bined with doc2query–T5. Using source code pro-\n",
      "vided by the authors,3we trained such a model\n",
      "from scratch, using the same hyperparameters as\n",
      "the authors. This variant leads to a nearly two-point\n",
      "gain in effectiveness, as shown in row (2f).\n",
      "In another interesting extension, if we reduce the\n",
      "token dimension of COIL to one, the model degen-\n",
      "erates into producing scalar weights, which then\n",
      "becomes directly comparable to DeepCT, row (2a)\n",
      "and the “no-expansion” variant of DeepImpact, row\n",
      "(2c). These comparisons isolate the effects of differ-\n",
      "ent term weighting models. We dub this variant of\n",
      "COIL “uniCOIL”, on top of which we can also add\n",
      "doc2query–T5, which produces a fair comparison\n",
      "to DeepImpact, row (2d). The original formulation\n",
      "of COIL, even with a token dimension of one, is\n",
      "not directly amenable to retrieval using inverted\n",
      "indexes because weights can be negative. To ad-\n",
      "dress this issue, we added a ReLU operation on\n",
      "the output term weights of the base COIL model to\n",
      "force the model to generate non-negative weights.\n",
      "Once again, we retrained the model from scratch\n",
      "using the same hyperparameters provided by the\n",
      "authors. When encoding the corpus, we quantized\n",
      "these weights into 8 bits to obtain impact scores;\n",
      "query weights are similarly quantized. After these\n",
      "modiﬁcations, uniCOIL is directly compatible with\n",
      "inverted indexes. Our experimental results are re-\n",
      "ported with the Anserini toolkit (Yang et al., 2017,\n",
      "2018), which is built on Lucene.\n",
      "It is no surprise that uniCOIL without doc2query–\n",
      "T5, row (2g), is less effective than COIL-tok ( d=\n",
      "32), row (2e). However, uniCOIL with doc2query–\n",
      "T5, row (2h), outperforms COIL-tok without need-\n",
      "ing any specialized retrieval infrastructure—the\n",
      "weights are just impact scores, like in DeepImpact.\n",
      "These results suggest that contextualized “weight\n",
      "vectors” in COIL aren’t necessary to achieve good\n",
      "effectiveness—adding expansion appears sufﬁcient\n",
      "to make up for the lost expressivity of weight vec-\n",
      "tors, as shown in row (2h) vs. row (2e). To our\n",
      "knowledge, our uniCOIL model, row (2h), repre-\n",
      "sents the state of the art in sparse retrieval using\n",
      "learned impact weights, beating DeepImpact by\n",
      "around two points.\n",
      "The second main block of Table 2 provides a\n",
      "number of comparable dense retrieval results from\n",
      "the literature. The highest score that we are aware\n",
      "of is RocketQA (Qu et al., 2021), whose effective-\n",
      "ness beats all known sparse conﬁgurations. Note\n",
      "3https://github.com/luyug/COILthat ColBERT (Khattab and Zaharia, 2020) uses\n",
      "the more expressive MaxSim operator to compare\n",
      "query and document representations; all other tech-\n",
      "niques use inner products.\n",
      "The ﬁnal block of Table 2 presents the results of\n",
      "dense–sparse hybrids. Lin et al. (2021) reported\n",
      "the results of dense–sparse hybrids when TCT-\n",
      "ColBERTv2, row (3f), is combined with BM25,\n",
      "row (1a), and doc2query–T5, row (1b). To this,\n",
      "we added fusion with DeepImpact, uniCOIL, and\n",
      "COIL-tok (d= 32 ). For a fair comparison, we fol-\n",
      "lowed the same technique for combining dense and\n",
      "sparse results as Lin et al. (2021), which is from Ma\n",
      "et al. (2021). For each query q, we used the corre-\n",
      "sponding dense and sparse techniques to retrieve\n",
      "top-1k documents. The ﬁnal fusion score of each\n",
      "document is calculated by sdense +\u000b",
      "\u0001ssparse . Since\n",
      "the range of the two different scores are quite differ-\n",
      "ent, we ﬁrst normalized the scores into range(0, 1).\n",
      "The\u000b",
      "was tuned in the range(0, 2) with a simple\n",
      "line search on a subset of the MS MARCO passage\n",
      "training set.\n",
      "With these hybrid combinations, we are able\n",
      "to achieve, to our knowledge, the highest reported\n",
      "scores on the MS MARCO passage ranking task for\n",
      "single-stage techniques (i.e., no reranking). Note\n",
      "that, as before, uniCOIL is compatible with stan-\n",
      "dard inverted indexes, unlike COIL-tok, which re-\n",
      "quires custom infrastructure.\n",
      "4 Next Steps\n",
      "In most recent work, dense retrieval techniques are\n",
      "compared to BM25 and experiments show that they\n",
      "handily win. However, this is not a fair compari-\n",
      "son, since BM25 is unsupervised, whereas dense\n",
      "retrieval techniques exploit supervised relevance\n",
      "signals from large datasets. A more appropriate\n",
      "comparison would be between learned dense vs.\n",
      "sparse representations—and there, no clear win-\n",
      "ner emerges at present. However, it seems clear\n",
      "that they are complementary, as hybrid approaches\n",
      "appear to be more effective than either alone.\n",
      "An important point to make here is that neu-\n",
      "ral networks, particularly transformers, have not\n",
      "made sparse representations obsolete. Both dense\n",
      "and sparse learned representations clearly exploit\n",
      "transformers—the trick is that the latter class of\n",
      "techniques then “projects” the learned knowledge\n",
      "back into the sparse vocabulary space. This al-\n",
      "lows us to reuse decades of innovation in inverted\n",
      "indexes (e.g., integer coding techniques to com-press inverted lists) and efﬁcient query evaluation\n",
      "algorithms (e.g., smart skipping to reduce query\n",
      "latency): for example, the Lucene index used in\n",
      "our uniCOIL experiments is only 1.3 GB, com-\n",
      "pared to \u001840 GB for COIL-tok, 26 GB for TCT-\n",
      "ColBERTv2, and 154 GB for ColBERT. We note,\n",
      "however, that with dense retrieval techniques, ﬁxed-\n",
      "width vectors can be approximated with binary\n",
      "hash codes, yielding far more compact representa-\n",
      "tions with sacriﬁcing much effectiveness (Yamada\n",
      "et al., 2021). Once again, no clear winner emerges\n",
      "at present.\n",
      "The complete design space of modern informa-\n",
      "tion retrieval techniques requires proper accounting\n",
      "of the tradeoffs between output quality (effective-\n",
      "ness), time (query latency), and space (index size).\n",
      "Here, we have only focused on the ﬁrst aspect.\n",
      "Learned representations for information retrieval\n",
      "are clearly the future, but the advantages and dis-\n",
      "advantages of dense vs. sparse approaches along\n",
      "these dimensions are not yet fully understood. It’ll\n",
      "be exciting to see what comes next!\n",
      "5 Acknowledgments\n",
      "This research was supported in part by the Canada\n",
      "First Research Excellence Fund and the Natural Sci-\n",
      "ences and Engineering Research Council (NSERC)\n",
      "of Canada. Computational resources were provided\n",
      "by Compute Ontario and Compute Canada.\n",
      "References\n",
      "V o Ngoc Anh, Owen de Kretser, and Alistair Moffat.\n",
      "2001. Vector-space ranking with effective early ter-\n",
      "mination. In Proceedings of the 24th Annual Inter-\n",
      "national ACM SIGIR Conference on Research and\n",
      "Development in Information Retrieval (SIGIR 2001) ,\n",
      "pages 35–42, New Orleans, Louisiana.\n",
      "Avinash Atreya and Charles Elkan. 2010. Latent se-\n",
      "mantic indexing (LSI) fails for TREC collections.\n",
      "SIGKDD Explorations , 12(2):5–10.\n",
      "Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng,\n",
      "Jianfeng Gao, Xiaodong Liu, Rangan Majumder,\n",
      "Andrew McNamara, Bhaskar Mitra, Tri Nguyen,\n",
      "Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Ti-\n",
      "wary, and Tong Wang. 2018. MS MARCO: A Hu-\n",
      "man Generated MAchine Reading COmprehension\n",
      "Dataset. arXiv:1611.09268v3 .\n",
      "Zhuyun Dai and Jamie Callan. 2019. Context-aware\n",
      "sentence/passage term importance estimation for\n",
      "ﬁrst stage retrieval. arXiv:1910.10687 .\n",
      "Scott Deerwester, Susan T. Dumais, George W. Furnas,\n",
      "Thomas K. Landauer, and Richard Harshman. 1990.Indexing by latent semantic analysis. Journal of\n",
      "the Association for Information Science , 41(6):391–\n",
      "407.\n",
      "George W. Furnas, Thomas K. Landauer, Louis M.\n",
      "Gomez, and Susan T. Dumais. 1987. The vo-\n",
      "cabulary problem in human-system communication.\n",
      "Communications of the ACM , 30(11):964–971.\n",
      "Luyu Gao, Zhuyun Dai, and Jamie Callan. 2021a.\n",
      "COIL: Revisit exact lexical match in information\n",
      "retrieval with contextualized inverted list. In Pro-\n",
      "ceedings of the 2021 Conference of the North Amer-\n",
      "ican Chapter of the Association for Computational\n",
      "Linguistics: Human Language Technologies , pages\n",
      "3030–3042.\n",
      "Luyu Gao, Zhuyun Dai, Tongfei Chen, Zhen Fan, Ben-\n",
      "jamin Van Durme, and Jamie Callan. 2021b. Com-\n",
      "plementing lexical retrieval with semantic residual\n",
      "embedding. In Proceedings of the 43rd European\n",
      "Conference on Information Retrieval (ECIR 2021),\n",
      "Part I , pages 146–160.\n",
      "Sebastian Hofstätter, Sophia Althammer, Michael\n",
      "Schröder, Mete Sertkan, and Allan Hanbury.\n",
      "2020. Improving efﬁcient neural ranking mod-\n",
      "els with cross-architecture knowledge distillation.\n",
      "arXiv:2010.02666 .\n",
      "Sebastian Hofstätter, Sheng-Chieh Lin, Jheng-Hong\n",
      "Yang, Jimmy Lin, and Allan Hanbury. 2021. Ef-\n",
      "ﬁciently teaching an effective dense retriever with\n",
      "balanced topic aware sampling. In Proceedings of\n",
      "the 44th Annual International ACM SIGIR Confer-\n",
      "ence on Research and Development in Information\n",
      "Retrieval (SIGIR 2021) .\n",
      "Vladimir Karpukhin, Barlas O ˘guz, Sewon Min, Patrick\n",
      "Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\n",
      "Wen-tau Yih. 2020. Dense passage retrieval for\n",
      "open-domain question answering. In Proceedings of\n",
      "the 2020 Conference on Empirical Methods in Nat-\n",
      "ural Language Processing (EMNLP) , pages 6769–\n",
      "6781.\n",
      "Omar Khattab and Matei Zaharia. 2020. ColBERT: Ef-\n",
      "ﬁcient and effective passage search via contextual-\n",
      "ized late interaction over BERT. In Proceedings of\n",
      "the 43rd International ACM SIGIR Conference on\n",
      "Research and Development in Information Retrieval\n",
      "(SIGIR 2020) , pages 39–48.\n",
      "Jimmy Lin, Rodrigo Nogueira, and Andrew Yates.\n",
      "2020. Pretrained transformers for text ranking:\n",
      "BERT and beyond. arXiv:2010.06467 .\n",
      "Sheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin.\n",
      "2021. In-batch negatives for knowledge distillation\n",
      "with tightly-coupled teachers for dense retrieval. In\n",
      "Proceedings of the 6th Workshop on Representation\n",
      "Learning for NLP .\n",
      "Xueguang Ma, Kai Sun, Ronak Pradeep, and Jimmy\n",
      "Lin. 2021. A replication study of dense passage re-\n",
      "triever. arXiv:2104.05740 .Antonio Mallia, Omar Khattab, Torsten Suel, and\n",
      "Nicola Tonellotto. 2021. Learning passage impacts\n",
      "for inverted indexes. In Proceedings of the 44th An-\n",
      "nual International ACM SIGIR Conference on Re-\n",
      "search and Development in Information Retrieval\n",
      "(SIGIR 2021) .\n",
      "Rodrigo Nogueira and Jimmy Lin. 2019. From\n",
      "doc2query to docTTTTTquery.\n",
      "Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang\n",
      "Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu,\n",
      "and Haifeng Wang. 2021. RocketQA: An opti-\n",
      "mized training approach to dense passage retrieval\n",
      "for open-domain question answering. In Proceed-\n",
      "ings of the 2021 Conference of the North Ameri-\n",
      "can Chapter of the Association for Computational\n",
      "Linguistics: Human Language Technologies , pages\n",
      "5835–5847.\n",
      "Xing Wei and W. Bruce Croft. 2006. LDA-based doc-\n",
      "ument models for ad-hoc retrieval. In Proceedings\n",
      "of the 29th Annual International ACM SIGIR Con-\n",
      "ference on Research and Development in Informa-\n",
      "tion Retrieval (SIGIR 2006) , pages 178–185, Seattle,\n",
      "Washington.\n",
      "W. John Wilbur. 2001. Global term weights for docu-\n",
      "ment retrieval learned from TREC data. Journal of\n",
      "Information Science , 27(5):303–310.\n",
      "Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,\n",
      "Jialin Liu, Paul N. Bennett, Junaid Ahmed, and\n",
      "Arnold Overwijk. 2021. Approximate nearest neigh-\n",
      "bor negative contrastive learning for dense text re-\n",
      "trieval. In Proceedings of the 9th International Con-\n",
      "ference on Learning Representations (ICLR 2021) .\n",
      "Ikuya Yamada, Akari Asai, and Hannaneh Ha-\n",
      "jishirzi. 2021. Efﬁcient passage retrieval with\n",
      "hashing for open-domain question answering.\n",
      "arXiv:2106.00882 .\n",
      "Peilin Yang, Hui Fang, and Jimmy Lin. 2017. Anserini:\n",
      "enabling the use of Lucene for information retrieval\n",
      "research. In Proceedings of the 40th Annual Inter-\n",
      "national ACM SIGIR Conference on Research and\n",
      "Development in Information Retrieval (SIGIR 2017) ,\n",
      "pages 1253–1256, Tokyo, Japan.\n",
      "Peilin Yang, Hui Fang, and Jimmy Lin. 2018. Anserini:\n",
      "reproducible ranking baselines using Lucene. Jour-\n",
      "nal of Data and Information Quality , 10(4):Article\n",
      "16.\n",
      "-----------------------------------\n",
      "Extracted text from 'Reference2.pdf':\n",
      "ColBERT: Efficient and Effective Passage Search via\n",
      "Contextualized Late Interaction over BERT\n",
      "Omar Khattab\n",
      "Stanford University\n",
      "okhattab@stanford.eduMatei Zaharia\n",
      "Stanford University\n",
      "matei@cs.stanford.edu\n",
      "ABSTRACT\n",
      "Recent progress in Natural Language Understanding (NLU) is driv-\n",
      "ing fast-paced advances in Information Retrieval (IR), largely owed\n",
      "to fine-tuning deep language models (LMs) for document ranking.\n",
      "While remarkably effective, the ranking models based on these LMs\n",
      "increase computational cost by orders of magnitude over prior ap-\n",
      "proaches, particularly as they must feed each query–document pair\n",
      "through a massive neural network to compute a single relevance\n",
      "score. To tackle this, we present ColBERT, a novel ranking model\n",
      "that adapts deep LMs (in particular, BERT) for efficient retrieval.\n",
      "ColBERT introduces a late interaction architecture that indepen-\n",
      "dently encodes the query and the document using BERT and then\n",
      "employs a cheap yet powerful interaction step that models their fine-\n",
      "grained similarity. By delaying and yet retaining this fine-granular\n",
      "interaction, ColBERT can leverage the expressiveness of deep LMs\n",
      "while simultaneously gaining the ability to pre-compute document\n",
      "representations offline, considerably speeding up query processing.\n",
      "Crucially, ColBERT’s pruning-friendly interaction mechanism en-\n",
      "ables leveraging vector-similarity indexes for end-to-end retrieval\n",
      "directly from millions of documents. We extensively evaluate Col-\n",
      "BERT using two recent passage search datasets. Results show that\n",
      "ColBERT’s effectiveness is competitive with existing BERT-based\n",
      "models (and outperforms every non-BERT baseline), while exe-\n",
      "cuting two orders-of-magnitude faster and requiring up to four\n",
      "orders-of-magnitude fewer FLOPs per query.\n",
      "KEYWORDS\n",
      "Neural IR; Efficiency; Deep Language Models; BERT\n",
      "ACM Reference Format:\n",
      "Omar Khattab and Matei Zaharia. 2020. ColBERT: Efficient and Effective Pas-\n",
      "sage Search via Contextualized Late Interaction over BERT. In Proceedings of\n",
      "the 43rd International ACM SIGIR Conference on Research and Development in\n",
      "Information Retrieval (SIGIR ’20), July 25–30, 2020, Virtual Event, China. ACM,\n",
      "New York, NY, USA, 10 pages. https://doi.org/10.1145/3397271.3401075\n",
      "1 INTRODUCTION\n",
      "Over the past few years, the Information Retrieval (IR) community\n",
      "has witnessed the introduction of a host of neural ranking models,\n",
      "including DRMM [ 7], KNRM [ 4,36], and Duet [ 20,22]. In contrast\n",
      "Permission to make digital or hard copies of all or part of this work for personal or\n",
      "classroom use is granted without fee provided that copies are not made or distributed\n",
      "for profit or commercial advantage and that copies bear this notice and the full citation\n",
      "on the first page. Copyrights for components of this work owned by others than the\n",
      "author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\n",
      "republish, to post on servers or to redistribute to lists, requires prior specific permission\n",
      "and/or a fee. Request permissions from permissions@acm.org.\n",
      "SIGIR ’20, July 25–30, 2020, Virtual Event, China\n",
      "©2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.\n",
      "ACM ISBN 978-1-4503-8016-4/20/07. . . $15.00\n",
      "https://doi.org/10.1145/3397271.3401075\n",
      "0.15 0.20 0.25 0.30 0.35 0.40\n",
      "MRR@10101102103104105Query Latency (ms)\n",
      "BM25doc2queryKNRMDuet\n",
      "DeepCTfT+ConvKNRM\n",
      "docTTTTTqueryBERT-baseBERT-large\n",
      "ColBERT (re-rank)ColBERT (full retrieval)Bag-of-Words (BoW) Model\n",
      "BoW Model with NLU Augmentation\n",
      "Neural Matching Model\n",
      "Deep Language Model\n",
      "ColBERT (ours)Figure 1: Effectiveness (MRR@10) versus Mean Query La-\n",
      "tency (log-scale) for a number of representative ranking\n",
      "models on MS MARCO Ranking [24]. The figure also shows\n",
      "ColBERT. Neural re-rankers run on top of the official BM25\n",
      "top-1000 results and use a Tesla V100 GPU. Methodology and\n",
      "detailed results are in §4.\n",
      "to prior learning-to-rank methods that rely on hand-crafted fea-\n",
      "tures, these models employ embedding-based representations of\n",
      "queries and documents and directly model local interactions (i.e.,\n",
      "fine-granular relationships) between their contents. Among them,\n",
      "a recent approach has emerged that fine-tunes deep pre-trained\n",
      "language models (LMs) like ELMo [ 29] and BERT [ 5] for estimating\n",
      "relevance. By computing deeply-contextualized semantic repre-\n",
      "sentations of query–document pairs, these LMs help bridge the\n",
      "pervasive vocabulary mismatch [ 21,42] between documents and\n",
      "queries [ 30]. Indeed, in the span of just a few months, a number\n",
      "of ranking models based on BERT have achieved state-of-the-art\n",
      "results on various retrieval benchmarks [ 3,18,25,39] and have\n",
      "been proprietarily adapted for deployment by Google1and Bing2.\n",
      "However, the remarkable gains delivered by these LMs come\n",
      "at a steep increase in computational cost. Hofstätter et al. [9] and\n",
      "MacAvaney et al. [18] observe that BERT-based models in the lit-\n",
      "erature are 100-1000 ×more computationally expensive than prior\n",
      "models—some of which are arguably notinexpensive to begin with\n",
      "[13]. This quality–cost tradeoff is summarized by Figure 1, which\n",
      "compares two BERT-based rankers [ 25,27] against a representative\n",
      "set of ranking models. The figure uses MS MARCO Ranking [ 24],\n",
      "a recent collection of 9M passages and 1M queries from Bing’s\n",
      "logs. It reports retrieval effectiveness (MRR@10) on the official\n",
      "validation set as well as average query latency (log-scale) using a\n",
      "high-end server that dedicates one Tesla V100 GPU per query for\n",
      "neural re-rankers. Following the re-ranking setup of MS MARCO,\n",
      "ColBERT (re-rank), the Neural Matching Models, and the Deep LMs\n",
      "re-rank MS MARCO’s official top-1000 documents per query. Other\n",
      "1https://blog.google/products/search/search-language-understanding-bert/\n",
      "2https://azure.microsoft.com/en-us/blog/bing-delivers-its-largest-improvement-\n",
      "in-search-experience-using-azure-gpus/\n",
      "Session 1A: NeuIR and Semantic Matching \n",
      " \n",
      "SIGIR ’20, July 25–30, 2020, Virtual Event, China\n",
      "39\n",
      "Query Document\n",
      "MaxSim∑\n",
      "MaxSim MaxSims\n",
      "QueryCNN  /  Match KernelsCNN  /  Match Kernels / MLPMLPs\n",
      "Document\n",
      "(c) All-to-all Interaction\n",
      "(e.g., BERT)(b) Query-Document Interaction\n",
      "(e.g., DRMM, KNRM, Conv-KNRM)(d) Late Interaction\n",
      "(i.e., the proposed ColBERT)(a) Representation-based Similarity\n",
      "(e.g., DSSM, SNRM)Query Document\n",
      "s\n",
      "Query Document\n",
      "sFigure 2: Schematic diagrams illustrating query–document matching paradigms in neural IR. The figure contrasts existing\n",
      "approaches (sub-figures (a), (b), and (c)) with the proposed late interaction paradigm (sub-figure (d)).\n",
      "methods, including ColBERT (full retrieval), directly retrieve the\n",
      "top-1000 results from the entire collection.\n",
      "As the figure shows, BERT considerably improves search preci-\n",
      "sion, raising MRR@10 by almost 7% against the best previous meth-\n",
      "ods; simultaneously, it increases latency by up to tens of thousands\n",
      "of milliseconds even with a high-end GPU. This poses a challenging\n",
      "tradeoff since raising query response times by as little as 100ms is\n",
      "known to impact user experience and even measurably diminish\n",
      "revenue [ 17]. To tackle this problem, recent work has started explor-\n",
      "ing using Natural Language Understanding (NLU) techniques to\n",
      "augment traditional retrieval models like BM25 [ 32]. For example,\n",
      "Nogueira et al. [26,28] expand documents with NLU-generated\n",
      "queries before indexing with BM25 scores and Dai & Callan [ 2] re-\n",
      "place BM25’s term frequency with NLU-estimated term importance.\n",
      "Despite successfully reducing latency, these approaches generally\n",
      "reduce precision substantially relative to BERT.\n",
      "To reconcile efficiency and contextualization in IR, we propose\n",
      "ColBERT , a ranking model based on contextualized late interac-\n",
      "tion over BERT . As the name suggests, ColBERT proposes a novel\n",
      "late interaction paradigm for estimating relevance between a query\n",
      "𝑞and a document 𝑑. Under late interaction, 𝑞and𝑑are separately\n",
      "encoded into two sets of contextual embeddings, and relevance is\n",
      "evaluated using cheap and pruning-friendly computations between\n",
      "both sets—that is, fast computations that enable ranking without\n",
      "exhaustively evaluating every possible candidate.\n",
      "Figure 2 contrasts our proposed late interaction approach with\n",
      "existing neural matching paradigms. On the left, Figure 2 (a) illus-\n",
      "trates representation-focused rankers, which independently compute\n",
      "an embedding for 𝑞and another for 𝑑and estimate relevance as\n",
      "a single similarity score between two vectors [ 12,41]. Moving to\n",
      "the right, Figure 2 (b) visualizes typical interaction-focused rankers.\n",
      "Instead of summarizing 𝑞and𝑑into individual embeddings, these\n",
      "rankers model word- and phrase-level relationships across 𝑞and𝑑\n",
      "and match them using a deep neural network (e.g., with CNNs/MLPs\n",
      "[22] or kernels [ 36]). In the simplest case, they feed the neural net-\n",
      "work an interaction matrix that reflects the similiarity between\n",
      "every pair of words across 𝑞and𝑑. Further right, Figure 2 (c) illus-\n",
      "trates a more powerful interaction-based paradigm, which models\n",
      "the interactions between words within as well as across𝑞and𝑑at\n",
      "the same time, as in BERT’s transformer architecture [25].These increasingly expressive architectures are in tension. While\n",
      "interaction-based models (i.e., Figure 2 (b) and (c)) tend to be su-\n",
      "perior for IR tasks [ 8,21], a representation-focused model—by iso-\n",
      "lating the computations among 𝑞and𝑑—makes it possible to pre-\n",
      "compute document representations offline [ 41], greatly reducing\n",
      "the computational load per query. In this work, we observe that\n",
      "the fine-grained matching in interaction-based models and the pre-\n",
      "computation in representation-based models can be combined by\n",
      "retaining yet judiciously delaying the query–document interaction.\n",
      "Figure 2 (d) illustrates an architecture that precisely does so. As\n",
      "illustrated, every query embedding interacts with all document\n",
      "embeddings via a MaxSim operator, which computes maximum\n",
      "similarity (e.g., cosine), and the scalar outputs of these operators\n",
      "are summed across query terms. This paradigm allows ColBERT to\n",
      "exploit deep LM-based representations while shifting the cost of\n",
      "encoding documents offline and amortizing the cost of encoding\n",
      "the query once across all ranked documents. Crucially, it enables\n",
      "ColBERT to leverage vector-similarity search indexes (e.g., [ 1,15])\n",
      "to retrieve the top- 𝑘results directly from a large document collec-\n",
      "tion. This ability substantially improves recall over existing models,\n",
      "which only re-rank the output of term-based retrieval.\n",
      "As Figure 1 illustrates, ColBERT can serve queries in tens or, for\n",
      "end-to-end retrieval from millions of documents, few hundreds of\n",
      "milliseconds. For instance, when used for re-ranking as in “ColBERT\n",
      "(re-rank)”, it delivers over 170 ×speedup (and requires 14,000 ×fewer\n",
      "FLOPs) relative to existing BERT-based models [ 25,27], while be-\n",
      "ing more effective than every non-BERT baseline (§4.2 & 4.3). Col-\n",
      "BERT’s indexing—the only time it needs to feed documents through\n",
      "BERT—is also practical: it can index the MS MARCO collection of\n",
      "9M passages in about 3 hours using a single server with four GPUs\n",
      "(§4.5), retaining its effectiveness with a space footprint of as little\n",
      "as few tens of GiBs. Our ablation study (§4.4) shows that late in-\n",
      "teraction, its implementation via MaxSim operations, and crucial\n",
      "design choices within our BERT-based encoders are all essential to\n",
      "ColBERT’s effectiveness.\n",
      "Our main contributions are as follows.\n",
      "(1)We propose late interaction (§3.1) as a paradigm for efficient\n",
      "and effective neural ranking.\n",
      "(2)We present ColBERT (§3.2 & 3.3), a highly-effective model\n",
      "that employs novel BERT-based query and document en-\n",
      "coders within the late interaction paradigm.\n",
      "Session 1A: NeuIR and Semantic Matching \n",
      " \n",
      "SIGIR ’20, July 25–30, 2020, Virtual Event, China\n",
      "40(3)We show how to leverage ColBERT both for re-ranking on\n",
      "top of a term-based retrieval model (§3.5) and for searching\n",
      "a full collection using vector similarity indexes (§3.6).\n",
      "(4)We evaluate ColBERT on MS MARCO and TREC CAR, two\n",
      "recent passage search collections.\n",
      "We release our reference implementation as open source.3\n",
      "2 RELATED WORK\n",
      "Neural Matching Models. Over the past few years, IR researchers\n",
      "have introduced numerous neural architectures for ranking. In\n",
      "this work, we compare against KNRM [ 4,36], Duet [ 20,22], Con-\n",
      "vKNRM [ 4], and fastText+ConvKNRM [ 10]. KNRM proposes a dif-\n",
      "ferentiable kernel-pooling technique for extracting matching sig-\n",
      "nals from an interaction matrix, while Duet combines signals from\n",
      "exact-match-based as well as embedding-based similarities for rank-\n",
      "ing. Introduced in 2018, ConvKNRM learns to match 𝑛-grams in the\n",
      "query and the document. Lastly, fastText+ConvKNRM (abbreviated\n",
      "fT+ConvKNRM) tackles the absence of rare words from typical\n",
      "word embeddings lists by adopting sub-word token embeddings.\n",
      "In 2018, Zamani et al. [41] introduced SNRM, a representation-\n",
      "focused IR model that encodes each query and each document as\n",
      "a single, sparse high-dimensional vector of “latent terms”. By pro-\n",
      "ducing a sparse-vector representation for each document, SNRM\n",
      "is able to use a traditional IR inverted index for representing docu-\n",
      "ments, allowing fast end-to-end retrieval. Despite highly promising\n",
      "results and insights, SNRM’s effectiveness is substantially outper-\n",
      "formed by the state of the art on the datasets with which it was\n",
      "evaluated (e.g., see [ 18,38]). While SNRM employs sparsity to al-\n",
      "low using inverted indexes, we relax this assumption and compare\n",
      "a (dense) BERT-based representation-focused model against our\n",
      "late-interaction ColBERT in our ablation experiments in §4.4. For a\n",
      "detailed overview of existing neural ranking models, we refer the\n",
      "readers to two recent surveys of the literature [8, 21].\n",
      "Language Model Pretraining for IR. Recent work in NLU\n",
      "emphasizes the importance pre-training language representation\n",
      "models in an unsupervised fashion before subsequently fine-tuning\n",
      "them on downstream tasks. A notable example is BERT [ 5], a bi-\n",
      "directional transformer-based language model whose fine-tuning\n",
      "advanced the state of the art on various NLU benchmarks. Nogueira et\n",
      "al.[25], MacAvaney et al. [18], and Dai & Callan [ 3] investigate\n",
      "incorporating such LMs (mainly BERT, but also ELMo [ 29]) on dif-\n",
      "ferent ranking datasets. As illustrated in Figure 2 (c), the common\n",
      "approach (and the one adopted by Nogueira et al. on MS MARCO\n",
      "and TREC CAR) is to feed the query–document pair through BERT\n",
      "and use an MLP on top of BERT’s [CLS] output token to produce a\n",
      "relevance score. Subsequent work by Nogueira et al. [27] introduced\n",
      "duoBERT, which fine-tunes BERT to compare the relevance of a\n",
      "pair of documents given a query. Relative to their single-document\n",
      "BERT, this gives duoBERT about 1% MRR@10 advantage on MS\n",
      "MARCO while increasing the cost by at least 1.4×.\n",
      "BERT Optimizations. As discussed in §1, these rankers can be\n",
      "highly expensive in practice. Orthogonal to our approach, there are\n",
      "ongoing efforts in the NLU literature for distilling [ 14,33], compress-\n",
      "ing [ 40], and pruning [ 19] BERT. Other optimizations may trade\n",
      "quality and speed specifically for IR (e.g., re-ranking with a smaller\n",
      "3https://github.com/stanford-futuredata/ColBERT\n",
      "Query Document\n",
      "Query Encoder, fQ Document Encoder, fDMaxSim MaxSim MaxSimscore\n",
      "Offline IndexingFigure 3: The general architecture of ColBERT given a query\n",
      "𝑞and a document 𝑑.\n",
      "depth𝑘(§4.2) or truncating longer documents). While these efforts\n",
      "can be instrumental in narrowing the efficiency gap, they generally\n",
      "achieve much smaller speedups than our re-designed architecture\n",
      "for IR, due to their generic nature, and the more aggressive ones\n",
      "often come at the cost of noticeably lower quality.\n",
      "Efficient NLU-based Models. Recently, a direction emerged\n",
      "that employs expensive NLU computation offline. This includes\n",
      "doc2query [ 28] and DeepCT [ 2]. The doc2query model expands\n",
      "each document with a pre-defined number of synthetic queries, gen-\n",
      "erated by a seq2seq transformer model trained to generate queries\n",
      "given a document. It then relies on a BM25 index for retrieval from\n",
      "the (expanded) documents. DeepCT uses BERT to produce the term\n",
      "frequency component of BM25 in a context-aware manner, essen-\n",
      "tially representing a feasible realization of the term-independence\n",
      "assumption with neural networks [ 23]. Lastly, docTTTTTquery [ 26]\n",
      "is identical to doc2query except that it fine-tunes a pre-trained\n",
      "model (namely, T5 [31]) for generating the predicted queries.\n",
      "Concurrently with the drafting of this paper, Hofstätter et al. [11]\n",
      "published their Transformer-Kernel (TK) model. At a high level, TK\n",
      "improves the KNRM architecture described earlier: while KNRM\n",
      "employs kernel pooling on top of word-embedding-based inter-\n",
      "action, TK uses a Transformer [ 34] component for contextually\n",
      "encoding queries and documents before kernel pooling. TK estab-\n",
      "lishes a new state-of-the-art for non-BERT models on MS MARCO\n",
      "(Dev); however, the best non-ensemble MRR@10 it achieves is 31%\n",
      "while ColBERT reaches up to 36%. Moreover, due to indexing docu-\n",
      "ment representations offline and employing a MaxSim-based late\n",
      "interaction mechanism, ColBERT is much more scalable, enabling\n",
      "end-to-end retrieval which is not supported by TK.\n",
      "3 COLBERT\n",
      "ColBERT prescribes a simple framework for balancing the quality\n",
      "and cost of neural IR, particularly deep language models like BERT.\n",
      "As introduced earlier, delaying the query–document interaction can\n",
      "facilitate cheap neural re-ranking (i.e., through pre-computation)\n",
      "and even support practical end-to-end neural retrieval (i.e., through\n",
      "pruning via vector-similarity search). ColBERT addresses how to\n",
      "do so while still preserving the effectiveness of state-of-the-art\n",
      "models, which condition the bulk of their computations on the joint\n",
      "query–document pair.\n",
      "Session 1A: NeuIR and Semantic Matching \n",
      " \n",
      "SIGIR ’20, July 25–30, 2020, Virtual Event, China\n",
      "41Even though ColBERT’s late-interaction framework can be ap-\n",
      "plied to a wide variety of architectures (e.g., CNNs, RNNs, transform-\n",
      "ers, etc.), we choose to focus this work on bi-directional transformer-\n",
      "based encoders (i.e., BERT) owing to their state-of-the-art effective-\n",
      "ness yet very high computational cost.\n",
      "3.1 Architecture\n",
      "Figure 3 depicts the general architecture of ColBERT, which com-\n",
      "prises: (a) a query encoder 𝑓𝑄, (b) a document encoder 𝑓𝐷, and (c)\n",
      "the late interaction mechanism. Given a query 𝑞and document 𝑑,\n",
      "𝑓𝑄encodes𝑞into a bag of fixed-size embeddings 𝐸𝑞while𝑓𝐷en-\n",
      "codes𝑑into another bag 𝐸𝑑. Crucially, each embeddings in 𝐸𝑞and\n",
      "𝐸𝑑iscontextualized based on the other terms in 𝑞or𝑑, respectively.\n",
      "We describe our BERT-based encoders in §3.2.\n",
      "Using𝐸𝑞and𝐸𝑑, ColBERT computes the relevance score be-\n",
      "tween𝑞and𝑑via late interaction, which we define as a summation\n",
      "of maximum similarity (MaxSim) operators. In particular, we find\n",
      "the maximum cosine similarity of each 𝑣∈𝐸𝑞with vectors in 𝐸𝑑,\n",
      "and combine the outputs via summation. Besides cosine, we also\n",
      "evaluate squared L2 distance as a measure of vector similarity. In-\n",
      "tuitively, this interaction mechanism softly searches for each query\n",
      "term𝑡𝑞—in a manner that reflects its context in the query—against\n",
      "the document’s embeddings, quantifying the strength of the “match”\n",
      "via the largest similarity score between 𝑡𝑞and a document term 𝑡𝑑.\n",
      "Given these term scores, it then estimates the document relevance\n",
      "by summing the matching evidence across all query terms.\n",
      "While more sophisticated matching is possible with other choices\n",
      "such as deep convolution and attention layers (i.e., as in typical\n",
      "interaction-focused models), a summation of maximum similarity\n",
      "computations has two distinctive characteristics. First, it stands\n",
      "out as a particularly cheap interaction mechanism, as we examine\n",
      "its FLOPs in §4.2. Second, and more importantly, it is amenable\n",
      "to highly-efficient pruning for top- 𝑘retrieval, as we evaluate in\n",
      "§4.3. This enables using vector-similarity algorithms for skipping\n",
      "documents without materializing the full interaction matrix or even\n",
      "considering each document in isolation. Other cheap choices (e.g.,\n",
      "a summation of average similarity scores, instead of maximum) are\n",
      "possible; however, many are less amenable to pruning. In §4.4, we\n",
      "conduct an extensive ablation study that empirically verifies the ad-\n",
      "vantage of our MaxSim-based late interaction against alternatives.\n",
      "3.2 Query & Document Encoders\n",
      "Prior to late interaction, ColBERT encodes each query or document\n",
      "into a bag of embeddings, employing BERT-based encoders. We\n",
      "share a single BERT model among our query and document en-\n",
      "coders but distinguish input sequences that correspond to queries\n",
      "and documents by prepending a special token [Q]to queries and\n",
      "another token [D]to documents.\n",
      "Query Encoder. Given a textual query 𝑞, we tokenize it into its\n",
      "BERT-based WordPiece [ 35] tokens𝑞1𝑞2...𝑞𝑙. We prepend the token\n",
      "[Q]to the query. We place this token right after BERT’s sequence-\n",
      "start token [CLS] . If the query has fewer than a pre-defined number\n",
      "of tokens𝑁𝑞, we pad it with BERT’s special [mask] tokens up\n",
      "to length𝑁𝑞(otherwise, we truncate it to the first 𝑁𝑞tokens).\n",
      "This padded sequence of input tokens is then passed into BERT’sdeep transformer architecture, which computes a contextualized\n",
      "representation of each token.\n",
      "We denote the padding with masked tokens as query augmen-\n",
      "tation , a step that allows BERT to produce query-based embeddings\n",
      "at the positions corresponding to these masks. Query augmentation\n",
      "is intended to serve as a soft, differentiable mechanism for learning\n",
      "to expand queries with new terms or to re-weigh existing terms\n",
      "based on their importance for matching the query. As we show in\n",
      "§4.4, this operation is essential for ColBERT’s effectiveness.\n",
      "Given BERT’s representation of each token, our encoder passes\n",
      "the contextualized output representations through a linear layer\n",
      "with no activations. This layer serves to control the dimension of\n",
      "ColBERT’s embeddings, producing 𝑚-dimensional embeddings for\n",
      "the layer’s output size 𝑚. As we discuss later, we typically fix 𝑚to\n",
      "be much smaller than BERT’s fixed hidden dimension.\n",
      "While ColBERT’s embedding dimension has limited impact on\n",
      "the efficiency of query encoding, this step is crucial for controlling\n",
      "the space footprint of documents, as we show in §4.5. In addition,\n",
      "it can have a significant impact on query execution time, particu-\n",
      "larly the time taken for transferring the document representations\n",
      "onto the GPU from system memory (where they reside before pro-\n",
      "cessing a query). In fact, as we show in §4.2, gathering, stacking,\n",
      "and transferring the embeddings from CPU to GPU can be the\n",
      "most expensive step in re-ranking with ColBERT. Finally, the out-\n",
      "put embeddings are normalized so each has L2 norm equal to one.\n",
      "The result is that the dot-product of any two embeddings becomes\n",
      "equivalent to their cosine similarity, falling in the [−1,1]range.\n",
      "Document Encoder. Our document encoder has a very similar\n",
      "architecture. We first segment a document 𝑑into its constituent\n",
      "tokens𝑑1𝑑2...𝑑𝑚, to which we prepend BERT’s start token [CLS] fol-\n",
      "lowed by our special token [D]that indicates a document sequence.\n",
      "Unlike queries, we do not append [mask] tokens to documents. Af-\n",
      "ter passing this input sequence through BERT and the subsequent\n",
      "linear layer, the document encoder filters out the embeddings corre-\n",
      "sponding to punctuation symbols, determined via a pre-defined list.\n",
      "This filtering is meant to reduce the number of embeddings per doc-\n",
      "ument, as we hypothesize that (even contextualized) embeddings\n",
      "of punctuation are unnecessary for effectiveness.\n",
      "In summary, given 𝑞=𝑞0𝑞1...𝑞𝑙and𝑑=𝑑0𝑑1...𝑑𝑛, we compute\n",
      "the bags of embeddings 𝐸𝑞and𝐸𝑑in the following manner, where\n",
      "#refers to the [mask] tokens:\n",
      "𝐸𝑞:=Normalize(CNN(BERT(“[𝑄]𝑞0𝑞1...𝑞𝑙##...#”))) (1)\n",
      "𝐸𝑑:=Filter(Normalize(CNN(BERT(“[𝐷]𝑑0𝑑1...𝑑𝑛”)))) (2)\n",
      "3.3 Late Interaction\n",
      "Given the representation of a query 𝑞and a document 𝑑, the rele-\n",
      "vance score of 𝑑to𝑞, denoted as𝑆𝑞,𝑑, is estimated via late interaction\n",
      "between their bags of contextualized embeddings. As mentioned\n",
      "before, this is conducted as a sum of maximum similarity computa-\n",
      "tions, namely cosine similarity (implemented as dot-products due\n",
      "to the embedding normalization) or squared L2 distance.\n",
      "𝑆𝑞,𝑑:=Õ\n",
      "𝑖∈[|𝐸𝑞|]max\n",
      "𝑗∈[|𝐸𝑑|]𝐸𝑞𝑖·𝐸𝑇\n",
      "𝑑𝑗(3)\n",
      "Session 1A: NeuIR and Semantic Matching \n",
      " \n",
      "SIGIR ’20, July 25–30, 2020, Virtual Event, China\n",
      "42ColBERT is differentiable end-to-end. We fine-tune the BERT\n",
      "encoders and train from scratch the additional parameters (i.e., the\n",
      "linear layer and the [Q] and [D] markers’ embeddings) using the\n",
      "Adam [ 16] optimizer. Notice that our interaction mechanism has\n",
      "no trainable parameters. Given a triple ⟨𝑞,𝑑+,𝑑−⟩with query 𝑞,\n",
      "positive document 𝑑+and negative document 𝑑−, ColBERT is used\n",
      "to produce a score for each document individually and is optimized\n",
      "via pairwise softmax cross-entropy loss over the computed scores\n",
      "of𝑑+and𝑑−.\n",
      "3.4 Offline Indexing: Computing & Storing\n",
      "Document Embeddings\n",
      "By design, ColBERT isolates almost all of the computations be-\n",
      "tween queries and documents to enable pre-computing document\n",
      "representations offline. At a high level, our indexing procedure is\n",
      "straight-forward: we proceed over the documents in the collection\n",
      "in batches, running our document encoder 𝑓𝐷on each batch and\n",
      "storing the output embeddings per document. Although indexing a\n",
      "set of documents is an offline process, we incorporate a few simple\n",
      "optimizations for enhancing its throughput. As we show in §4.5,\n",
      "these can considerably reduce the offline cost of indexing.\n",
      "To begin with, we exploit multiple GPUs, if available, for faster\n",
      "encoding of batches of documents in parallel. When batching, we\n",
      "pad all documents to the maximum length of a document within\n",
      "the batch.4To make capping the sequence length on a per-batch\n",
      "basis effective, our indexer proceeds through documents in large\n",
      "groups of𝐵(e.g.,𝐵=100,000) documents. It sorts these documents\n",
      "by length and then feeds batches of 𝑏(e.g.,𝑏=128) documents of\n",
      "comparable length through our encoder. Such length-based bucket-\n",
      "ing is sometimes refered to as a BucketIterator in some libraries\n",
      "(e.g., allenNLP). Lastly, while most computations occur on the GPU,\n",
      "we found that a non-trivial portion of the indexing time is spent on\n",
      "pre-processing the text sequences, primarily BERT’s WordPiece to-\n",
      "kenization. Exploiting that these operations are independent across\n",
      "documents in a batch, we parallelize the pre-processing across the\n",
      "available CPU cores.\n",
      "Once the document representations are produced, they are saved\n",
      "to disk using 32-bit or 16-bit values to represent each dimension.\n",
      "As we describe in §3.5 and 3.6, these representations are either\n",
      "simply loaded from disk for ranking or are subsequently indexed\n",
      "for vector-similarity search, respectively.\n",
      "3.5 Top-𝑘Re-ranking with ColBERT\n",
      "Recall that ColBERT can be used for re-ranking the output of an-\n",
      "other retrieval model, typically a term-based model, or directly\n",
      "for end-to-end retrieval from a document collection. In this sec-\n",
      "tion, we discuss how we use ColBERT for ranking a small set of\n",
      "𝑘(e.g.,𝑘=1000) documents given a query 𝑞. Since𝑘is small, we\n",
      "rely on batch computations to exhaustively score each document\n",
      "(unlike our approach in §3.6). To begin with, our query serving sub-\n",
      "system loads the indexed documents representations into memory,\n",
      "representing each document as a matrix of embeddings.\n",
      "Given a query 𝑞, we compute its bag of contextualized embed-\n",
      "dings𝐸𝑞(Equation 1) and, concurrently, gather the document repre-\n",
      "sentations into a 3-dimensional tensor 𝐷consisting of 𝑘document\n",
      "4The public BERT implementations we saw simply pad to a pre-defined length.matrices. We pad the 𝑘documents to their maximum length to\n",
      "facilitate batched operations, and move the tensor 𝐷to the GPU’s\n",
      "memory. On the GPU, we compute a batch dot-product of 𝐸𝑞and\n",
      "𝐷, possibly over multiple mini-batches. The output materializes a\n",
      "3-dimensional tensor that is a collection of cross-match matrices\n",
      "between𝑞and each document. To compute the score of each docu-\n",
      "ment, we reduce its matrix across document terms via a max-pool\n",
      "(i.e., representing an exhaustive implementation of our MaxSim\n",
      "computation) and reduce across query terms via a summation. Fi-\n",
      "nally, we sort the 𝑘documents by their total scores.\n",
      "Relative to existing neural rankers (especially, but not exclusively,\n",
      "BERT-based ones), this computation is very cheap that, in fact, the\n",
      "cost of a simple implementation is dominated by the gathering\n",
      "and transferring of the pre-computed embeddings. To illustrate,\n",
      "ranking𝑘documents via typical BERT rankers requires feeding\n",
      "BERT𝑘different inputs each of length 𝑙=|𝑞|+|𝑑𝑖|for query𝑞and\n",
      "documents𝑑𝑖, where attention has quadratic cost in the length of\n",
      "the sequence. In contrast, ColBERT feeds BERT only a single, much\n",
      "shorter sequence of length 𝑙=|𝑞|. Consequently, ColBERT is not\n",
      "only cheaper, it also scales much better with 𝑘(§4.2).\n",
      "3.6 End-to-end Top- 𝑘Retrieval with ColBERT\n",
      "As mentioned before, ColBERT’s late-interaction operator is specifi-\n",
      "cally designed to enable end-to-end retrieval from a large collection,\n",
      "largely to improve recall relative to term-based retrieval approaches.\n",
      "This section is concerned with cases where the number of docu-\n",
      "ments to be ranked is too large for exhaustive evaluation of each\n",
      "possible candidate document, particularly when we are only in-\n",
      "terested in the highest scoring ones. Concretely, we focus here on\n",
      "retrieving the top- 𝑘results directly from a large document collec-\n",
      "tion with𝑁(e.g.,𝑁=10,000,000) documents, where 𝑘≪𝑁.\n",
      "To do so, we leverage the pruning-friendly nature of the MaxSim\n",
      "operations at the backbone of late interaction. Instead of applying\n",
      "MaxSim between one of the query embeddings and all of one docu-\n",
      "ment’s embeddings, we can use fast vector-similarity data structures\n",
      "to efficiently conduct this search between the query embedding\n",
      "andalldocument embeddings across the full collection. For this,\n",
      "we employ an off-the-shelf library for large-scale vector-similarity\n",
      "search, namely faiss [15] from Facebook.5In particular, at the\n",
      "end of offline indexing (§3.4), we maintain a mapping from each\n",
      "embedding to its document of origin and then index all document\n",
      "embeddings into faiss.\n",
      "Subsequently, when serving queries, we use a two-stage pro-\n",
      "cedure to retrieve the top- 𝑘documents from the entire collection.\n",
      "Both stages rely on ColBERT’s scoring: the first is an approximate\n",
      "stage aimed at filtering while the second is a refinement stage. For\n",
      "the first stage, we concurrently issue 𝑁𝑞vector-similarity queries\n",
      "(corresponding to each of the embeddings in 𝐸𝑞) onto our faiss in-\n",
      "dex. This retrieves the top- 𝑘′(e.g.,𝑘′=𝑘/2) matches for that vector\n",
      "over all document embeddings. We map each of those to its docu-\n",
      "ment of origin, producing 𝑁𝑞×𝑘′document IDs, only 𝐾≤𝑁𝑞×𝑘′\n",
      "of which are unique. These 𝐾documents likely contain one or more\n",
      "embeddings that are highly similar to the query embeddings. For\n",
      "the second stage, we refine this set by exhaustively re-ranking only\n",
      "those𝐾documents in the usual manner described in §3.5.\n",
      "5https://github.com/facebookresearch/faiss\n",
      "Session 1A: NeuIR and Semantic Matching \n",
      " \n",
      "SIGIR ’20, July 25–30, 2020, Virtual Event, China\n",
      "43In our faiss -based implementation, we use an IVFPQ index\n",
      "(“inverted file with product quantization”). This index partitions\n",
      "the embedding space into 𝑃(e.g.,𝑃=1000) cells based on 𝑘-means\n",
      "clustering and then assigns each document embedding to its nearest\n",
      "cell based on the selected vector-similarity metric. For serving\n",
      "queries, when searching for the top- 𝑘′matches for a single query\n",
      "embedding, only the nearest 𝑝(e.g.,𝑝=10) partitions are searched.\n",
      "To improve memory efficiency, every embedding is divided into 𝑠\n",
      "(e.g.,𝑠=16) sub-vectors, each represented using one byte. Moreover,\n",
      "the index conducts the similarity computations in this compressed\n",
      "domain, leading to cheaper computations and thus faster search.\n",
      "4 EXPERIMENTAL EVALUATION\n",
      "We now turn our attention to empirically testing ColBERT, address-\n",
      "ing the following research questions.\n",
      "RQ1: In a typical re-ranking setup, how well can ColBERT bridge\n",
      "the existing gap (highlighted in §1) between highly-efficient and\n",
      "highly-effective neural models? (§4.2)\n",
      "RQ2: Beyond re-ranking, can ColBERT effectively support end-\n",
      "to-end retrieval directly from a large collection? (§4.3)\n",
      "RQ3: What does each component of ColBERT (e.g., late interac-\n",
      "tion, query augmentation) contribute to its quality? (§4.4)\n",
      "RQ4: What are ColBERT’s indexing-related costs in terms of\n",
      "offline computation and memory overhead? (§4.5)\n",
      "4.1 Methodology\n",
      "4.1.1 Datasets & Metrics. Similar to related work [ 2,27,28], we\n",
      "conduct our experiments on the MS MARCO Ranking [ 24] (hence-\n",
      "forth, MS MARCO) and TREC Complex Answer Retrieval (TREC-\n",
      "CAR) [ 6] datasets. Both of these recent datasets provide large train-\n",
      "ing data of the scale that facilitates training and evaluating deep\n",
      "neural networks. We describe both in detail below.\n",
      "MS MARCO. MS MARCO is a dataset (and a corresponding\n",
      "competition) introduced by Microsoft in 2016 for reading compre-\n",
      "hension and adapted in 2018 for retrieval. It is a collection of 8.8M\n",
      "passages from Web pages, which were gathered from Bing’s re-\n",
      "sults to 1M real-world queries. Each query is associated with sparse\n",
      "relevance judgements of one (or very few) documents marked as\n",
      "relevant and no documents explicitly indicated as irrelevant. Per\n",
      "the official evaluation, we use MRR@10 to measure effectiveness.\n",
      "We use three query sets in our evaluation. The official develop-\n",
      "ment and evaluation sets contain roughly 7k queries. The relevance\n",
      "judgements of the evaluation set are held-out by Microsoft and\n",
      "MRR@10 results can only be obtained by submitting to the com-\n",
      "petition’s organizers. We submitted our main re-ranking ColBERT\n",
      "model for §4.2. In addition, the collection includes roughly 55k\n",
      "queries (with labels) that are provided as additional validation data.\n",
      "We re-purpose a random sample of 5k queries among those (i.e.,\n",
      "ones not in our development or training sets) as a “local” evaluation\n",
      "set. Along with the official development set, we use this held-out\n",
      "set for testing our models as well as baselines in §4.3. We do so to\n",
      "avoid submitting multiple variants of the same model at once, as\n",
      "the organizers discourage too many submissions by the same team.\n",
      "TREC CAR. Introduced by Dietz [ 6]et al. in 2017, TREC CAR is\n",
      "a synthetic dataset based on Wikipedia that consists of about 29M\n",
      "passages. Similar to related work [ 25], we dedicate the first fourof five pre-defined folds for training (and the fifth for validation),\n",
      "which amounts to roughly 3M queries generated by concatenating\n",
      "the title of a Wikipedia page with the heading of one of its sections.\n",
      "That section’s passages are marked as relevant to the corresponding\n",
      "query. Our evaluation is conducted on the test set used in TREC\n",
      "2017 CAR, which contains 2,254 queries.\n",
      "4.1.2 Implementation. Our ColBERT models are implemented us-\n",
      "ing Python 3 and PyTorch 1. We use the popular transformers6\n",
      "library for pre-trained BERT. Similar to [ 25], we fine-tune all Col-\n",
      "BERT models with learning rate 3×10−6with a batch size 32. We fix\n",
      "the number of embeddings per query at 𝑁𝑞=32. Unless otherwise\n",
      "stated, we set our ColBERT embedding dimension 𝑚to 128; §4.5\n",
      "demonstrates ColBERT’s robustness to a wide range of dimensions.\n",
      "For MS MARCO, we initialize the BERT components of the Col-\n",
      "BERT query and document encoders using Google’s official pre-\n",
      "trained BERT basemodel and train all models for 200k iterations.\n",
      "For TREC CAR, we follow related work [ 2,25] and use a different\n",
      "pre-trained model to the official ones. To explain, the official BERT\n",
      "models were pre-trained on Wikipedia, which is the source of TREC\n",
      "CAR’s training and test sets. To avoid leaking test data into train,\n",
      "Nogueira and Cho [ 25] pre-train a BERT model on the Wiki pages\n",
      "corresponding to training subset of TREC CAR. They release their\n",
      "BERT large pre-trained model, which we fine-tune for ColBERT’s ex-\n",
      "periments on TREC CAR. As BERT large embeddings are larger, we\n",
      "set𝑚to 200, and since fine-tuning this model is significantly slower\n",
      "than BERT base, we train on TREC CAR for only 125k iterations.\n",
      "In our re-ranking results, unless otherwise stated, we use 4 bytes\n",
      "per dimension in our embeddings and employ cosine as our vector-\n",
      "similarity function. For end-to-end ranking, we use (squared) L2\n",
      "distance, as we found our faiss index was faster at L2-based re-\n",
      "trieval. For our faiss index, we set the number of partitions to\n",
      "𝑃=2,000, and search the nearest 𝑝=10to each query embedding to\n",
      "retrieve𝑘′=𝑘=1000 document vectors per query embedding. We\n",
      "divide each embedding into 𝑠=16sub-vectors, each encoded using\n",
      "one byte. To represent the index used for the second stage of our\n",
      "end-to-end retrieval procedure, we use 16-bit values per dimension.\n",
      "4.1.3 Hardware & Time Measurements. To evaluate the latency of\n",
      "neural re-ranking models in §4.2, we use a single Tesla V100 GPU\n",
      "that has 32 GiBs of memory on a server with two Intel Xeon Gold\n",
      "6132 CPUs, each with 14 physical cores (24 hyperthreads), and 469\n",
      "GiBs of RAM. For the mostly CPU-based retrieval experiments in\n",
      "§4.3 and the indexing experiments in §4.5, we use another server\n",
      "with the same CPU and system memory specifications but which\n",
      "has four Titan V GPUs attached, each with 12 GiBs of memory.\n",
      "Across all experiments, only one GPU is dedicated per query for\n",
      "retrieval (i.e., for methods with neural computations) but we use\n",
      "up to all four GPUs during indexing.\n",
      "4.2 Quality–Cost Tradeoff: Top- 𝑘Re-ranking\n",
      "In this section, we examine ColBERT’s efficiency and effectiveness\n",
      "at re-ranking the top- 𝑘results extracted by a bag-of-words retrieval\n",
      "model, which is the most typical setting for testing and deploying\n",
      "neural ranking models. We begin with the MS MARCO dataset. We\n",
      "6https://github.com/huggingface/transformers\n",
      "Session 1A: NeuIR and Semantic Matching \n",
      " \n",
      "SIGIR ’20, July 25–30, 2020, Virtual Event, China\n",
      "44Method MRR@10 (Dev) MRR@10 (Eval) Re-ranking Latency (ms) FLOPs/query\n",
      "BM25 (official) 16.7 16.5 - -\n",
      "KNRM 19.8 19.8 3 592M (0.085×)\n",
      "Duet 24.3 24.5 22 159B (23×)\n",
      "fastText+ConvKNRM 29.0 27.7 28 78B (11×)\n",
      "BERT base[25] 34.7 - 10,700 97T (13,900×)\n",
      "BERT base(our training) 36.0 - 10,700 97T (13,900×)\n",
      "BERT large [25] 36.5 35.9 32,900 340T (48,600×)\n",
      "ColBERT (over BERT base) 34.9 34.9 61 7B (1×)\n",
      "Table 1: “Re-ranking” results on MS MARCO. Each neural model re-ranks the official top-1000 results produced by BM25.\n",
      "Latency is reported for re-ranking only. To obtain the end-to-end latency in Figure 1, we add the BM25 latency from Table 2.\n",
      "Method MRR@10 (Dev) MRR@10 (Local Eval) Latency (ms) Recall@50 Recall@200 Recall@1000\n",
      "BM25 (official) 16.7 - - - - 81.4\n",
      "BM25 (Anserini) 18.7 19.5 62 59.2 73.8 85.7\n",
      "doc2query 21.5 22.8 85 64.4 77.9 89.1\n",
      "DeepCT 24.3 - 62(est.) 69 [2] 82 [2] 91 [2]\n",
      "docTTTTTquery 27.7 28.4 87 75.6 86.9 94.7\n",
      "ColBERT L2(re-rank) 34.8 36.4 - 75.3 80.5 81.4\n",
      "ColBERT L2(end-to-end) 36.0 36.7 458 82.9 92.3 96.8\n",
      "Table 2: End-to-end retrieval results on MS MARCO. Each model retrieves the top-1000 documents per query directly from the\n",
      "entire 8.8M document collection.\n",
      "compare against KNRM, Duet, and fastText+ConvKNRM, a repre-\n",
      "sentative set of neural matching models that have been previously\n",
      "tested on MS MARCO. In addition, we compare against the adapta-\n",
      "tion of BERT for ranking by Nogueira and Cho [ 25], in particular,\n",
      "their BERT baseand its deeper counterpart BERT large.7\n",
      "We report the competition’s official metric, namely MRR@10, on\n",
      "the validation set (Dev) and the evaluation set (Eval). We also report\n",
      "the re-ranking latency, which we measure using a single Tesla V100\n",
      "GPU, and the FLOPs per query for each neural ranking model. To do\n",
      "so, we adapt the baselines’ publicly-available reference implemen-\n",
      "tations into our pytorch testbed. For ColBERT, our reported latency\n",
      "subsumes the entire computation from gathering the document\n",
      "representations, moving them to the GPU, tokenizing then encod-\n",
      "ing the query, and applying late interaction to compute document\n",
      "scores. For the baselines, we measure the scoring computations on\n",
      "the GPU and exclude the CPU-based text preprocessing (similar\n",
      "to [9]). In principle, the baselines can pre-compute most of this\n",
      "preprocessing (e.g., document tokenization) offline. We estimate\n",
      "the FLOPs using the torchprofile8library.\n",
      "We now proceed to study the results, which are reported in Ta-\n",
      "ble 1. To begin with, we notice the fast progress from KNRM in\n",
      "2017 to the BERT-based models in 2019, manifesting itself in over\n",
      "16% increase in MRR@10. As described in §1, the simultaneous\n",
      "increase in computational cost is difficult to miss. Judging by their\n",
      "rather monotonic pattern of increasingly larger cost and higher ef-\n",
      "fectiveness, these results appear to paint a picture where expensive\n",
      "models are necessary for high-quality ranking.\n",
      "In contrast with this trend, ColBERT (which employs late in-\n",
      "teraction over BERT base) performs competitively with the original\n",
      "7https://github.com/nyu-dl/dl4marco-bert/\n",
      "8https://github.com/mit-han-lab/torchprofileadaptation of BERT baseand BERT large for ranking by Nogueira\n",
      "and Cho [ 25,27]. Interestingly, ColBERT appears no worse than\n",
      "BERT basein MRR@10—although the latter uses a different loss func-\n",
      "tion to ColBERT’s (§3.3). To confirm the intuition that ColBERT’s\n",
      "late interaction does trade away some of BERT’s quality, the table\n",
      "also reports results of “BERT base(our training)”, which is based on\n",
      "Nogueira and Cho’s model of the same size but is optimized with\n",
      "pairwise softmax cross-entropy loss. We train it with learning rate\n",
      "3×10−6and batch size 16 for 200k iterations. Unlike the original\n",
      "BERT baseranker, results show that this model does in fact have an\n",
      "edge over ColBERT’s effectiveness.\n",
      "While highly competitive in retrieval quality, ColBERT is orders\n",
      "of magnitude cheaper than BERT base, in particular, by over 170 ×in\n",
      "latency and 13,900×in FLOPs. This highlights the expressiveness\n",
      "of our proposed late interaction mechanism when coupled with a\n",
      "powerful pre-trained LM like BERT. While ColBERT’s re-ranking\n",
      "latency is slightly higher than the non-BERT models shown (i.e., by\n",
      "10s of milliseconds), this difference is explained by the time it takes\n",
      "a simple Python implementation to gather, stack, and transfer the\n",
      "document embeddings to the GPU. In particular, the query encoding\n",
      "and interaction in ColBERT consume only 13 milliseconds of its\n",
      "total execution time.\n",
      "Diving deeper into the quality–cost tradeoff between BERT and\n",
      "ColBERT, Figure 4 demonstrates the relationships between FLOPs\n",
      "and effectiveness (MRR@10) as a function of the re-ranking depth\n",
      "𝑘when re-ranking the top- 𝑘results by BM25, comparing ColBERT\n",
      "and BERT base(our training). We conduct this experiment on MS\n",
      "MARCO (Dev). We note here that as the official top-1000 ranking\n",
      "does not provide the BM25 order (and also lacks documents beyond\n",
      "the top-1000 per query), the models in this experiment re-rank the\n",
      "Session 1A: NeuIR and Semantic Matching \n",
      " \n",
      "SIGIR ’20, July 25–30, 2020, Virtual Event, China\n",
      "450.27 0.29 0.31 0.33 0.35 0.37\n",
      "MRR@10103104105106107108109Million FLOPs (log-scale)\n",
      "k=10205010020050010002000\n",
      "k=10 20 50 100\n",
      "200500\n",
      "10002000\n",
      "BERTbase (our training)\n",
      "ColBERTFigure 4: FLOPs (in millions) and MRR@10 as functions\n",
      "of the re-ranking depth 𝑘. Since the official BM25 ranking\n",
      "is not ordered, the initial top- 𝑘retrieval is conducted with\n",
      "Anserini’s BM25.\n",
      "Anserini [ 37] toolkit’s BM25 output. Consequently, both MRR@10\n",
      "values at𝑘=1000 are slightly higher from those reported in Table 1.\n",
      "Studying the results in Figure 4, we notice that not only is Col-\n",
      "BERT much cheaper than BERT for the same model size (i.e., 12-\n",
      "layer “base” transformer encoder), it also scales better with the\n",
      "number of ranked documents. In part, this is because ColBERT\n",
      "only needs to process the query once, irrespective of the number of\n",
      "documents evaluated. For instance, at 𝑘=10, BERT requires nearly\n",
      "180×more FLOPs than ColBERT; at 𝑘=1000, BERT’s overhead\n",
      "jumps to 13,900×. It then reaches 23,000× at𝑘=2000.\n",
      "We observe that this orders-of-magnitude reduction in FLOPs\n",
      "makes it practical to run ColBERT entirely on the CPU. In fact, sub-\n",
      "sequent informal experimentation suggests that ColBERT’s latency\n",
      "and FLOPs can be considerably reduced further by a number of\n",
      "optimizations, some entailing a controllable quality tradeoff. These\n",
      "include using smaller vector dimensions (whose MRR@10 is tested\n",
      "in §4.5), padding queries to shorter 𝑁𝑞, processing documents in\n",
      "a lengths-aware fashion, and distilling/quantizing the encoder(s)\n",
      "(§2), the final two of which are also applicable to the baseline BERT\n",
      "reference implementation by Nogueira and Cho [ 25]. Addition-\n",
      "ally, caching the document embeddings on the GPU(s)—if sufficient\n",
      "GPU memory exists—can significantly reduce ColBERT’s latency.\n",
      "Lastly, batch-processing of multiple queries can enhance ColBERT’s\n",
      "throughput by improving the GPU utilization of query encoding.\n",
      "We leave exploring these opportunities for future work.\n",
      "Method MAP MRR@10\n",
      "BM25 (Anserini) 15.3 -\n",
      "doc2query 18.1 -\n",
      "DeepCT 24.6 33.2\n",
      "BM25 + BERT base 31.0 -\n",
      "BM25 + BERT large 33.5 -\n",
      "BM25 + ColBERT 31.3 44.2\n",
      "Table 3: Results on TREC CAR.\n",
      "Having studied our results on MS MARCO, we now consider\n",
      "TREC CAR, whose official metric is MAP. Similar to Table 1, we\n",
      "also report MRR@10. The results are summarized in Table 3, which\n",
      "includes a number of important baselines (BM25, doc2query, and\n",
      "DeepCT) in addition to re-ranking baselines that have previouslybeen tested on this dataset. As the table shows, the results mirror\n",
      "those seen with MS MARCO.\n",
      "4.3 End-to-end Top- 𝑘Retrieval\n",
      "Beyond cheap re-ranking, ColBERT is amenable to top- 𝑘retrieval di-\n",
      "rectly from a full collection. Table 2 considers full retrieval, wherein\n",
      "each model retrieves the top-1000 documents directly from MS\n",
      "MARCO’s 8.8M documents per query. In addition to MRR@10 and\n",
      "latency in milliseconds, the table reports Recall@50, Recall@200,\n",
      "and Recall@1000, important metrics for a full-retrieval model that\n",
      "essentially filters down a large collection on a per-query basis.\n",
      "We compare against BM25, in particular MS MARCO’s official\n",
      "BM25 ranking as well as a well-tuned baseline based on the Anserini\n",
      "toolkit.9While many other traditional models exist, we are not\n",
      "aware of any that substantially outperform Anserini’s BM25 im-\n",
      "plementation (e.g., see RM3 in [ 28], LMDir in [ 2], or Microsoft’s\n",
      "proprietary feature-based RankSVM on the leaderboard).\n",
      "We also compare against doc2query, DeepCT, and docTTTTT-\n",
      "query. All three rely on a traditional bag-of-words model (primarily\n",
      "BM25) for retrieval. Crucially, however, they re-weigh the frequency\n",
      "of terms per document and/or expand the set of terms in each doc-\n",
      "ument before building the BM25 index. In particular, doc2query\n",
      "expands each document with a pre-defined number of synthetic\n",
      "queries generated by a seq2seq transformer model (which docTTT-\n",
      "Tquery replaced with a pre-trained language model, T5 [ 31]). In\n",
      "contrast, DeepCT uses BERT to produce the term frequency com-\n",
      "ponent of BM25 in a context-aware manner.\n",
      "For the latency of Anserini’s BM25, doc2query, and docTTTT-\n",
      "query, we use the authors’ [ 26,28] Anserini-based implementation.\n",
      "While this implementation supports multi-threading, it only utilizes\n",
      "parallelism across different queries. We thus report single-threaded\n",
      "latency for these models, noting that simply parallelizing their com-\n",
      "putation over shards of the index can substantially decrease their\n",
      "already-low latency. For DeepCT, we only estimate its latency us-\n",
      "ing that of BM25 (as denoted by (est.) in the table), since DeepCT\n",
      "re-weighs BM25’s term frequency without modifying the index\n",
      "otherwise.10As discussed in §4.1, we use ColBERT L2for end-to-\n",
      "end retrieval, which employs negative squared L2 distance as its\n",
      "vector-similarity function. For its latency, we measure the time for\n",
      "faiss -based candidate filtering and the subsequent re-ranking. In\n",
      "this experiment, faiss uses all available CPU cores.\n",
      "Looking at Table 2, we first see Anserini’s BM25 baseline at 18.7\n",
      "MRR@10, noticing its very low latency as implemented in Anserini\n",
      "(which extends the well-known Lucene system), owing to both\n",
      "very cheap operations and decades of bag-of-words top- 𝑘retrieval\n",
      "optimizations. The three subsequent baselines, namely doc2query,\n",
      "DeepCT, and docTTTTquery, each brings a decisive enhancement\n",
      "to effectiveness. These improvements come at negligible overheads\n",
      "in latency, since these baselines ultimately rely on BM25-based\n",
      "retrieval. The most effective among these three, docTTTTquery,\n",
      "demonstrates a massive 9% gain over vanilla BM25 by fine-tuning\n",
      "the recent language model T5.\n",
      "9http://anserini.io/\n",
      "10In practice, a myriad of reasons could still cause DeepCT’s latency to differ\n",
      "slightly from BM25’s. For instance, the top- 𝑘pruning strategy employed, if any, could\n",
      "interact differently with a changed distribution of scores.\n",
      "Session 1A: NeuIR and Semantic Matching \n",
      " \n",
      "SIGIR ’20, July 25–30, 2020, Virtual Event, China\n",
      "46Shifting our attention to ColBERT’s end-to-end retrieval effec-\n",
      "tiveness, we see its major gains in MRR@10 over all of these end-\n",
      "to-end models. In fact, using ColBERT in the end-to-end setup is su-\n",
      "perior in terms of MRR@10 to re-ranking with the same model due\n",
      "to the improved recall. Moving beyond MRR@10, we also see large\n",
      "gains in Recall@ 𝑘for𝑘equals to 50, 200, and 1000. For instance, its\n",
      "Recall@50 actually exceeds the official BM25’s Recall@1000 and\n",
      "even all but docTTTTTquery’s Recall@200, emphasizing the value\n",
      "of end-to-end retrieval (instead of just re-ranking) with ColBERT.\n",
      "4.4 Ablation Studies\n",
      "0.220.240.260.280.300.320.340.36\n",
      "MRR@10BERT [CLS]-based dot-product (5-layer)  [A]\n",
      "ColBERT via average similarity (5-layer)  [B]\n",
      "ColBERT without query augmentation (5-layer)  [C]\n",
      "ColBERT (5-layer)  [D]\n",
      "ColBERT (12-layer)  [E]\n",
      "ColBERT + e2e retrieval (12-layer)  [F]\n",
      "Figure 5: Ablation results on MS MARCO (Dev). Between\n",
      "brackets is the number of BERT layers used in each model.\n",
      "The results from §4.2 indicate that ColBERT is highly effective\n",
      "despite the low cost and simplicity of its late interaction mechanism.\n",
      "To better understand the source of this effectiveness, we examine a\n",
      "number of important details in ColBERT’s interaction and encoder\n",
      "architecture. For this ablation, we report MRR@10 on the validation\n",
      "set of MS MARCO in Figure 5, which shows our main re-ranking\n",
      "ColBERT model [E], with MRR@10 of 34.9%.\n",
      "Due to the cost of training all models, we train a copy of our\n",
      "main model that retains only the first 5 layers of BERT out of 12\n",
      "(i.e., model [D]) and similarly train all our ablation models for\n",
      "200k iterations with five BERT layers. To begin with, we ask if the\n",
      "fine-granular interaction in late interaction is necessary. Model [A]\n",
      "tackles this question: it uses BERT to produce a single embedding\n",
      "vector for the query and another for the document, extracted from\n",
      "BERT’s [CLS] contextualized embedding and expanded through a\n",
      "linear layer to dimension 4096 (which equals 𝑁𝑞×128=32×128).\n",
      "Relevance is estimated as the inner product of the query’s and the\n",
      "document’s embeddings, which we found to perform better than\n",
      "cosine similarity for single-vector re-ranking. As the results show,\n",
      "this model is considerably less effective than ColBERT, reinforcing\n",
      "the importance of late interaction.\n",
      "Subsequently, we ask if our MaxSim-based late interaction is bet-\n",
      "ter than other simple alternatives. We test a model [B] that replaces\n",
      "ColBERT’s maximum similarity with average similarity. The results\n",
      "suggest the importance of individual terms in the query paying\n",
      "special attention to particular terms in the document. Similarly,\n",
      "the figure emphasizes the importance of our query augmentation\n",
      "mechanism: without query augmentation [C], ColBERT has a no-\n",
      "ticeably lower MRR@10. Lastly, we see the impact of end-to-end\n",
      "retrieval not only on recall but also on MRR@10. By retrieving\n",
      "directly from the full collection, ColBERT is able to retrieve to the\n",
      "top-10 documents missed entirely from BM25’s top-1000.\n",
      "4.5 Indexing Throughput & Footprint\n",
      "Lastly, we examine the indexing throughput and space footprint\n",
      "of ColBERT. Figure 6 reports indexing throughput on MS MARCO\n",
      "0 10000 20000 30000 40000 50000\n",
      "Throughput (documents/minute)Basic ColBERT Indexing\n",
      "+multi-GPU document processing\n",
      "+per-batch maximum sequence length\n",
      "+length-based bucketing\n",
      "+multi-core pre-processingFigure 6: Effect of ColBERT’s indexing optimizations on the\n",
      "offline indexing throughput.\n",
      "documents with ColBERT and four other ablation settings, which\n",
      "individually enable optimizations described in §3.4 on top of basic\n",
      "batched indexing. Based on these throughputs, ColBERT can index\n",
      "MS MARCO in about three hours. Note that any BERT-based model\n",
      "must incur the computational cost of processing each document\n",
      "at least once. While ColBERT encodes each document with BERT\n",
      "exactly once, existing BERT-based rankers would repeat similar\n",
      "computations on possibly hundreds of documents for each query.\n",
      "Setting Dimension( 𝑚) Bytes/Dim Space(GiBs) MRR@10\n",
      "Re-rank Cosine 128 4 286 34.9\n",
      "End-to-end L2 128 2 154 36.0\n",
      "Re-rank L2 128 2 143 34.8\n",
      "Re-rank Cosine 48 4 54 34.4\n",
      "Re-rank Cosine 24 2 27 33.9\n",
      "Table 4: Space Footprint vs MRR@10 (Dev) on MS MARCO.\n",
      "Table 4 reports the space footprint of ColBERT under various\n",
      "settings as we reduce the embeddings dimension and/or the bytes\n",
      "per dimension. Interestingly, the most space-efficient setting, that\n",
      "is, re-ranking with cosine similarity with 24-dimensional vectors\n",
      "stored as 2-byte floats, is only 1% worse in MRR@10 than the most\n",
      "space-consuming one, while the former requires only 27 GiBs to\n",
      "represent the MS MARCO collection.\n",
      "5 CONCLUSIONS\n",
      "In this paper, we introduced ColBERT, a novel ranking model that\n",
      "employs contextualized late interaction over deep LMs (in particular,\n",
      "BERT) for efficient retrieval. By independently encoding queries\n",
      "and documents into fine-grained representations that interact via\n",
      "cheap and pruning-friendly computations, ColBERT can leverage\n",
      "the expressiveness of deep LMs while greatly speeding up query\n",
      "processing. Crucially, doing so allows scaling ColBERT to end-\n",
      "to-end neural retrieval directly from a large document collection,\n",
      "which can greatly improve recall over existing models. Our results\n",
      "show that ColBERT is two orders-of-magnitude faster than existing\n",
      "BERT-based models, all while only minimally impacting re-ranking\n",
      "quality and while outperforming every non-BERT baseline.\n",
      "Acknowledgments. OK was supported by the Eltoukhy Family\n",
      "Graduate Fellowship at the Stanford School of Engineering. This\n",
      "research was supported in part by affiliate members and other\n",
      "supporters of the Stanford DAWN project—Ant Financial, Facebook,\n",
      "Google, Infosys, NEC, and VMware—as well as Cisco, SAP, and the\n",
      "NSF under CAREER grant CNS-1651570. Any opinions, findings,\n",
      "and conclusions or recommendations expressed in this material are\n",
      "those of the authors and do not necessarily reflect the views of the\n",
      "National Science Foundation.\n",
      "Session 1A: NeuIR and Semantic Matching \n",
      " \n",
      "SIGIR ’20, July 25–30, 2020, Virtual Event, China\n",
      "47REFERENCES\n",
      "[1]Firas Abuzaid, Geet Sethi, Peter Bailis, and Matei Zaharia. 2019. To Index or Not\n",
      "to Index: Optimizing Exact Maximum Inner Product Search. In 2019 IEEE 35th\n",
      "International Conference on Data Engineering (ICDE). IEEE, 1250–1261.\n",
      "[2]Zhuyun Dai and Jamie Callan. 2019. Context-Aware Sentence/Passage Term\n",
      "Importance Estimation For First Stage Retrieval. arXiv preprint arXiv:1910.10687\n",
      "(2019).\n",
      "[3]Zhuyun Dai and Jamie Callan. 2019. Deeper Text Understanding for IR with\n",
      "Contextual Neural Language Modeling. arXiv preprint arXiv:1905.09217 (2019).\n",
      "[4]Zhuyun Dai, Chenyan Xiong, Jamie Callan, and Zhiyuan Liu. 2018. Convolutional\n",
      "neural networks for soft-matching n-grams in ad-hoc search. In Proceedings of the\n",
      "eleventh ACM international conference on web search and data mining. 126–134.\n",
      "[5]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:\n",
      "Pre-training of deep bidirectional transformers for language understanding. arXiv\n",
      "preprint arXiv:1810.04805 (2018).\n",
      "[6]Laura Dietz, Manisha Verma, Filip Radlinski, and Nick Craswell. 2017. TREC\n",
      "Complex Answer Retrieval Overview.. In TREC.\n",
      "[7]Jiafeng Guo, Yixing Fan, Qingyao Ai, and W Bruce Croft. 2016. A deep relevance\n",
      "matching model for ad-hoc retrieval. In Proceedings of the 25th ACM International\n",
      "on Conference on Information and Knowledge Management. ACM, 55–64.\n",
      "[8]Jiafeng Guo, Yixing Fan, Liang Pang, Liu Yang, Qingyao Ai, Hamed Zamani, Chen\n",
      "Wu, W Bruce Croft, and Xueqi Cheng. 2019. A deep look into neural ranking\n",
      "models for information retrieval. arXiv preprint arXiv:1903.06902 (2019).\n",
      "[9]Sebastian Hofstätter and Allan Hanbury. 2019. Let’s measure run time! Extending\n",
      "the IR replicability infrastructure to include performance aspects. arXiv preprint\n",
      "arXiv:1907.04614 (2019).\n",
      "[10] Sebastian Hofstätter, Navid Rekabsaz, Carsten Eickhoff, and Allan Hanbury. 2019.\n",
      "On the effect of low-frequency terms on neural-IR models. In Proceedings of\n",
      "the 42nd International ACM SIGIR Conference on Research and Development in\n",
      "Information Retrieval. 1137–1140.\n",
      "[11] Sebastian Hofstätter, Markus Zlabinger, and Allan Hanbury. 2019. TU Wien@\n",
      "TREC Deep Learning’19–Simple Contextualization for Re-ranking. arXiv preprint\n",
      "arXiv:1912.01385 (2019).\n",
      "[12] Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry\n",
      "Heck. 2013. Learning deep structured semantic models for web search using\n",
      "clickthrough data. In Proceedings of the 22nd ACM international conference on\n",
      "Information & Knowledge Management. 2333–2338.\n",
      "[13] Shiyu Ji, Jinjin Shao, and Tao Yang. 2019. Efficient Interaction-based Neural\n",
      "Ranking with Locality Sensitive Hashing. In The World Wide Web Conference.\n",
      "ACM, 2858–2864.\n",
      "[14] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang,\n",
      "and Qun Liu. 2019. Tinybert: Distilling bert for natural language understanding.\n",
      "arXiv preprint arXiv:1909.10351 (2019).\n",
      "[15] Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2017. Billion-scale similarity\n",
      "search with GPUs. arXiv preprint arXiv:1702.08734 (2017).\n",
      "[16] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-\n",
      "mization. arXiv preprint arXiv:1412.6980 (2014).\n",
      "[17] Ron Kohavi, Alex Deng, Brian Frasca, Toby Walker, Ya Xu, and Nils Pohlmann.\n",
      "2013. Online controlled experiments at large scale. In SIGKDD.\n",
      "[18] Sean MacAvaney, Andrew Yates, Arman Cohan, and Nazli Goharian. 2019. Cedr:\n",
      "Contextualized embeddings for document ranking. In Proceedings of the 42nd\n",
      "International ACM SIGIR Conference on Research and Development in Information\n",
      "Retrieval. ACM, 1101–1104.\n",
      "[19] Paul Michel, Omer Levy, and Graham Neubig. 2019. Are Sixteen Heads Really\n",
      "Better than One?. In Advances in Neural Information Processing Systems . 14014–\n",
      "14024.\n",
      "[20] Bhaskar Mitra and Nick Craswell. 2019. An Updated Duet Model for Passage\n",
      "Re-ranking. arXiv preprint arXiv:1903.07666 (2019).\n",
      "[21] Bhaskar Mitra, Nick Craswell, et al .2018. An introduction to neural information\n",
      "retrieval. Foundations and Trends® in Information Retrieval 13, 1 (2018), 1–126.\n",
      "[22] Bhaskar Mitra, Fernando Diaz, and Nick Craswell. 2017. Learning to match using\n",
      "local and distributed representations of text for web search. In Proceedings of the26th International Conference on World Wide Web. International World Wide Web\n",
      "Conferences Steering Committee, 1291–1299.\n",
      "[23] Bhaskar Mitra, Corby Rosset, David Hawking, Nick Craswell, Fernando Diaz,\n",
      "and Emine Yilmaz. 2019. Incorporating query term independence assumption\n",
      "for efficient retrieval and ranking using deep neural networks. arXiv preprint\n",
      "arXiv:1907.03693 (2019).\n",
      "[24] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan\n",
      "Majumder, and Li Deng. 2016. MS MARCO: A Human-Generated MAchine\n",
      "Reading COmprehension Dataset. (2016).\n",
      "[25] Rodrigo Nogueira and Kyunghyun Cho. 2019. Passage Re-ranking with BERT.\n",
      "arXiv preprint arXiv:1901.04085 (2019).\n",
      "[26] Rodrigo Nogueira, Jimmy Lin, and AI Epistemic. 2019. From doc2query to\n",
      "docTTTTTquery. (2019).\n",
      "[27] Rodrigo Nogueira, Wei Yang, Kyunghyun Cho, and Jimmy Lin. 2019. Multi-Stage\n",
      "Document Ranking with BERT. arXiv preprint arXiv:1910.14424 (2019).\n",
      "[28] Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. 2019. Document\n",
      "Expansion by Query Prediction. arXiv preprint arXiv:1904.08375 (2019).\n",
      "[29] Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher\n",
      "Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word\n",
      "representations. arXiv preprint arXiv:1802.05365 (2018).\n",
      "[30] Yifan Qiao, Chenyan Xiong, Zhenghao Liu, and Zhiyuan Liu. 2019. Understanding\n",
      "the Behaviors of BERT in Ranking. arXiv preprint arXiv:1904.07531 (2019).\n",
      "[31] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\n",
      "Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring the lim-\n",
      "its of transfer learning with a unified text-to-text transformer. arXiv preprint\n",
      "arXiv:1910.10683 (2019).\n",
      "[32] Stephen E Robertson, Steve Walker, Susan Jones, Micheline M Hancock-Beaulieu,\n",
      "Mike Gatford, et al. 1995. Okapi at TREC-3. NIST Special Publication (1995).\n",
      "[33] Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, and Jimmy Lin.\n",
      "2019. Distilling task-specific knowledge from BERT into simple neural networks.\n",
      "arXiv preprint arXiv:1903.12136 (2019).\n",
      "[34] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\n",
      "Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\n",
      "you need. In Advances in neural information processing systems. 5998–6008.\n",
      "[35] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi,\n",
      "Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al .\n",
      "2016. Google’s neural machine translation system: Bridging the gap between\n",
      "human and machine translation. arXiv preprint arXiv:1609.08144 (2016).\n",
      "[36] Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, and Russell Power. 2017.\n",
      "End-to-end neural ad-hoc ranking with kernel pooling. In Proceedings of the 40th\n",
      "International ACM SIGIR conference on research and development in information\n",
      "retrieval. 55–64.\n",
      "[37] Peilin Yang, Hui Fang, and Jimmy Lin. 2018. Anserini: Reproducible ranking\n",
      "baselines using Lucene. Journal of Data and Information Quality (JDIQ) 10, 4\n",
      "(2018), 1–20.\n",
      "[38] Wei Yang, Kuang Lu, Peilin Yang, and Jimmy Lin. 2019. Critically Examining\n",
      "the\" Neural Hype\" Weak Baselines and the Additivity of Effectiveness Gains\n",
      "from Neural Ranking Models. In Proceedings of the 42nd International ACM SIGIR\n",
      "Conference on Research and Development in Information Retrieval. 1129–1132.\n",
      "[39] Zeynep Akkalyoncu Yilmaz, Wei Yang, Haotian Zhang, and Jimmy Lin. 2019.\n",
      "Cross-domain modeling of sentence-level evidence for document retrieval. In\n",
      "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Pro-\n",
      "cessing and the 9th International Joint Conference on Natural Language Processing\n",
      "(EMNLP-IJCNLP). 3481–3487.\n",
      "[40] Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat. 2019. Q8bert:\n",
      "Quantized 8bit bert. arXiv preprint arXiv:1910.06188 (2019).\n",
      "[41] Hamed Zamani, Mostafa Dehghani, W Bruce Croft, Erik Learned-Miller, and\n",
      "Jaap Kamps. 2018. From neural re-ranking to neural ranking: Learning a sparse\n",
      "representation for inverted indexing. In Proceedings of the 27th ACM International\n",
      "Conference on Information and Knowledge Management. ACM, 497–506.\n",
      "[42] Le Zhao. 2012. Modeling and solving term mismatch for full-text retrieval. Ph.D.\n",
      "Dissertation. Carnegie Mellon University.\n",
      "Session 1A: NeuIR and Semantic Matching \n",
      " \n",
      "SIGIR ’20, July 25–30, 2020, Virtual Event, China\n",
      "48\n",
      "-----------------------------------\n",
      "Extracted text from 'Reference3.pdf':\n",
      "Doc2Query--: When Less is More\n",
      "Mitko Gospodinov1, Sean MacAvaney2, and Craig Macdonald2\n",
      "University of Glasgow\n",
      "12024810G@student.gla.ac.uk\n",
      "2{first}.{last}@glasgow.ac.uk\n",
      "Abstract. Doc2Query — the process of expanding the content of a\n",
      "document before indexing using a sequence-to-sequence model — has\n",
      "emerged as a prominent technique for improving the ﬁrst-stage retrieval\n",
      "eﬀectivenessofsearchengines.However,sequence-to-sequencemodelsare\n",
      "known to be prone to “hallucinating” content that is not present in the\n",
      "source text. We argue that Doc2Query is indeed prone to hallucination,\n",
      "which ultimately harms retrieval eﬀectiveness and inﬂates the index size.\n",
      "In this work, we explore techniques for ﬁltering out these harmful queries\n",
      "prior to indexing. We ﬁnd that using a relevance model to remove poor-\n",
      "quality queries can improve the retrieval eﬀectiveness of Doc2Query by\n",
      "up to 16%, while simultaneously reducing mean query execution time by\n",
      "23% and cutting the index size by 33%. We release the code, data, and\n",
      "a live demonstration to facilitate reproduction and further exploration.1\n",
      "1 Introduction\n",
      "Neural network models, particularly those based on contextualised language\n",
      "models, have been shown to improve search eﬀectiveness [3]. While some ap-\n",
      "proaches focus on re-ranking document sets from a ﬁrst-stage retrieval function\n",
      "to improve precision [27], others aim to improve the ﬁrst stage itself [4]. In this\n",
      "work, we focus on one of these ﬁrst-stage approaches: Doc2Query [29]. This ap-\n",
      "proach trains a sequence-to-sequence model (e.g., T5 [33]) to predict queries that\n",
      "may be relevant to a particular text. Then, when indexing, this model is used\n",
      "toexpandthe document by generating a collection of queries and appending\n",
      "them to the document. Though computationally expensive at index time [34],\n",
      "this approach has been shown to be remarkably eﬀective even when retrieving\n",
      "using simple lexical models like BM25 [28]. Numerous works have shown that\n",
      "the approach can produce a high-quality pool of results that are eﬀective for\n",
      "subsequent stages in the ranking pipeline [19,20,23,40].\n",
      "However, sequence-to-sequence models are well-known to be prone to gener-\n",
      "ate content that does not reﬂect the input text – a defect known in literature\n",
      "as “hallucination” [25]. We ﬁnd that existing Doc2Query models are no excep-\n",
      "tion. Figure 1 provides example generated queries from the state-of-the-art T5\n",
      "Doc2Query model [28]. In this example, we see that many of the generated\n",
      "queries cannot actually be answered by the source passage (score \u00141).\n",
      "1https://github.com/terrierteam/pyterrier_doc2queryarXiv:2301.03266v3  [cs.IR]  27 Feb 20232 Gospodinov et al.\n",
      "Original Passage: Barley (Hordeum vulgare L.), a\n",
      "member of the grass family, is a major cereal grain. It\n",
      "was one of the ﬁrst cultivated grains and is now grown\n",
      "widely. Barley grain is a staple in Tibetan cuisine and\n",
      "waseatenwidelybypeasantsinMedievalEurope.Bar-\n",
      "ley has also been used as animal fodder, as a source\n",
      "of fermentable material for beer and certain distilled\n",
      "beverages,andasacomponentofvarioushealthfoods.Generated Queries: (1) where does barley originate\n",
      "from\u0001(2) what is the name of the cereal grain used\n",
      "in tibetan cooking? \u0001(3) what is barley used for \u0001(1)\n",
      "what is barley in food \u0001(0) what is bare wheat \u0001(3)\n",
      "what family of organisms is barley in \u0001(1) why is bar-\n",
      "ley important in tibetan diet \u0001(3) what is barley \u0001\n",
      "(2) where is barley grown \u0001(1) where was barley ﬁrst\n",
      "grown and eaten \u0001(1) where was barley ﬁrst used ...\n",
      "Fig. 1.Example passage from MS MARCO and generated queries using the T5\n",
      "Doc2Query model. The relevance of each query to the passage is scored by the au-\n",
      "thors on a scale of 0–3 using the TREC Deep Learning passage relevance criteria.\n",
      "Based on this observation, we hypothesise that retrieval performance of\n",
      "Doc2Querywouldimproveifhallucinatedquerieswereremoved.Inthispaper,we\n",
      "conduct experiments where we apply a new ﬁltering phase that aims to remove\n",
      "poor queries prior to indexing. Given that this approach removes queries, we\n",
      "call the approach Doc2Query-- (Doc2Query-minus-minus). Rather than training\n",
      "a new model for this task, we identify that relevance models are already ﬁt for\n",
      "this purpose: they estimate how relevant a passage is to a query. We therefore\n",
      "explore ﬁltering strategies that make use of existing neural relevance models.\n",
      "Through experimentation on the MS MARCO dataset, we ﬁnd that our ﬁl-\n",
      "tering approach can improve the retrieval eﬀectiveness of indexes built using\n",
      "Doc2Query-- by up to 16%; less can indeed be more. Meanwhile, ﬁltering nat-\n",
      "urally reduces the index size, lowering storage and query-time computational\n",
      "costs. Finally, we conduct an exploration of the index-time overheads introduced\n",
      "bytheﬁlteringprocessandconcludethatthegainsfromﬁlteringmorethanmake\n",
      "up for the additional time spent generating more queries. The approach also has\n",
      "a positive impact on the environmental costs of applying Doc2Query; the same\n",
      "retrieval eﬀectiveness can be achieved with only about a third of the compu-\n",
      "tational cost when indexing. To facilitate last-metre, last-mile, and complete\n",
      "reproduction eﬀorts [36], we release the code, indices, and ﬁltering scores.1In\n",
      "summary, we contribute a technique to improve the eﬀectiveness and eﬃciency\n",
      "of Doc2Query by ﬁltering out queries that do not reﬂect the original passage.\n",
      "2 Related Work\n",
      "The classical lexical mismatch problem is a key one in information retrieval -\n",
      "documents that do not contain the query terms may not be retrieved. In the\n",
      "literature, various approaches have addressed this: query reformulation – includ-\n",
      "ing stemming, query expansion models (e.g. Rocchio, Bo1 [1], RM3 [12]) – and\n",
      "document expansion [9,30,35]. Classically, query expansion models have been\n",
      "popular, as they avoid the costs associated with making additional processing\n",
      "for each document needed for document expansion. However, query expansion\n",
      "may result in reduced performance [11], as queries are typically short and the\n",
      "necessary evidence to understand the context of the user is limited.Doc2Query--: When Less is More 3\n",
      "The application of latent representations of queries and documents, such\n",
      "as using latent semantic indexing [8] allow retrieval to not be driven directly\n",
      "by lexical signals. More recently, transformer-based language models (such as\n",
      "BERT [6]) have resulted in representations of text where the contextualised\n",
      "meaning of words are accounted for. In particular, in dense retrieval, queries\n",
      "and documents are represented in embeddings spaces [14,37], often facilitated\n",
      "by Approximate Nearest Neighbour (ANN) data structures [13]. However, even\n",
      "when using ANN, retrieval can still be ineﬃcient or insuﬃciently eﬀective [15].\n",
      "Others have explored approaches for augmenting lexical representations with\n",
      "additional terms that may be relevant. In this work, we explore Doc2Query [29],\n",
      "which uses a sequence-to-sequence model that maps a document to queries that\n",
      "it might be able to answer. By appending these generated queries to a docu-\n",
      "ment’s content before indexing, the document is more likely to be retrieved for\n",
      "user queries when using a model like BM25. An alternative style of document\n",
      "expansion, proposed by MacAvaney et al. [19] and since used by several other\n",
      "models (e.g., [10,39,40]), uses the built-in Masked Language Modelling (MLM)\n",
      "mechanism. MLM expansion generates individual tokens to append to the docu-\n",
      "ment as a bag of words (rather than as a sequence). Although MLM expansion is\n",
      "also prone to hallucination,2the bag-of-words nature of MLM expansion means\n",
      "that individual expansion tokens may not have suﬃcient context to apply ﬁl-\n",
      "tering eﬀectively. We therefore focus only on sequence-style expansion and leave\n",
      "the exploration of MLM expansion for future work.\n",
      "3 Doc2Query--\n",
      "Doc2Query-- consists of two phases: a generation phrase and a ﬁltering phase.\n",
      "In the generation phase, a Doc2Query model generates a set of nqueries that\n",
      "each document might be able to answer. However, as shown in Figure 1, not\n",
      "all of the queries are necessarily relevant to the document. To mitigate this\n",
      "problem, Doc2Query-- then proceeds to a ﬁltering phase, which is responsible\n",
      "for eliminating the generated queries that are least relevant to the source doc-\n",
      "ument. Because hallucinated queries contain details not present in the original\n",
      "text (by deﬁnition), we argue that hallucinated queries are less useful for re-\n",
      "trieval than non-hallucinated ones. Filtering is accomplished by retaining only\n",
      "the most relevant pproportion of generated queries over the entire corpus. The\n",
      "retained queries are then concatenated to their corresponding documents prior\n",
      "to indexing, as per the existing Doc2Query approach.\n",
      "More formally, consider an expansion function ethat maps a document to n\n",
      "queries: e:D7!Qn. In Doc2Query, each document in corpus Dare concate-\n",
      "natedwiththeirexpansionqueries,forminganewcorpus D0=fConcat (d; e(d))j\n",
      "d2Dg,whichisthen indexedbya retrievalsystem.Doc2Query--addsaﬁltering\n",
      "mechanism that uses a relevance model that maps a query and document to a\n",
      "real-valued relevance score s:Q\u0002D7!R(with larger values indicating higher\n",
      "2For instance, we ﬁnd that SPLADE [10] generates the following seemingly-unrelated\n",
      "terms for the passage in Figure 1 in the top 20 expansion terms: reed,herb, and troy.4 Gospodinov et al.\n",
      "relevance). The relevance scoring function is used to ﬁlter down the queries to\n",
      "those that meet a certain score threshold tas follows:\n",
      "D0=n\n",
      "Concat\u0000\n",
      "d;\b\n",
      "qjq2e(d)^s(q; d)\u0015t\t\u0001\n",
      "jd2Do\n",
      "(1)\n",
      "The relevance threshold tis naturally dependent upon the relevance scoring\n",
      "function. It can be set empirically, chosen based on operational criteria (e.g.,\n",
      "targetindexsize),or(forawell-calibratedrelevancescoringfunction)determined\n",
      "a priori. In this work, we combine the ﬁrst two strategies: we pick tbased on\n",
      "the distribution of relevance scores across all expansion queries. For instance,\n",
      "atp= 0:3we only keep queries with relevance scores in the top 30%, which is\n",
      "t= 3:215for the ELECTRA [31] scoring model on the MS MARCO dataset [26].\n",
      "4 Experimental Setup\n",
      "We conduct experiments to answer the following research questions:\n",
      "RQ1 Does Doc2Query-- improve the eﬀectiveness of document expansion?\n",
      "RQ2 What are the trade-oﬀs in terms of eﬀectiveness, eﬃciency, and storage when\n",
      "using Doc2Query--?\n",
      "Datasets and Measures. We conduct tests using the MS MARCO [26] v1\n",
      "passage corpus. We use ﬁve test collections:3(1) the MS MARCO Dev (small)\n",
      "collection, consisting of 6,980 queries (1.1 qrels/query); (2) the Dev2 collection,\n",
      "consisting of 4,281 (1.1 qrels/query); (3) the MS MARCO Eval set, consisting of\n",
      "6,837 queries (held-out leaderboard set); (4/5) the TREC DL’19/’20 collections,\n",
      "consisting of 43/54 queries (215/211 qrels/query). We evaluate using the oﬃcial\n",
      "task evaluation measures: Reciprocal Rank at 10 (RR@10) for Dev/Dev2/Eval,\n",
      "nDCG@10 for DL’19/’20. We tune systems4on Dev, leaving the remaining col-\n",
      "lections as held-out test sets.\n",
      "Models. We use the T5 Doc2Query model from Nogueira and Lin [28], mak-\n",
      "ing use of the inferred queries released by the authors (80 per passage). To the\n",
      "best of our knowledge, this is the highest-performing Doc2Query model avail-\n",
      "able. We consider three neural relevance models for ﬁltering: ELECTRA5[31],\n",
      "MonoT56[32],andTCT-ColBERT7[16],coveringtwostrongcross-encodermod-\n",
      "els and one strong bi-encoder model. We also explored ﬁlters that use the prob-\n",
      "abilities from the generation process itself but found them to be ineﬀective and\n",
      "therefore omit these results due to space constraints.\n",
      "Tools and Environment. WeusethePyTerriertoolkit[22]withaPISA[24,17]\n",
      "index to conduct our experiments. We deploy PISA’s Block-Max WAND [7] im-\n",
      "plementation for BM25 retrieval. Inference was conducted on an NVIDIA 3090\n",
      "GPU. Evaluation was conducted using the ir-measures package [18].\n",
      "3ir-datasets [21] IDs: msmarco-passage/dev/small ,msmarco-passage/dev/2 ,\n",
      "msmarco-passage/eval/small , msmarco-passage/trec-dl-2019/judged ,\n",
      "msmarco-passage/trec-dl-2020/judged4BM25’s k1,b, and whether to\n",
      "remove stopwords were tuned for all systems; the ﬁltering percentage ( p)\n",
      "was also tuned for ﬁltered systems.5crystina-z/monoELECTRA_LCE_nneg31\n",
      "6castorini/monot5-base-msmarco7castorini/tct_colbert-v2-hnp-msmarcoDoc2Query--: When Less is More 5\n",
      "Table 1. Eﬀectiveness and eﬃciency measurements for Doc2Query-- and baselines.\n",
      "Signiﬁcant diﬀerences between Doc2Query and their corresponding ﬁltered versions\n",
      "for Dev, Dev2, DL’19 and DL’20 are indicated with * (paired t-test, p < 0:05). Values\n",
      "marked withyare taken from the corresponding submissions to the public leaderboard.\n",
      "RR@10 nDCG@10 ms/q GB\n",
      "System Dev Dev2 Eval DL’19 DL’20 MRT Index\n",
      "BM25 0.185 0.182y0.186 0.499 0.479 5 0.71\n",
      "Doc2Query ( n= 40) 0.277 0.265y0.272 0.626 0.607 30 1.17\n",
      "w/ ELECTRA Filter (30%) *0.316 *0.310 -0.667 0.611 23 0.89\n",
      "w/ MonoT5 Filter (40%) *0.308 *0.298 0.306 0.650 0.611 29 0.93\n",
      "w/ TCT Filter (50%) *0.287 *0.280 - 0.640 0.599 30 0.94\n",
      "Doc2Query ( n= 80) 0.279 0.267 - 0.627 0.605 30 1.41\n",
      "w/ ELECTRA Filter (30%) *0.323 *0.316 0.325 0.670 0.614 23 0.95\n",
      "w/ MonoT5 Filter (40%) *0.311 *0.298 - 0.665 0.609 28 1.04\n",
      "w/ TCT Filter (50%) *0.293 *0.283 - 0.642 0.588 28 1.05\n",
      "5 Results\n",
      "We ﬁrst explore RQ1: whether relevance ﬁltering can improve the retrieval of\n",
      "Doc2Query models. Table 1 compares the eﬀectiveness of Doc2Query with var-\n",
      "ious ﬁlters. We observe that all the ﬁlters signiﬁcantly improve the retrieval\n",
      "eﬀectiveness on the Dev and Dev2 datasets at both n= 40andn= 80. We also\n",
      "observe a large boost in performance on the Eval dataset.8Though the diﬀer-\n",
      "ences in DL’19 and DL’20 appear to be considerable (e.g., 0.627 to 0.670), these\n",
      "diﬀerences are not statistically signiﬁcant.\n",
      "Diggingalittledeeper,Figure2showstheretrievaleﬀectivenessofDoc2Query\n",
      "with various numbers of generated queries (in dotted black) and the correspond-\n",
      "ing performance when ﬁltering using the top-performing ELECTRA scorer (in\n",
      "solid blue). We observe that performing relevance ﬁltering at each value of n\n",
      "improves the retrieval eﬀectiveness. For instance, keeping only 30% of expan-\n",
      "sion queries at n= 80, performance is increased from 0.279 to 0.323 – a 16%\n",
      "improvement.\n",
      "In aggregate, results from Table 1 and Figure 2 answer RQ1: Doc2Query--\n",
      "ﬁltering can signiﬁcantly improve the retrieval eﬀectiveness of Doc2Query across\n",
      "various scoring models, numbers of generated queries ( n) and thresholds ( p).\n",
      "Next,weexplorethetrade-oﬀsintermsofeﬀectiveness,eﬃciency,andstorage\n",
      "when using Doc2Query--. Table 1 includes the mean response time and index\n",
      "sizes for each of the settings. As expected, ﬁltering reduces the index size since\n",
      "fewer terms are stored. For the best-performing setting ( n= 80with ELECTRA\n",
      "8Signiﬁcance cannot be determined due to the held-out nature of the dataset. Further,\n",
      "due to restrictions on the number of submissions to the leaderboard, we only are able\n",
      "to submit two runs. The ﬁrst aims to be a fair comparison with the existing Doc2Query\n",
      "Eval result, using the same number of generated queries and same base T5 model for\n",
      "scoring. The second is our overall best-performing setting, using the ELECTRA ﬁlter\n",
      "atn= 80generated queries.6 Gospodinov et al.\n",
      "0 1 2 3 4 5\n",
      "Total Tokens 1e90.2250.2500.2750.3000.325RR@10\n",
      "90%80%70%60%50%40% 30%\n",
      "n=5n=10n=20n=40 n=80\n",
      "Generation PhaseFiltering Phase\n",
      "Fig. 2.Eﬀectiveness (RR@10) on the Dev set, compared with the total number of\n",
      "indexed tokens. The generation phase is shown in dotted black (at various values of\n",
      "n), and the ELECTRA ﬁltering phase is shown in solid blue (at various values of p).\n",
      "ﬁlter), this amounts to a 33% reduction in index size (1.41 GB down to 0.95 GB).\n",
      "Naturally, such a reduction has an impact on query processing time as well; it\n",
      "yields a 23% reduction in mean response time (30ms down to 23ms).\n",
      "Doc2Query-- ﬁltering adds substantial cost an indexing time, mostly due to\n",
      "scoring each of the generated queries. Table 2 reports the cost (in hours of GPU\n",
      "time) of the generation and ﬁltering phases. We observe that ELECTRA ﬁlter-\n",
      "ing can yield up to a 78% increase in GPU time ( n= 10). However, we ﬁnd that\n",
      "the improved eﬀectiveness makes up for this cost. To demonstrate this, we al-\n",
      "locate the time spent ﬁltering to generating additional queries for each passage.\n",
      "For instance, the 15 hours spent scoring n= 5queries could instead be spent\n",
      "generating 6 more queries per passage (for a total of n= 11). We ﬁnd that when\n",
      "comparing against an unﬁltered nthat closely approximates the total time when\n",
      "Table 2. Retrieval eﬀectiveness comparison for comparable indexing computational\n",
      "budgets (in hours of GPU time). Values of nwithout a ﬁlter are chosen to best approx-\n",
      "imate the total compute hours or the Dev eﬀectiveness of the corresponding ﬁltered\n",
      "version. Signiﬁcant diﬀerences between in RR@10 performance are indicated with *\n",
      "(paired t-test, p < 0:05).\n",
      "GPU Hours RR@10\n",
      "nFilter Gen+Filt=Tot Dev Dev2 Comment\n",
      "5 ELECTRA 20 + 15 = 34 0.273 0.270\n",
      "11None 34 + 0 = 34 *0.261 *0.256 \u00004% Dev RR for sim. GPU hrs\n",
      "31None 99 + 0 = 99 0.273 0.265\u00022:9GPU hrs to match Dev RR\n",
      "10 ELECTRA 32 + 25 = 57 0.292 0.292\n",
      "18None 59 + 0 = 59 *0.270 *0.260 \u00008% Dev RR for sim. GPU hrs\n",
      "20 ELECTRA 66 + 47 = 113 0.307 0.303\n",
      "36None 113 + 0 = 113 *0.275 *0.265 \u000010% Dev RR for sim. GPU hrs\n",
      "40 ELECTRA 128 + 86 = 214 0.316 0.310\n",
      "68None 216 + 0 = 216 *0.279 *0.267 \u000012% Dev RR for sim. GPU hrsDoc2Query--: When Less is More 7\n",
      "ﬁltering, the ﬁltered results consistently yield signiﬁcantly higher retrieval eﬀec-\n",
      "tiveness. As the computational budget increases, so does the margin between\n",
      "Doc2Query and Doc2Query--, from 4% at 34 hours up to 12% at 216 hours.\n",
      "From the opposite perspective, Doc2Query consumes 2.9 \u0002or more GPU\n",
      "time than Doc2Query-- to achieve similar eﬀectiveness ( n= 13with no ﬁlter\n",
      "vs.n= 5with ELECTRA ﬁlter). Since the eﬀectiveness of Doc2Query ﬂattens\n",
      "out between n= 40andn= 80(as seen in Figure 2), it likely requires a\n",
      "massive amount of additional compute to reach the eﬀectiveness of Doc2Query--\n",
      "atn\u001510, if that eﬀectiveness is achievable at all. These comparisons show that\n",
      "if a deployment is targeting a certain level of eﬀectiveness (rather than a target\n",
      "compute budget), Doc2Query-- is also preferable to Doc2Query.\n",
      "TheseresultscollectivelyanswerRQ2:Doc2Query--provideshighereﬀective-\n",
      "ness at lower query-time costs, even when controlling for the additional compute\n",
      "required at index time.\n",
      "6 Conclusions\n",
      "Thisworkdemonstratedthatthereareuntappedadvantagesingeneratingnatural-\n",
      "language for document expansion. Speciﬁcally, we presented Doc2Query--, which\n",
      "isanewapproachforimprovingtheeﬀectivenessandeﬃciencyoftheDoc2Query\n",
      "model by ﬁltering out the least relevant queries. We observed that a 16% im-\n",
      "provement in retrieval eﬀectiveness can be achieved, while reducing the index\n",
      "size by 33% and mean query execution time by 23%.\n",
      "The technique of ﬁltering text generated from language models using rel-\n",
      "evance scoring is ripe for future work. For instance, relevance ﬁltering could\n",
      "potentially apply to approaches that generate alternative forms of queries [38],\n",
      "training data [2], or natural language responses to queries [5] — all of which\n",
      "are potentially aﬀected by hallucinated content. Furthermore, future work could\n",
      "explore approaches for relevance ﬁltering over masked language modelling ex-\n",
      "pansion [19], rather than sequence-to-sequence expansion.\n",
      "Acknowledgements\n",
      "SeanMacAvaneyandCraigMacdonaldacknowledgeEPSRCgrantEP/R018634/1:\n",
      "Closed-Loop Data Science for Complex, Computationally- & Data-Intensive An-\n",
      "alytics.\n",
      "References\n",
      "1. Amati, G., Van Rijsbergen, C.J.: Probabilistic models of information retrieval\n",
      "based on measuring the divergence from randomness. ACM Trans. Inf. Syst. 20(4)\n",
      "(2002)\n",
      "2. Bonifacio,L.,Abonizio,H.,Fadaee,M.,Nogueira,R.:InPars:Unsuperviseddataset\n",
      "generation for information retrieval. In: Proceedings of SIGIR (2022)8 Gospodinov et al.\n",
      "3. Dai, Z., Callan, J.: Deeper text understanding for IR with contextual neural lan-\n",
      "guage modeling. In: Proceedings of SIGIR (2019)\n",
      "4. Dai, Z., Callan, J.: Context-aware document term weighting for ad-hoc search. In:\n",
      "Proceedings of The Web Conference (2020)\n",
      "5. Das, R., Dhuliawala, S., Zaheer, M., McCallum, A.: Multi-step retriever-reader\n",
      "interaction for scalable open-domain question answering. In: Proceedings of ICLR\n",
      "(2019)\n",
      "6. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: Pre-training of deep\n",
      "bidirectional transformers for language understanding. In: Proceedings of NAACL-\n",
      "HLT (2019)\n",
      "7. Ding, S., Suel, T.: Faster top-k document retrieval using block-max indexes. In:\n",
      "Proceedings of SIGIR (2011)\n",
      "8. Dumais, S.T., Furnas, G.W., Landauer, T.K., Deerwester, S., Harshman, R.: Using\n",
      "latent semantic analysis to improve access to textual information. In: Proceedings\n",
      "of SIGCHI CHI (1988)\n",
      "9. Efron, M., Organisciak, P., Fenlon, K.: Improving retrieval of short texts through\n",
      "document expansion. In: Proceedings of SIGIR (2012)\n",
      "10. Formal, T., Piwowarski, B., Clinchant, S.: SPLADE: Sparse lexical and expansion\n",
      "model for ﬁrst stage ranking. In: Proceedings of SIGIR (2021)\n",
      "11. He, B., Ounis, I.: Studying query expansion eﬀectiveness. In: Proceedings of ECIR\n",
      "(2009)\n",
      "12. Jaleel, N.A., Allan, J., Croft, W.B., Diaz, F., Larkey, L.S., Li, X., Smucker, M.D.,\n",
      "Wade, C.: Umass at TREC 2004: Novelty and HARD. In: TREC (2004)\n",
      "13. Johnson, J., Douze, M., Jegou, H.: Billion-scale similarity search with GPUs. IEEE\n",
      "Transactions on Big Data 7(03) (2021)\n",
      "14. Khattab, O., Zaharia, M.: ColBERT: Eﬃcient and eﬀective passage search via\n",
      "contextualized late interaction over BERT. In: Proceedings of SIGIR (2020)\n",
      "15. Lin, J., Ma, X., Mackenzie, J., Mallia, A.: On the separation of logical and physical\n",
      "ranking models for text retrieval applications. In: Proceedings of DESIRES (2021)\n",
      "16. Lin, S.C., Yang, J.H., Lin, J.: In-batch negatives for knowledge distillation with\n",
      "tightly-coupled teachers for dense retrieval. In: Proceedings of RepL4NLP (2021)\n",
      "17. MacAvaney, S., Macdonald, C.: A Python interface to PISA! In: Proceedings of\n",
      "SIGIR (2022)\n",
      "18. MacAvaney,S.,Macdonald,C.,Ounis,I.:Streamliningevaluationwithir-measures.\n",
      "In: Proceedings of ECIR (2022)\n",
      "19. MacAvaney, S., Nardini, F.M., Perego, R., Tonellotto, N., Goharian, N., Frieder,\n",
      "O.: Expansion via prediction of importance with contextualization. In: Proceedings\n",
      "of SIGIR (2020)\n",
      "20. MacAvaney, S., Tonellotto, N., Macdonald, C.: Adaptive re-ranking with a corpus\n",
      "graph. In: Proceedings of CIKM (2022)\n",
      "21. MacAvaney, S., Yates, A., Feldman, S., Downey, D., Cohan, A., Goharian, N.:\n",
      "Simpliﬁed data wrangling with ir_datasets. In: Proceedings of SIGIR (2021)\n",
      "22. Macdonald, C., Tonellotto, N.: Declarative experimentation in information re-\n",
      "trieval using PyTerrier. In: Proceedings of ICTIR (2020)\n",
      "23. Mallia, A., Khattab, O., Suel, T., Tonellotto, N.: Learning passage impacts for\n",
      "inverted indexes. In: Proceedings of SIGIR (2021)\n",
      "24. Mallia, A., Siedlaczek, M., Mackenzie, J., Suel, T.: PISA: performant indexes and\n",
      "search for academia. In: Proceedings of OSIRRC@SIGIR (2019)\n",
      "25. Maynez, J., Narayan, S., Bohnet, B., McDonald, R.: On faithfulness and factuality\n",
      "in abstractive summarization. In: Proceedings of ACL (2020)Doc2Query--: When Less is More 9\n",
      "26. Nguyen, T., Rosenberg, M., Song, X., Gao, J., Tiwary, S., Majumder, R., Deng,\n",
      "L.: MS MARCO: A human generated machine reading comprehension dataset. In:\n",
      "Proceedings of CoCo@NIPS (2016)\n",
      "27. Nogueira, R., Cho, K.: Passage re-ranking with BERT. ArXiv abs/1901.04085\n",
      "(2019)\n",
      "28. Nogueira, R., Lin, J.: From doc2query to doctttttquery (2019)\n",
      "29. Nogueira, R., Yang, W., Lin, J.J., Cho, K.: Document expansion by query predic-\n",
      "tion. ArXiv abs/1904.08375 (2019)\n",
      "30. Pickens, J., Cooper, M., Golovchinsky, G.: Reverted indexing for feedback and\n",
      "expansion. In: Proceedings of CIKM (2010)\n",
      "31. Pradeep, R., Liu, Y., Zhang, X., Li, Y., Yates, A., Lin, J.: Squeezing water from a\n",
      "stone: A bag of tricks for further improving cross-encoder eﬀectiveness for rerank-\n",
      "ing. In: Proceedings of ECIR (2022)\n",
      "32. Pradeep, R., Nogueira, R., Lin, J.: The expando-mono-duo design pattern for text\n",
      "ranking with pretrained sequence-to-sequence models. ArXiv abs/2101.05667\n",
      "(2021)\n",
      "33. Raﬀel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y.,\n",
      "Li, W., Liu, P.J., et al.: Exploring the limits of transfer learning with a uniﬁed\n",
      "text-to-text transformer. J. Mach. Learn. Res. 21(140) (2020)\n",
      "34. Scells, H., Zhuang, S., Zuccon, G.: Reduce, reuse, recycle: Green information re-\n",
      "trieval research. In: Proceedings of SIGIR (2022)\n",
      "35. Tao, T., Wang, X., Mei, Q., Zhai, C.: Language model information retrieval with\n",
      "document expansion. In: Proceedings of HLT-NAACL (2006)\n",
      "36. Wang, X., MacAvaney, S., Macdonald, C., Ounis, I.: An inspection of the repro-\n",
      "ducibility and replicability of TCT-ColBERT. In: Proceedings of SIGIR (2022)\n",
      "37. Xiong, L., Xiong, C., Li, Y., Tang, K.F., Liu, J., Bennett, P.N., Ahmed, J., Over-\n",
      "wijk,A.:Approximatenearestneighbornegativecontrastivelearningfordensetext\n",
      "retrieval. In: Proceedings of ICLR (2021)\n",
      "38. Yu, S.Y., Liu, J., Yang, J., Xiong, C., Bennett, P.N., Gao, J., Liu, Z.: Few-shot\n",
      "generative conversational query rewriting. In: Proceedings of SIGIR (2020)\n",
      "39. Zhao, T., Lu, X., Lee, K.: SPARTA: Eﬃcient open-domain question answering via\n",
      "sparse transformer matching retrieval. arXiv abs/2009.13013 (2020)\n",
      "40. Zhuang, S., Zuccon, G.: TILDE: Term independent likelihood model for passage\n",
      "re-ranking. In: Proceedings of SIGIR (2021)\n",
      "-----------------------------------\n",
      "Extracted text from 'Reference4.pdf':\n",
      "Query2doc: Query Expansion with Large Language Models\n",
      "Liang Wang and Nan Yang and Furu Wei\n",
      "Microsoft Research\n",
      "{wangliang,nanya,fuwei}@microsoft.com\n",
      "Abstract\n",
      "This paper introduces a simple yet effec-\n",
      "tive query expansion approach, denoted as\n",
      "query2doc , to improve both sparse and dense\n",
      "retrieval systems. The proposed method\n",
      "ﬁrst generates pseudo-documents by few-shot\n",
      "prompting large language models (LLMs), and\n",
      "then expands the query with generated pseudo-\n",
      "documents. LLMs are trained on web-scale\n",
      "text corpora and are adept at knowledge mem-\n",
      "orization. The pseudo-documents from LLMs\n",
      "often contain highly relevant information that\n",
      "can aid in query disambiguation and guide\n",
      "the retrievers. Experimental results demon-\n",
      "strate that query2doc boosts the performance\n",
      "of BM25 by 3% to 15% on ad-hoc IR datasets,\n",
      "such as MS-MARCO and TREC DL, with-\n",
      "out any model ﬁne-tuning. Furthermore, our\n",
      "method also beneﬁts state-of-the-art dense re-\n",
      "trievers in terms of both in-domain and out-of-\n",
      "domain results.\n",
      "1 Introduction\n",
      "Information retrieval (IR) aims to locate relevant\n",
      "documents from a large corpus given a user is-\n",
      "sued query. It is a core component in modern\n",
      "search engines and researchers have invested for\n",
      "decades in this ﬁeld. There are two mainstream\n",
      "paradigms for IR: lexical-based sparse retrieval,\n",
      "such as BM25, and embedding-based dense re-\n",
      "trieval (Xiong et al., 2021; Qu et al., 2021). Al-\n",
      "though dense retrievers perform better when large\n",
      "amounts of labeled data are available (Karpukhin\n",
      "et al., 2020), BM25 remains competitive on out-of-\n",
      "domain datasets (Thakur et al., 2021).\n",
      "Query expansion (Rocchio, 1971; Lavrenko\n",
      "and Croft, 2001) is a long-standing technique\n",
      "that rewrites the query based on pseudo-relevance\n",
      "feedback or external knowledge sources such as\n",
      "WordNet. For sparse retrieval, it can help bridge\n",
      "the lexical gap between the query and the docu-\n",
      "ments. However, query expansion methods like\n",
      "RM3 (Lavrenko and Croft, 2001; Lv and Zhai,2009) have only shown limited success on popular\n",
      "datasets (Campos et al., 2016), and most state-of-\n",
      "the-art dense retrievers do not adopt this technique.\n",
      "In the meantime, document expansion methods like\n",
      "doc2query (Nogueira et al., 2019) have proven to\n",
      "be effective for sparse retrieval.\n",
      "In this paper, we demonstrate the effectiveness\n",
      "of LLMs (Brown et al., 2020) as query expan-\n",
      "sion models by generating pseudo-documents con-\n",
      "ditioned on few-shot prompts. Given that search\n",
      "queries are often short, ambiguous, or lack neces-\n",
      "sary background information, LLMs can provide\n",
      "relevant information to guide retrieval systems, as\n",
      "they memorize an enormous amount of knowledge\n",
      "and language patterns by pre-training on trillions\n",
      "of tokens.\n",
      "Our proposed method, called query2doc , gen-\n",
      "erates pseudo-documents by few-shot prompting\n",
      "LLMs and concatenates them with the original\n",
      "query to form a new query. This method is simple\n",
      "to implement and does not require any changes in\n",
      "training pipelines or model architectures, making it\n",
      "orthogonal to the progress in the ﬁeld of LLMs and\n",
      "information retrieval. Future methods can easily\n",
      "build upon our query expansion framework.\n",
      "For in-domain evaluation, we adopt the MS-\n",
      "MARCO passage ranking (Campos et al., 2016),\n",
      "TREC DL 2019 and 2020 datasets. Pseudo-\n",
      "documents are generated by prompting an im-\n",
      "proved version of GPT-3 text-davinci-003 from\n",
      "OpenAI (Brown et al., 2020). Results show that\n",
      "query2doc substantially improves the off-the-shelf\n",
      "BM25 algorithm without ﬁne-tuning any model,\n",
      "particularly for hard queries from the TREC DL\n",
      "track. Strong dense retrievers, including DPR\n",
      "(Karpukhin et al., 2020), SimLM (Wang et al.,\n",
      "2022a), and E5 (Wang et al., 2022b) also bene-\n",
      "ﬁt from query2doc , although the gains tend to be\n",
      "diminishing when distilling from a strong cross-\n",
      "encoder based re-ranker. Experiments in zero-shot\n",
      "OOD settings demonstrate that our method out-arXiv:2303.07678v1  [cs.IR]  14 Mar 2023performs strong baselines on most datasets. Fur-\n",
      "ther analysis also reveals the importance of model\n",
      "scales: query2doc works best when combined with\n",
      "the most capable LLMs while small language mod-\n",
      "els only provide marginal improvements over base-\n",
      "lines.\n",
      "To aid reproduction, we release all\n",
      "the generations from text-davinci-003\n",
      "at https://huggingface.co/datasets/\n",
      "intfloat/query2doc_msmarco .\n",
      "2 Method\n",
      "Write a passage that answers the given query:\n",
      "Query: what state is this zip code 85282\n",
      "Passage: Welcome to TEMPE, AZ 85282. \n",
      "85282 is a rural zip code in Tempe, Arizona. \n",
      "The population is primarily white…\n",
      "…\n",
      "Query: when was pokemon green released\n",
      "Passage:LLM Prompts\n",
      "Pokemon Green was released in Japan on \n",
      "February 27th, 1996. It was the first in the \n",
      "Pokemon series of games and served as the \n",
      "basis for Pokemon Red and Blue, which were \n",
      "released in the US in 1998. The original \n",
      "Pokemon Green remains a beloved classic \n",
      "among fans of the series.LLM Output\n",
      "Figure 1: Illustration of query2doc few-shot prompting.\n",
      "We omit some in-context examples for space reasons.\n",
      "Given a query q, we employ few-shot prompting\n",
      "to generate a pseudo-document d0as depicted in\n",
      "Figure 1. The prompt comprises a brief instruction\n",
      "“Write a passage that answers the given query:”\n",
      "andklabeled pairs randomly sampled from a\n",
      "training set. We use k= 4throughout this paper.\n",
      "Subsequently, we rewrite qto a new query q+\n",
      "by concatenating with the pseudo-document d0.\n",
      "There are slight differences in the concatenation\n",
      "operation for sparse and dense retrievers, which\n",
      "we elaborate on in the following section.\n",
      "Sparse Retrieval Since the query qis typically\n",
      "much shorter than pseudo-documents, we boost the\n",
      "query term weights by repeating the query ntimes\n",
      "before concatenating with the pseudo-document d0:q+=concat(fqg\u0002n; d0) (1)\n",
      "Here, “concat” denotes the string concatenation\n",
      "function. q+is used as the new query for\n",
      "BM25 retrieval. We ﬁnd that n= 5 is a gener-\n",
      "ally good value and do not tune it on a dataset basis.\n",
      "Dense Retrieval The new query q+is a sim-\n",
      "ple concatenation of the original query qand the\n",
      "pseudo-document d0separated by [SEP]:\n",
      "q+=concat(q;[SEP]; d0) (2)\n",
      "For training dense retrievers, several factors can\n",
      "inﬂuence the ﬁnal performance, such as hard nega-\n",
      "tive mining (Xiong et al., 2021), intermediate pre-\n",
      "training (Gao and Callan, 2021), and knowledge\n",
      "distillation from a cross-encoder based re-ranker\n",
      "(Qu et al., 2021). In this paper, we investigate two\n",
      "settings to gain a more comprehensive understand-\n",
      "ing of our method. The ﬁrst setting is training DPR\n",
      "(Karpukhin et al., 2020) models initialized from\n",
      "BERT basewith BM25 hard negatives only. The op-\n",
      "timization objective is a standard contrastive loss:\n",
      "Lcont=\u0000logehq\u0001hd\n",
      "ehq\u0001hd+P\n",
      "di2Nehq\u0001hdi(3)\n",
      "where hqandhdrepresent the embeddings for the\n",
      "query and document, respectively. Ndenotes the\n",
      "set of hard negatives.\n",
      "The second setting is to build upon state-of-the-\n",
      "art dense retrievers and use KL divergence to distill\n",
      "from a cross-encoder teacher model.\n",
      "minDKL(pce;pstu) +\u000b",
      "Lcont (4)\n",
      "pceandpstuare the probabilities from the cross-\n",
      "encoder and our student model, respectively. \u000b",
      "is\n",
      "a coefﬁcient to balance the distillation loss and\n",
      "contrastive loss.\n",
      "Comparison with Pseudo-relevance Feedback\n",
      "Our proposed method can be viewed as a variant\n",
      "of pseudo-relevance feedback (PRF) (Lavrenko\n",
      "and Croft, 2001; Lv and Zhai, 2009). In conven-\n",
      "tional PRF, the feedback signals for query expan-\n",
      "sion come from the top-k documents obtained in\n",
      "the initial retrieval step, while our method prompts\n",
      "LLMs to generate pseudo-documents. Our method\n",
      "does not rely on the quality of the initial retrieval re-\n",
      "sults, which are often noisy or irrelevant. Rather, it\n",
      "exploits cutting-edge LLMs to generate documents\n",
      "that are more likely to contain relevant terms.Method Fine-tuningMS MARCO dev TREC DL 19 TREC DL 20\n",
      "MRR@10 R@50 R@1k nDCG@10 nDCG@10\n",
      "Sparse retrieval\n",
      "BM25 7 18.4 58.5 85.7 51.2\u000347.7\u0003\n",
      "+ query2doc 7 21.4+3.065.3+6.891.8+6.166.2+15.062.9+15.2\n",
      "BM25 + RM3 7 15.8 56.7 86.4 52.2 47.4\n",
      "docT5query (Nogueira and Lin) 3 27.7 75.6 94.7 64.2 -\n",
      "Dense retrieval w/o distillation\n",
      "ANCE (Xiong et al., 2021) 3 33.0 - 95.9 64.5 64.6\n",
      "HyDE (Gao et al., 2022) 7 - - - 61.3 57.9\n",
      "DPR bert-base (our impl.) 3 33.7 80.5 95.9 64.7 64.1\n",
      "+ query2doc 3 35.1+1.482.6+2.197.2+1.368.7+4.067.1+3.0\n",
      "Dense retrieval w/ distillation\n",
      "RocketQAv2 (Ren et al., 2021) 3 38.8 86.2 98.1 - -\n",
      "AR2 (Zhang et al., 2021) 3 39.5 87.8 98.6 - -\n",
      "SimLM (Wang et al., 2022a) 3 41.1 87.8 98.7 71.4 69.7\n",
      "+ query2doc 3 41.5+0.488.0+0.298.8+0.172.9+1.571.6+1.9\n",
      "E5base+ KD (Wang et al., 2022b) 3 40.7 87.6 98.6 74.3 70.7\n",
      "+ query2doc 3 41.5+0.888.1+0.598.7+0.174.9+0.672.5+1.8\n",
      "Table 1: Main results on the MS-MARCO passage ranking and TREC datasets. The “Fine-tuning” column indi-\n",
      "cates whether the method requires ﬁne-tuning model on labeled data or not. \u0003: our reproduction.\n",
      "3 Experiments\n",
      "3.1 Setup\n",
      "Evaluation Datasets For in-domain evaluation,\n",
      "we utilize the MS-MARCO passage ranking (Cam-\n",
      "pos et al., 2016), TREC DL 2019 (Craswell et al.,\n",
      "2020a) and 2020 (Craswell et al., 2020b) datasets.\n",
      "For zero-shot out-of-domain evaluation, we select\n",
      "ﬁve low-resource datasets from the BEIR bench-\n",
      "mark (Thakur et al., 2021). The evaluation met-\n",
      "rics include MRR@10, R@k ( k2f50;1kg), and\n",
      "nDCG@10.\n",
      "Hyperparameters For sparse retrieval including\n",
      "BM25 and RM3, we adopt the default implementa-\n",
      "tion from Pyserini (Lin et al., 2021). When training\n",
      "dense retrievers, we use mostly the same hyper-\n",
      "parameters as SimLM (Wang et al., 2022a), with\n",
      "the exception of increasing the maximum query\n",
      "length to 144to include pseudo-documents. When\n",
      "prompting LLMs, we include 4in-context exam-\n",
      "ples and use the default temperature of 1to sample\n",
      "at most 128tokens. For further details, please refer\n",
      "to Appendix A.\n",
      "3.2 Main Results\n",
      "In Table 1, we list the results on the MS-MARCO\n",
      "passage ranking and TREC DL datasets. For sparse\n",
      "retrieval, “BM25 + query2doc” beats the BM25\n",
      "baseline with over 15%improvements on TREC\n",
      "DL 2019 and 2020 datasets. Our manual inspection\n",
      "reveals that most queries from the TREC DL trackare long-tailed entity-centric queries, which beneﬁt\n",
      "more from the exact lexical match. The traditional\n",
      "query expansion method RM3 only marginally\n",
      "improves the R@1k metric. Although the docu-\n",
      "ment expansion method docT5query achieves bet-\n",
      "ter numbers on the MS-MARCO dev set, it requires\n",
      "training a T5-based query generator with all the\n",
      "available labeled data, while “BM25 + query2doc”\n",
      "does not require any model ﬁne-tuning.\n",
      "For dense retrieval, the model variants that com-\n",
      "bine with query2doc also outperform the corre-\n",
      "sponding baselines on all metrics. However, the\n",
      "gain brought by query2doc tends to diminish when\n",
      "using intermediate pre-training or knowledge distil-\n",
      "lation from cross-encoder re-rankers, as shown by\n",
      "the “SimLM + query2doc” and “E5 + query2doc”\n",
      "results.\n",
      "For zero-shot out-of-domain retrieval, the results\n",
      "are mixed as shown in Table 2. Entity-centric\n",
      "datasets like DBpedia see the largest improvements.\n",
      "On the NFCorpus and Scifact datasets, we observe\n",
      "a minor decrease in ranking quality. This is likely\n",
      "due to the distribution mismatch between training\n",
      "and evaluation.\n",
      "4 Analysis\n",
      "Scaling up LLMs is Critical For our proposed\n",
      "method, a question that naturally arises is: how\n",
      "does the model scale affect the quality of query\n",
      "expansion? Table 3 shows that the performance\n",
      "steadily improves as we go from the 1.3B modelDBpedia NFCorpus Scifact Trec-Covid Touche2020\n",
      "BM25 31.3 32.5 66.5 65.6 36.7\n",
      "+ query2doc 37.0+5.734.9+2.468.6+2.172.2+6.639.8+3.1\n",
      "SimLM (Wang et al., 2022a) 34.9 32.7 62.4 55.0 18.9\n",
      "+ query2doc 38.3+3.432.1-0.659.5-2.959.9+4.925.6+6.7\n",
      "E5base+ KD (Wang et al., 2022b) 40.7 35.0 70.4 74.1 30.9\n",
      "+ query2doc 42.4+1.735.2+0.267.5-2.975.1+1.031.7+0.8\n",
      "Table 2: Zero-shot out-of-domain results on 5 low-resource datasets from the BEIR benchmark (Thakur et al.,\n",
      "2021). The reported numbers are nDCG@10. For a fair comparison, the in-context examples for prompting LLMs\n",
      "come from the MS-MARCO training set.\n",
      "# params TREC 19 TREC 20\n",
      "BM25 - 51.2 47.7\n",
      "w/ babbage 1.3B 52.0 50.2\n",
      "w/ curie 6.7B 55.1 50.1\n",
      "w/ davinci-001 175B 63.5 58.2\n",
      "w/ davinci-003 175B 66.2 62.9\n",
      "Table 3: Query expansion with different model sizes.\n",
      "to 175B models. Empirically, the texts generated\n",
      "by smaller language models tend to be shorter and\n",
      "contain more factual errors. Also, the “davinci-003”\n",
      "model outperforms its earlier version “davinci-001”\n",
      "by using better training data and improved\n",
      "instruction tuning.\n",
      "1 10 30 50 100\n",
      "% labeled data for fine-tuning202224262830323436MRR on dev set\n",
      "21.427.331.432.833.7\n",
      "22.728.532.134.135.1\n",
      "DPR w/o query2doc\n",
      "DPR w/ query2doc\n",
      "Figure 2: MRR on MS-MARCO dev set w.r.t the per-\n",
      "centage of labeled data used for ﬁne-tuning.\n",
      "Performance Gains are Consistent across Data\n",
      "Scales Figure 2 presents a comparison between\n",
      "two variants of DPR models, which differ in the\n",
      "amount of labeled data used. The results show\n",
      "that the “DPR + query2doc” variant consistently\n",
      "outperforms the DPR baseline by approximately\n",
      "1%, regardless of the amount of data used for\n",
      "ﬁne-tuning. This observation highlights that ourcontribution is orthogonal to the continual scaling\n",
      "up of supervision signals.\n",
      "TREC 19 TREC 20\n",
      "BM25 + query2doc 66.2 62.9\n",
      "w/ query only 51.2 47.7\n",
      "w/ pseudo-doc only 48.7 44.5\n",
      "Table 4: Using the concatenation of the original query\n",
      "and the generated pseudo-documents perform substan-\n",
      "tially better.\n",
      "How to Use Pseudo-documents In this paper,\n",
      "we concatenate the original query and pseudo-\n",
      "documents as the new query. Alternatively, one can\n",
      "solely use the pseudo-documents, as done in the\n",
      "approach of HyDE (Gao et al., 2022). The results\n",
      "presented in Table 4 demonstrate that the original\n",
      "query and pseudo-documents are complementary,\n",
      "and their combination leads to substantially better\n",
      "performance in sparse retrieval.\n",
      "Case Analysis In Table 5, we show two queries\n",
      "along with their respective pseudo-documents and\n",
      "groundtruth. The pseudo-documents, which are\n",
      "generated by LLMs, offer detailed and mostly ac-\n",
      "curate information, thereby reducing the lexical\n",
      "mismatch between the query and documents. In\n",
      "some cases, the pseudo-documents are sufﬁcient to\n",
      "meet the user’s information needs, rendering the\n",
      "retrieval step unnecessary. However, it is worth\n",
      "noting that the LLM generations may contain fac-\n",
      "tual errors. For instance, in the second query, the\n",
      "theme song \"It’s a Jungle Out There\" was used as\n",
      "of season two in 2003, not 20021. Although such\n",
      "errors may appear subtle and difﬁcult to verify, they\n",
      "pose a signiﬁcant challenge to building trustworthy\n",
      "systems using LLMs.\n",
      "1Refer to https://en.wikipedia.org/wiki/It’s_a_\n",
      "Jungle_Out_There_(song)query who killed nicholas ii of russia\n",
      "LLM generation Nicholas II of Russia, the last Tsar of Russia, was executed by Bolshevik revolutionaries on\n",
      "July 17th, 1918 , along with his wife Alexandra and their ﬁve children . The execution was\n",
      "ordered by Vladimir Lenin , the leader of the Bolshevik Party. The actual execution was carried\n",
      "out by a ﬁring squad of Bolshevik troops, and the bodies were later buried in a mass grave. . .\n",
      "Groundtruth (1868- 1918 ). Nicholas II was the lastemperor, or tsar, of Russia, serving from 1894 to\n",
      "1917. Nicholas, his wife, and their ﬁve children were killed by the Bolsheviks , members of a\n",
      "revolutionary group (led by Lenin ) who seized control of the government in Russia during\n",
      "the October Revolution (see Russian Revolution) of 1917.\n",
      "query who sings monk theme song\n",
      "LLM generation The theme song for the television show Monk is entitled \" It’s a Jungle Out There \" and is\n",
      "sung by American singer-songwriter Randy Newman . The song was written speciﬁcally for\n",
      "the show, and it has been used as the theme song since the series premiered in 2002 . It has\n",
      "been praised by critics and fans alike and is often regarded as one of the best theme songs in\n",
      "television history.\n",
      "Groundtruth exists and is an alternate of. The Monk theme song is It’s a Jungle Out There byRandy\n",
      "Newman . The Monk theme song is It’s a Jungle Out There byRandy Newman .\n",
      "Table 5: Examples from the TREC DL 2020 dataset. Bold texts are the overlapping words between groundtruth\n",
      "and pseudo-documents generated from LLMs. The italicized red sentence demonstrates a factual error in language\n",
      "model generations.\n",
      "5 Related Work\n",
      "Query Expansion and Document Expansion\n",
      "are two classical techniques to improve retrieval\n",
      "quality, particularly for sparse retrieval systems.\n",
      "Both techniques aim to minimize the lexical gap be-\n",
      "tween the query and the documents. Query expan-\n",
      "sion typically involves rewriting the query based\n",
      "on relevance feedback (Lavrenko and Croft, 2001;\n",
      "Rocchio, 1971) or lexical resources such as Word-\n",
      "Net (Miller, 1992). In cases where labeled rele-\n",
      "vance feedback is not available, the top-k retrieved\n",
      "documents can serve as pseudo-relevance feedback\n",
      "signals (Lv and Zhai, 2009).\n",
      "In contrast, document expansion enriches the\n",
      "document representation by appending additional\n",
      "relevant terms. Doc2query (Nogueira et al., 2019)\n",
      "trains a seq2seq model to predict pseudo-queries\n",
      "based on documents and then adds generated\n",
      "pseudo-queries to the document index. Learned\n",
      "sparse retrieval models such as SPLADE (Formal\n",
      "et al., 2021) and uniCOIL (Lin and Ma, 2021)\n",
      "also learn document term weighting in an end-to-\n",
      "end fashion. However, most state-of-the-art dense\n",
      "retrievers (Ren et al., 2021; Wang et al., 2022a)\n",
      "do not adopt any expansion techniques. Our pa-\n",
      "per demonstrates that strong dense retrievers also\n",
      "beneﬁt from query expansion using LLMs.\n",
      "Large Language Models (LLMs) such as GPT-3\n",
      "(Brown et al., 2020), PaLM (Chowdhery et al.,\n",
      "2022), and LLaMA (Touvron et al., 2023) are\n",
      "trained on trillions of tokens with billions of param-\n",
      "eters, exhibiting unparalleled generalization abilityacross various tasks. LLMs can follow instruc-\n",
      "tions in a zero-shot manner or conduct in-context\n",
      "learning through few-shot prompting. Labeling a\n",
      "few high-quality examples only requires minimal\n",
      "human effort. In this paper, we employ few-shot\n",
      "prompting to generate pseudo-documents from a\n",
      "given query. A closely related recent work HyDE\n",
      "(Gao et al., 2022) instead focuses on the zero-\n",
      "shot setting and uses embeddings of the pseudo-\n",
      "documents for similarity search. HyDE implicitly\n",
      "assumes that the groundtruth document and pseudo-\n",
      "documents express the same semantics in different\n",
      "words, which may not hold for some queries. In the\n",
      "ﬁeld of question answering, RECITE (Sun et al.,\n",
      "2022) and GENREAD (Yu et al., 2022) demon-\n",
      "strate that LLMs are powerful context generators\n",
      "and can encode abundant factual knowledge. How-\n",
      "ever, as our analysis shows, LLMs can sometimes\n",
      "generate false claims, hindering their practical ap-\n",
      "plication in critical areas.\n",
      "6 Conclusion\n",
      "This paper presents a simple method query2doc\n",
      "to leverage LLMs for query expansion. It ﬁrst\n",
      "prompts LLMs with few-shot examples to gener-\n",
      "ate pseudo-documents and then integrates with ex-\n",
      "isting sparse or dense retrievers by augmenting\n",
      "queries with generated pseudo-documents. The un-\n",
      "derlying motivation is to distill the LLMs through\n",
      "prompting. Despite its simplicity, empirical evalua-\n",
      "tions demonstrate consistent improvements across\n",
      "various retrieval models and datasets.Limitations\n",
      "LLM call Index search\n",
      "BM25 - 16ms\n",
      "+ query2doc >2000ms 177ms\n",
      "Table 6: Latency analysis for retrieval systems with our\n",
      "proposed query2doc. We retrieve the top 100 results for\n",
      "MS-MARCO dev queries with a single thread and then\n",
      "average over all the queries. The latency for LLM API\n",
      "calls depends on server load and is difﬁcult to precisely\n",
      "measure.\n",
      "An apparent limitation is the efﬁciency of re-\n",
      "trieval. Our method requires running inference with\n",
      "LLMs which can be considerably slower due to the\n",
      "token-by-token autoregressive decoding. Moreover,\n",
      "with query2doc, searching the inverted index also\n",
      "becomes slower as the number of query terms in-\n",
      "creases after expansion. This is supported by the\n",
      "benchmarking results in Table 6. Real-world de-\n",
      "ployment of our method should take these factors\n",
      "into consideration.\n",
      "References\n",
      "Alexander Bondarenko, Maik Fröbe, Johannes Kiesel,\n",
      "Shahbaz Syed, Timon Gurcke, Meriem Beloucif,\n",
      "Alexander Panchenko, Chris Biemann, Benno Stein,\n",
      "Henning Wachsmuth, et al. 2022. Overview of\n",
      "touché 2022: argument retrieval. In Interna-\n",
      "tional Conference of the Cross-Language Evalua-\n",
      "tion Forum for European Languages , pages 311–\n",
      "336. Springer.\n",
      "Vera Boteva, Demian Gholipour, Artem Sokolov, and\n",
      "Stefan Riezler. 2016. A full-text learning to rank\n",
      "dataset for medical information retrieval. In Euro-\n",
      "pean Conference on Information Retrieval , pages\n",
      "716–722. Springer.\n",
      "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie\n",
      "Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\n",
      "Neelakantan, Pranav Shyam, Girish Sastry, Amanda\n",
      "Askell, Sandhini Agarwal, Ariel Herbert-V oss,\n",
      "Gretchen Krueger, Tom Henighan, Rewon Child,\n",
      "Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\n",
      "Clemens Winter, Christopher Hesse, Mark Chen,\n",
      "Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin\n",
      "Chess, Jack Clark, Christopher Berner, Sam Mc-\n",
      "Candlish, Alec Radford, Ilya Sutskever, and Dario\n",
      "Amodei. 2020. Language models are few-shot learn-\n",
      "ers. In Advances in Neural Information Processing\n",
      "Systems 33: Annual Conference on Neural Informa-\n",
      "tion Processing Systems 2020, NeurIPS 2020, De-\n",
      "cember 6-12, 2020, virtual .\n",
      "Daniel Fernando Campos, Tri Nguyen, Mir Rosenberg,\n",
      "Xia Song, Jianfeng Gao, Saurabh Tiwary, RanganMajumder, Li Deng, and Bhaskar Mitra. 2016. Ms\n",
      "marco: A human generated machine reading com-\n",
      "prehension dataset. ArXiv , abs/1611.09268.\n",
      "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,\n",
      "Maarten Bosma, Gaurav Mishra, Adam Roberts,\n",
      "Paul Barham, Hyung Won Chung, Charles Sutton,\n",
      "Sebastian Gehrmann, Parker Schuh, Kensen Shi,\n",
      "Sasha Tsvyashchenko, Joshua Maynez, Abhishek\n",
      "Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vin-\n",
      "odkumar Prabhakaran, Emily Reif, Nan Du, Ben-\n",
      "ton C. Hutchinson, Reiner Pope, James Bradbury, Ja-\n",
      "cob Austin, Michael Isard, Guy Gur-Ari, Pengcheng\n",
      "Yin, Toju Duke, Anselm Levskaya, Sanjay Ghe-\n",
      "mawat, Sunipa Dev, Henryk Michalewski, Xavier\n",
      "García, Vedant Misra, Kevin Robinson, Liam Fe-\n",
      "dus, Denny Zhou, Daphne Ippolito, David Luan,\n",
      "Hyeontaek Lim, Barret Zoph, Alexander Spiridonov,\n",
      "Ryan Sepassi, David Dohan, Shivani Agrawal, Mark\n",
      "Omernick, Andrew M. Dai, Thanumalayan Sankara-\n",
      "narayana Pillai, Marie Pellat, Aitor Lewkowycz,\n",
      "Erica Moreira, Rewon Child, Oleksandr Polozov,\n",
      "Katherine Lee, Zongwei Zhou, Xuezhi Wang, Bren-\n",
      "nan Saeta, Mark Díaz, Orhan Firat, Michele Catasta,\n",
      "Jason Wei, Kathleen S. Meier-Hellstern, Douglas\n",
      "Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022.\n",
      "Palm: Scaling language modeling with pathways.\n",
      "ArXiv , abs/2204.02311.\n",
      "Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel\n",
      "Campos, and Ellen M V oorhees. 2020a. Overview\n",
      "of the trec 2019 deep learning track. ArXiv preprint ,\n",
      "abs/2003.07820.\n",
      "Nick Craswell, Bhaskar Mitra, Emine Yilmaz,\n",
      "Daniel Fernando Campos, and Ellen M. V oorhees.\n",
      "2020b. Overview of the trec 2020 deep learning\n",
      "track. ArXiv , abs/2003.07820.\n",
      "Thibault Formal, Benjamin Piwowarski, and Stéphane\n",
      "Clinchant. 2021. Splade: Sparse lexical and expan-\n",
      "sion model for ﬁrst stage ranking. Proceedings of\n",
      "the 44th International ACM SIGIR Conference on\n",
      "Research and Development in Information Retrieval .\n",
      "Luyu Gao and Jamie Callan. 2021. Condenser: a pre-\n",
      "training architecture for dense retrieval. In Proceed-\n",
      "ings of the 2021 Conference on Empirical Methods\n",
      "in Natural Language Processing , pages 981–993,\n",
      "Online and Punta Cana, Dominican Republic. Asso-\n",
      "ciation for Computational Linguistics.\n",
      "Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie\n",
      "Callan. 2022. Precise zero-shot dense retrieval with-\n",
      "out relevance labels. ArXiv , abs/2212.10496.\n",
      "Faegheh Hasibi, Fedor Nikolaev, Chenyan Xiong,\n",
      "Krisztian Balog, Svein Erik Bratsberg, Alexander\n",
      "Kotov, and Jamie Callan. 2017. Dbpedia-entity v2:\n",
      "A test collection for entity search. In Proceedings\n",
      "of the 40th International ACM SIGIR Conference on\n",
      "Research and Development in Information Retrieval,\n",
      "Shinjuku, Tokyo, Japan, August 7-11, 2017 , pages\n",
      "1265–1268. ACM.Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\n",
      "Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\n",
      "Wen-tau Yih. 2020. Dense passage retrieval for\n",
      "open-domain question answering. In Proceedings of\n",
      "the 2020 Conference on Empirical Methods in Nat-\n",
      "ural Language Processing (EMNLP) , pages 6769–\n",
      "6781, Online. Association for Computational Lin-\n",
      "guistics.\n",
      "Victor Lavrenko and W. Bruce Croft. 2001. Relevance-\n",
      "based language models. ACM SIGIR Forum , 51:260\n",
      "– 267.\n",
      "Jimmy J. Lin and Xueguang Ma. 2021. A few brief\n",
      "notes on deepimpact, coil, and a conceptual frame-\n",
      "work for information retrieval techniques. ArXiv ,\n",
      "abs/2106.14807.\n",
      "Jimmy J. Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-\n",
      "Hong Yang, Ronak Pradeep, Rodrigo Nogueira, and\n",
      "David R. Cheriton. 2021. Pyserini: A python toolkit\n",
      "for reproducible information retrieval research with\n",
      "sparse and dense representations. Proceedings of the\n",
      "44th International ACM SIGIR Conference on Re-\n",
      "search and Development in Information Retrieval .\n",
      "Yuanhua Lv and ChengXiang Zhai. 2009. A compara-\n",
      "tive study of methods for estimating query language\n",
      "models with pseudo feedback. Proceedings of the\n",
      "18th ACM conference on Information and knowl-\n",
      "edge management .\n",
      "George A. Miller. 1992. WordNet: A lexical database\n",
      "for English. In Speech and Natural Language: Pro-\n",
      "ceedings of a Workshop Held at Harriman, New\n",
      "York, February 23-26, 1992 .\n",
      "Rodrigo Nogueira and Jimmy Lin. From doc2query to\n",
      "doctttttquery.\n",
      "Rodrigo Nogueira, Wei Yang, Jimmy J. Lin, and\n",
      "Kyunghyun Cho. 2019. Document expansion by\n",
      "query prediction. ArXiv , abs/1904.08375.\n",
      "Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang\n",
      "Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu,\n",
      "and Haifeng Wang. 2021. RocketQA: An opti-\n",
      "mized training approach to dense passage retrieval\n",
      "for open-domain question answering. In Proceed-\n",
      "ings of the 2021 Conference of the North Ameri-\n",
      "can Chapter of the Association for Computational\n",
      "Linguistics: Human Language Technologies , pages\n",
      "5835–5847, Online. Association for Computational\n",
      "Linguistics.\n",
      "Ruiyang Ren, Yingqi Qu, Jing Liu, Wayne Xin Zhao,\n",
      "QiaoQiao She, Hua Wu, Haifeng Wang, and Ji-Rong\n",
      "Wen. 2021. RocketQAv2: A joint training method\n",
      "for dense passage retrieval and passage re-ranking.\n",
      "InProceedings of the 2021 Conference on Empiri-\n",
      "cal Methods in Natural Language Processing , pages\n",
      "2825–2835, Online and Punta Cana, Dominican Re-\n",
      "public. Association for Computational Linguistics.\n",
      "J. J. Rocchio. 1971. Relevance feedback in information\n",
      "retrieval.Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and\n",
      "Denny Zhou. 2022. Recitation-augmented language\n",
      "models. ArXiv , abs/2210.01296.\n",
      "Nandan Thakur, Nils Reimers, Andreas Rücklé, Ab-\n",
      "hishek Srivastava, and Iryna Gurevych. 2021. Beir:\n",
      "A heterogeneous benchmark for zero-shot evalua-\n",
      "tion of information retrieval models. In Thirty-ﬁfth\n",
      "Conference on Neural Information Processing Sys-\n",
      "tems Datasets and Benchmarks Track (Round 2) .\n",
      "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\n",
      "Martinet, Marie-Anne Lachaux, Timothée Lacroix,\n",
      "Baptiste Rozière, Naman Goyal, Eric Hambro,\n",
      "Faisal Azhar, Aur’elien Rodriguez, Armand Joulin,\n",
      "Edouard Grave, and Guillaume Lample. 2023.\n",
      "Llama: Open and efﬁcient foundation language mod-\n",
      "els.ArXiv , abs/2302.13971.\n",
      "Ellen V oorhees, Tasmeer Alam, Steven Bedrick, Dina\n",
      "Demner-Fushman, William R Hersh, Kyle Lo, Kirk\n",
      "Roberts, Ian Soboroff, and Lucy Lu Wang. 2021.\n",
      "Trec-covid: constructing a pandemic information re-\n",
      "trieval test collection. In ACM SIGIR Forum , vol-\n",
      "ume 54, pages 1–12. ACM New York, NY , USA.\n",
      "David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu\n",
      "Wang, Madeleine van Zuylen, Arman Cohan, and\n",
      "Hannaneh Hajishirzi. 2020. Fact or ﬁction: Verify-\n",
      "ing scientiﬁc claims. In Proceedings of the 2020\n",
      "Conference on Empirical Methods in Natural Lan-\n",
      "guage Processing (EMNLP) , pages 7534–7550, On-\n",
      "line. Association for Computational Linguistics.\n",
      "Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao,\n",
      "Linjun Yang, Daxin Jiang, Rangan Majumder, and\n",
      "Furu Wei. 2022a. Simlm: Pre-training with rep-\n",
      "resentation bottleneck for dense passage retrieval.\n",
      "ArXiv , abs/2207.02578.\n",
      "Liang Wang, Nan Yang, Xiaolong Huang, Binx-\n",
      "ing Jiao, Linjun Yang, Daxin Jiang, Rangan Ma-\n",
      "jumder, and Furu Wei. 2022b. Text embeddings by\n",
      "weakly-supervised contrastive pre-training. ArXiv ,\n",
      "abs/2212.03533.\n",
      "Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,\n",
      "Jialin Liu, Paul N. Bennett, Junaid Ahmed, and\n",
      "Arnold Overwijk. 2021. Approximate nearest neigh-\n",
      "bor negative contrastive learning for dense text re-\n",
      "trieval. In 9th International Conference on Learning\n",
      "Representations, ICLR 2021, Virtual Event, Austria,\n",
      "May 3-7, 2021 . OpenReview.net.\n",
      "W. Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingx-\n",
      "uan Ju, Soumya Sanyal, Chenguang Zhu, Michael\n",
      "Zeng, and Meng Jiang. 2022. Generate rather than\n",
      "retrieve: Large language models are strong context\n",
      "generators. ArXiv , abs/2209.10063.\n",
      "Hang Zhang, Yeyun Gong, Yelong Shen, Jiancheng\n",
      "Lv, Nan Duan, and Weizhu Chen. 2021. Adversar-\n",
      "ial retriever-ranker for dense text retrieval. ArXiv\n",
      "preprint , abs/2110.03611.query who sings hey good looking\n",
      "LLM generation Hey Good Lookin’ was originally written and recorded by the country music singer Hank\n",
      "Williams in1951 . The song has since been covered by several other artists and bands,\n",
      "including Jimmy Buffett, Conway Twitty, and Willie Nelson.\n",
      "Groundtruth Hey, Good Lookin’. (1951) Hey, Good Lookin’ is a 1951 song written and recorded byHank\n",
      "Williams , and his version was inducted into the Grammy Hall of Fame in 2001. Since its\n",
      "original 1951 recording it has been covered by a variety of artists .\n",
      "query trumbull marriott fax number\n",
      "LLM generation The fax number for the Trumbull Marriott Shelton is 203-378 -4444 .\n",
      "Groundtruth Business name: Trumbull Marriott Merritt Parkway; Address: 180 Hawley Lane Trumbull,\n",
      "Connecticut 06611; Phone number: 203-378-4958; Fax number: 203-378 -1400; Business\n",
      "hours: 24; Credit cards accepted: Yes; Number of employees: 10-19; Map:\n",
      "query what is hra and hsa\n",
      "LLM generation HRA stands for Health Reimbursement Arrangement. It is a type of beneﬁt plan where an\n",
      "employer reimburses employees for qualifying medical expenses . It also allows employers\n",
      "to set aside money tax-free to reimburse employees for medical expenses that are not covered\n",
      "under the employer’s health insurance plan. HSA stands for Health Savings Account. It is\n",
      "a savings account that is used in conjunction with a qualiﬁed High Deductible Health Plan\n",
      "(HDHP) . An HSA enables an individual to pay for current health expenses and save for future\n",
      "health expenses on a tax-free basis. Contributions to the account are tax\n",
      "Groundtruth The key difference between HRA and HSA is that HRA is an employer funded health beneﬁt\n",
      "plan that reimburses formedical expenses including personal health insurance policy pre-\n",
      "miums of employees whereas HSA is also a tax-advantaged health beneﬁt plan exclusively\n",
      "available to taxpayers in the United States who are enrolled in a High-Deductible Health Plan\n",
      "(HDHP) .\n",
      "Table 7: More examples of LLM generations. The format is the same as in Table 5.\n",
      "A Implementation Details\n",
      "For dense retrieval experiments in Table 1, we\n",
      "list the hyperparameters in Table 8. When\n",
      "training dense retrievers with distillation from\n",
      "cross-encoder, we use the same teacher score\n",
      "released by Wang et al.. The SimLM and\n",
      "E5 checkpoints for initialization are pub-\n",
      "licly available at https://huggingface.\n",
      "co/intfloat/simlm-base-msmarco and\n",
      "https://huggingface.co/intfloat/\n",
      "e5-base-unsupervised . To compute the\n",
      "text embeddings, we utilize the [CLS] vector for\n",
      "SimLM and mean pooling for E5. This makes sure\n",
      "that the pooling mechanisms remain consistent\n",
      "between intermediate pre-training and ﬁne-tuning.\n",
      "When prompting LLMs, we include 4 in-context\n",
      "examples from the MS-MARCO training set. To\n",
      "increase prompt diversity, we randomly select 4\n",
      "examples for each API call. A complete prompt is\n",
      "shown in Table 9.\n",
      "Regarding out-of-domain evaluations on DBpe-\n",
      "dia (Hasibi et al., 2017), NFCorpus (Boteva et al.,\n",
      "2016), Scifact (Wadden et al., 2020), Trec-Covid\n",
      "(V oorhees et al., 2021), and Touche2020 (Bon-\n",
      "darenko et al., 2022), SimLM’s results are based\n",
      "on the released checkpoint by Wang et al..DPR w/ distillation\n",
      "learning rate 2\u000210\u000053\u000210\u00005\n",
      "PLM BERT base SimLM / E5 base-unsup\n",
      "# of GPUs 4 4\n",
      "warmup steps 1000 1000\n",
      "batch size 64 64\n",
      "epoch 3 6\n",
      "\u000b",
      " n.a. 0.2\n",
      "negatives depth 1000 200\n",
      "query length 144 144\n",
      "passage length 144 144\n",
      "# of negatives 15 23\n",
      "Table 8: Hyper-parameters for training dense retrievers\n",
      "on MS-MARCO passage ranking dataset.\n",
      "For ablation experiments in Figure 2, we ﬁne-\n",
      "tune for 40 epochs or 18k steps, whichever is\n",
      "reached ﬁrst.promptsWrite a passage that answers the given query:\n",
      "Query: what state is this zip code 85282\n",
      "Passage: Welcome to TEMPE, AZ 85282. 85282 is a rural zip code in Tempe, Arizona. The population\n",
      "is primarily white, and mostly single. At $200,200 the average home value here is a bit higher than\n",
      "average for the Phoenix-Mesa-Scottsdale metro area, so this probably isn’t the place to look for housing\n",
      "bargains.5282 Zip code is located in the Mountain time zone at 33 degrees latitude (Fun Fact: this is the\n",
      "same latitude as Damascus, Syria!) and -112 degrees longitude.\n",
      "Query: why is gibbs model of reﬂection good\n",
      "Passage: In this reﬂection, I am going to use Gibbs (1988) Reﬂective Cycle. This model is a recognised\n",
      "framework for my reﬂection. Gibbs (1988) consists of six stages to complete one cycle which is able\n",
      "to improve my nursing practice continuously and learning from the experience for better practice in the\n",
      "future.n conclusion of my reﬂective assignment, I mention the model that I chose, Gibbs (1988) Reﬂective\n",
      "Cycle as my framework of my reﬂective. I state the reasons why I am choosing the model as well as some\n",
      "discussion on the important of doing reﬂection in nursing practice.\n",
      "Query: what does a thousand pardons means\n",
      "Passage: Oh, that’s all right, that’s all right, give us a rest; never mind about the direction, hang the\n",
      "direction - I beg pardon, I beg a thousand pardons, I am not well to-day; pay no attention when I soliloquize,\n",
      "it is an old habit, an old, bad habit, and hard to get rid of when one’s digestion is all disordered with eating\n",
      "food that was raised forever and ever before he was born; good land! a man can’t keep his functions\n",
      "regular on spring chickens thirteen hundred years old.\n",
      "Query: what is a macro warning\n",
      "Passage: Macro virus warning appears when no macros exist in the ﬁle in Word. When you open\n",
      "a Microsoft Word 2002 document or template, you may receive the following macro virus warning,\n",
      "even though the document or template does not contain macros: C:\\<path>\\<ﬁle name>contains macros.\n",
      "Macros may contain viruses.\n",
      "Query: when was pokemon green released\n",
      "Passage:\n",
      "outputPokemon Green was released in Japan on February 27th, 1996. It was the ﬁrst in the Pokemon series of\n",
      "games and served as the basis for Pokemon Red and Blue, which were released in the US in 1998. The\n",
      "original Pokemon Green remains a beloved classic among fans of the series.\n",
      "Table 9: The full prompt used for the example in Figure 1.\n",
      "-----------------------------------\n",
      "Extracted text from 'Reference5.pdf':\n",
      "COIL: Revisit Exact Lexical Match in Information Retrieval\n",
      "with Contextualized Inverted List\n",
      "Luyu Gao, Zhuyun Dai, Jamie Callan\n",
      "Language Technologies Institute\n",
      "Carnegie Mellon University\n",
      "{luyug, zhuyund, callan}@cs.cmu.edu\n",
      "Abstract\n",
      "Classical information retrieval systems such as\n",
      "BM25 rely on exact lexical match and carry\n",
      "out search efﬁciently with inverted list index.\n",
      "Recent neural IR models shifts towards soft\n",
      "semantic matching all query document terms,\n",
      "but they lose the computation efﬁciency of\n",
      "exact match systems. This paper presents\n",
      "COIL, a contextualized exact match retrieval\n",
      "architecture that brings semantic lexical match-\n",
      "ing. COIL scoring is based on overlapping\n",
      "query document tokens’ contextualized repre-\n",
      "sentations. The new architecture stores con-\n",
      "textualized token representations in inverted\n",
      "lists, bringing together the efﬁciency of exact\n",
      "match and the representation power of deep\n",
      "language models. Our experimental results\n",
      "show COIL outperforms classical lexical re-\n",
      "trievers and state-of-the-art deep LM retrievers\n",
      "with similar or smaller latency.1\n",
      "1 Introduction\n",
      "Widely used, bag-of-words (BOW) information re-\n",
      "trieval (IR) systems such as BM25 rely on exact\n",
      "lexical match2between query and document terms.\n",
      "Recent study in neural IR takes a different approach\n",
      "and compute soft matching between all query and\n",
      "document terms to model complex matching.\n",
      "The shift to soft matching in neural IR models\n",
      "attempts to address vocabulary mismatch problems,\n",
      "that query and the relevant documents use differ-\n",
      "ent terms, e.g. cat v.s. kitty, for the same con-\n",
      "cept (Huang et al., 2013; Guo et al., 2016; Xiong\n",
      "et al., 2017). Later introduction of contextualized\n",
      "representations (Peters et al., 2018) from deep lan-\n",
      "guage models (LM) further address semantic mis-\n",
      "match , that the same term can refer to different\n",
      "concepts, e.g., bank of river vs. bank in ﬁnance.\n",
      "Fine-tuned deep LM rerankers produce token rep-\n",
      "resentations based on context and achieve state-of-\n",
      "1Our code is available at https://github.com/\n",
      "luyug/COIL .\n",
      "2Exact match up to morphological changes.the-art in text ranking with huge performance leap\n",
      "(Nogueira and Cho, 2019; Dai and Callan, 2019b).\n",
      "Though the idea of soft matching all tokens is\n",
      "carried through the development of neural IR mod-\n",
      "els, seeing the success brought by deep LMs, we\n",
      "take a step back and ask: how much gain can we get\n",
      "if we introduce contextualized representations back\n",
      "to lexical exact match systems? In other words, can\n",
      "we build a system that still performs exact query-\n",
      "document token matching but compute matching\n",
      "signals with contextualized token representations\n",
      "instead of heuristics? This may seem a constraint\n",
      "on the model, but exact lexical match produce more\n",
      "explainable and controlled patterns than soft match-\n",
      "ing. It also allows search to focus on only the\n",
      "subset of documents that have overlapping terms\n",
      "with query, which can be done efﬁciently with in-\n",
      "verted list index. Meanwhile, using dense contex-\n",
      "tualized token representations enables the model\n",
      "to handle semantic mismatch, which has been a\n",
      "long-standing problem in classic lexical systems.\n",
      "To answer the question, we propose a new lexi-\n",
      "cal matching scheme that uses vector similarities\n",
      "between query-document overlapping term contex-\n",
      "tualized representations to replace heuristic scor-\n",
      "ing used in classical systems. We present COn-\n",
      "textualized Inverted List (COIL), a new exact lex-\n",
      "ical match retrieval architecture armed with deep\n",
      "LM representations. COIL processes documents\n",
      "with deep LM ofﬂine and produces representations\n",
      "for each document token. The representations are\n",
      "grouped by their surface tokens into inverted lists.\n",
      "At search time, we build representation vectors\n",
      "for query tokens and perform contextualized ex-\n",
      "act match: use each query token to look up its\n",
      "own inverted list and compute vector similarity\n",
      "with document vectors stored in the inverted list\n",
      "as matching scores. COIL enables efﬁcient search\n",
      "with rich-in-semantic matching between query and\n",
      "document.\n",
      "Our contributions include 1) introduce a novelarXiv:2104.07186v1  [cs.IR]  15 Apr 2021retrieval architecture, contextualized inverted\n",
      "lists (COIL) that brings semantic matching into\n",
      "lexical IR systems, 2) show matching signals in-\n",
      "duced from exact lexical match can capture com-\n",
      "plicated matching patterns, 3) demonstrate COIL\n",
      "signiﬁcantly outperform classical and deep LM\n",
      "augmented lexical retrievers as well as state-of-the-\n",
      "art dense retrievers on two retrieval tasks.\n",
      "2 Related Work\n",
      "Lexical Retriever Classical IR systems rely on\n",
      "exact lexical match retrievers such as Boolean\n",
      "Retrieval, BM25 (Robertson and Walker, 1994)\n",
      "and statistical language models (Lafferty and Zhai,\n",
      "2001). This type of retrieval model can process\n",
      "queries very quickly by organizing the documents\n",
      "into inverted index, where each distinct term has\n",
      "an inverted list that stores information about docu-\n",
      "ments it appears in. Nowadays, they are still widely\n",
      "used in production systems. However, these re-\n",
      "trieval models fall short of matching related terms\n",
      "(vocabulary mismatch) or modeling context of the\n",
      "terms (semantic mismatch). Much early effort\n",
      "was put into improving exact lexical match retriev-\n",
      "ers, such as matching n-grams (Metzler and Croft,\n",
      "2005) or expanding queries with terms from related\n",
      "documents (Lavrenko and Croft, 2001). However,\n",
      "these methods still use BOW framework and have\n",
      "limited capability of modeling human languages.\n",
      "Neural Ranker In order to deal with vocab-\n",
      "ulary mismatch, neural retrievers that rely on\n",
      "soft matching between numerical text represen-\n",
      "tations are introduced. Early attempts compute\n",
      "similarity between pre-trained word embedding\n",
      "such as word2vec (Mikolov et al., 2013) and\n",
      "GLoVe (Pennington et al., 2014) to produce match-\n",
      "ing score (Ganguly et al., 2015; Diaz et al., 2016).\n",
      "One more recent approach encodes query and doc-\n",
      "ument each into a vector and computes vector sim-\n",
      "ilarity (Huang et al., 2013). Later researches real-\n",
      "ized the limited capacity of a single vector to en-\n",
      "code ﬁne-grained information and introduced full\n",
      "interaction models to perform soft matching be-\n",
      "tween all term vectors (Guo et al., 2016; Xiong\n",
      "et al., 2017). In these approaches, scoring is\n",
      "based on learned neural networks and the hugely\n",
      "increased computation cost limited their use to\n",
      "reranking a top candidate list generated by a lexical\n",
      "retriever.Deep LM Based Ranker and Retriever Deep\n",
      "LM made a huge impact on neural IR. Fine-\n",
      "tuned Transformer (Vaswani et al., 2017) LM\n",
      "BERT (Devlin et al., 2019) achieved state-of-the-\n",
      "art reranking performance for passages and docu-\n",
      "ments (Nogueira and Cho, 2019; Dai and Callan,\n",
      "2019b). As illustrated in Figure 1a, the common\n",
      "approach is to feed the concatenated query docu-\n",
      "ment text through BERT and use BERT’s [CLS]\n",
      "output token to produce a relevance score. The\n",
      "deep LM rerankers addressed both vocabulary and\n",
      "semantic mismatch by computing full cross atten-\n",
      "tion between contextualized token representations.\n",
      "Lighter deep LM rankers are developed (MacA-\n",
      "vaney et al., 2020; Gao et al., 2020), but their cross\n",
      "attention operations are still too expensive for full-\n",
      "collection retrieval.\n",
      "Later research therefore resorted to augment-\n",
      "ing lexical retrieval with deep LMs by expanding\n",
      "the document surface form to narrow the vocab-\n",
      "ulary gap, e.g., DocT5Query (Nogueira and Lin,\n",
      "2019), or altering term weights to emphasize impor-\n",
      "tant terms, e.g., DeepCT (Dai and Callan, 2019a).\n",
      "Smartly combining deep LM retriever and reranker\n",
      "can offer additive gain for end performance (Gao\n",
      "et al., 2021a). These retrievers however still suffer\n",
      "from vocabulary and semantic mismatch as tradi-\n",
      "tional lexical retrievers.\n",
      "Another line of research continues the work on\n",
      "single vector representation and build dense retriev-\n",
      "ers, as illustrated in Figure 1b. They store docu-\n",
      "ment vectors in a dense index and retrieve them\n",
      "through Nearest Neighbours search. Using deep\n",
      "LMs, dense retrievers have achieved promising re-\n",
      "sults on several retrieval tasks (Karpukhin et al.,\n",
      "2020). Later researches show that dense retrieval\n",
      "systems can be further improved by better train-\n",
      "ing (Xiong et al., 2020; Gao et al., 2021b).\n",
      "Single vector systems have also been extended\n",
      "to multi-vector representation systems. Poly-\n",
      "encoder (Humeau et al., 2020) encodes queries\n",
      "into a set of vectors. Similarly, Me-BERT (Luan\n",
      "et al., 2020) represents documents with a set of vec-\n",
      "tors. A concurrent work ColBERT (Figure 1c) use\n",
      "multiple vectors to encode both queries and docu-\n",
      "ments (Khattab and Zaharia, 2020). In particular, it\n",
      "represents a documents with all its terms’ vectors\n",
      "and a query with an expanded set of term vectors.\n",
      "It then computes all-to-all (Cartesian) soft match\n",
      "between the tokens. ColBERT performs interaction\n",
      "as dot product followed pooling operations, whichCLS bank account SEP bank river bankCLS bank account SEP bank river bankCLS bank account SEP bank river bankCLS bank account SEP bank river bankscore(a) Cross-Attention Model (e.g., BERT reranker)\n",
      "CLS bank account CLS bank river bankCLS bank accountCLS bank account\n",
      "CLS bank river bankCLS bank river bankscore (b) Dense Retrievers (e.g., DPR)\n",
      "CLS bank account CLS bank river bankCLS bank accountCLS bank account\n",
      "CLS bank river bankCLS bank river bank\n",
      "EXP EXPscore\n",
      "max max max\n",
      "EXP\n",
      "EXPEXP\n",
      "EXPmax max\n",
      "(c) ColBERT: All-to-All Match\n",
      "CLS bank account CLS bank river bankCLS bank accountCLS bank account\n",
      "CLS bank river bankCLS bank river bankdot maxsum (d) COIL: Contextualized Exact Match\n",
      "Figure 1: An illustration of reranking/retrieval mechanisms with deep LM, including our proposed model, COIL.\n",
      "Bank\n",
      "River\n",
      "AccountBank\n",
      "Account\n",
      "Traditional Inverted Lists Querydocid: 3\n",
      "tf: 2docid: 9\n",
      "tf: 1docid: 1\n",
      "tf: 1docid: 2\n",
      "tf: 1docid: 4\n",
      "tf: 1docid: 5\n",
      "tf: 2docid: 1\n",
      "tf: 1docid: 3\n",
      "tf: 1docid: 6\n",
      "tf: 1....\n",
      "....BM25 \n",
      "scoringBM25 \n",
      "scoring\n",
      "BM25 \n",
      "scoring\n",
      "Figure 2: An illustration of traditional inverted lists.\n",
      "The inverted list maps a term to the list of documents\n",
      "where the term occurs. Retriever looks up query terms’\n",
      "inverted lists and scores those documents with stored\n",
      "statistics such as term frequency (tf).\n",
      "allows it to also leverage a dense index to do full\n",
      "corpus retrieval. However, since ColBERT encodes\n",
      "a document with all tokens, it adds another order\n",
      "of magnitude of index complexity to all aforemen-\n",
      "tioned methods: document tokens in the collection\n",
      "need to be stored in a single huge index and con-\n",
      "sidered at query time. Consequently, ColBERT is\n",
      "engineering and hardware demanding.\n",
      "3 Methodologies\n",
      "In this section, we ﬁrst provide some preliminaries\n",
      "on exact lexical match systems. Then we discuss\n",
      "COIL’s contextualized exact match design and how\n",
      "its search index is organized. We also give a com-\n",
      "parison between COIL and other popular retrievers.\n",
      "Bank\n",
      "River\n",
      "AccountBank\n",
      "Account\n",
      "Contextualized Inverted Lists Querydocid  [1 3 6 7]\n",
      "docid  [1 2 4 5 5 9]\n",
      "docid  [3 3 9]vectors\n",
      "vectors\n",
      "vectorsCLSdocid  [1 2 3 4 .............C]\n",
      "vectors ...CLSmatrix\n",
      "product\n",
      "matrix\n",
      "product\n",
      "matrix\n",
      "productFigure 3: COIL’s index and retrieval architecture.\n",
      "COIL-tok relies on the exact token matching (lower).\n",
      "COIL-full includes in addition CLS matching (upper).\n",
      "3.1 Preliminaries\n",
      "Classic lexical retrieval system relies on overlap-\n",
      "ping query document terms under morphological\n",
      "generalization like stemming, in other words, exact\n",
      "lexical match , to score query document pair. A\n",
      "scoring function is deﬁned as a sum of matched\n",
      "term scores. The scores are usually based on statis-\n",
      "tics like term frequency ( tf). Generally, we can\n",
      "write,\n",
      "s=X\n",
      "t2q\\d\u001bt(hq(q;t);hd(d;t)) (1)\n",
      "where for each overlapping term tbetween query q\n",
      "and document d, functionshqandhdextract terminformation and a term scoring function \u001btcom-\n",
      "bines them. A popular example is BM25, which\n",
      "computes,\n",
      "sBM25 =X\n",
      "t2q\\didf(t)hBM25\n",
      "q(q;t)hBM25\n",
      "d(d;t)\n",
      "hBM25\n",
      "q(q;t) =tft;q(1 +k2)\n",
      "tft;q+k2\n",
      "hBM25\n",
      "d(d;t) =tft;d(1 +k1)\n",
      "tft;d+k1(1\u0000b+bjdj\n",
      "avgdl)(2)\n",
      "wheretft;drefers to term frequency of term tin\n",
      "documentd,tft;qrefers to the term frequency in\n",
      "query,idf(t)is inverse document frequency, and b,\n",
      "k1,k2are hyper-parameters.\n",
      "One key advantage of exact lexical match sys-\n",
      "tems lies in efﬁciency. With summation over exact\n",
      "matches, scoring of each query term only goes to\n",
      "documents that contain matching terms. This can\n",
      "be done efﬁciently using inverted list indexing (Fig-\n",
      "ure 2). The inverted list maps back from a term\n",
      "to a list of documents where the term occurs. To\n",
      "compute Equation 1, the retriever only needs to\n",
      "traverse the subset of documents in query terms’\n",
      "inverted lists instead of going over the entire docu-\n",
      "ment collection.\n",
      "While recent neural IR research mainly focuses\n",
      "on breaking the exact match bottleneck with soft\n",
      "matching of text, we hypothesize that exact match\n",
      "itself can be improved by replacing semantic in-\n",
      "dependent frequency-based scoring with semantic\n",
      "rich scoring. In the rest of this section, we show\n",
      "how to modify the exact lexical match framework\n",
      "with contextualized term representations to build\n",
      "effective and efﬁcient retrieval systems.\n",
      "3.2 Contextualized Exact Lexical Match\n",
      "Instead of term frequency, we desire to encode\n",
      "the semantics of terms to facilitate more effective\n",
      "matching. Inspired by recent advancements in deep\n",
      "LM, we encode both query and document tokens\n",
      "into contextualized vector representations and carry\n",
      "out matching between exact lexical matched tokens.\n",
      "Figure 1d illustrates the scoring model of COIL.\n",
      "In this work, we use a Transformer language\n",
      "model3as the contextualization function. We en-\n",
      "code a query qwith the language model (LM) and\n",
      "represent its i-th token by projecting the corre-\n",
      "sponding output:\n",
      "vq\n",
      "i=WtokLM(q;i) +btok (3)\n",
      "3We used the base, uncased variant of BERT.where Wnt\u0002nlm\n",
      "tokis a matrix that maps the LM’s\n",
      "nlmdimension output into a vector of lower di-\n",
      "mensionnt. We down project the vectors as we\n",
      "hypothesize that it sufﬁces to use lower dimension\n",
      "token vectors. We conﬁrm this in section 5. Simi-\n",
      "larly, we encode a document d’sj-th tokendjwith:\n",
      "vd\n",
      "j=WtokLM(d;j) +btok (4)\n",
      "We then deﬁne the contextualized exact lexical\n",
      "match scoring function between query document\n",
      "based on vector similarities between exact matched\n",
      "query document token pairs:\n",
      "stok(q;d) =X\n",
      "qi2q\\dmax\n",
      "dj=qi(vq\n",
      "i|vd\n",
      "j) (5)\n",
      "Note that, importantly, the summation goes through\n",
      "only overlapping terms, qi2q\\d. For each query\n",
      "tokenqi, we ﬁnds all same tokensdjin the docu-\n",
      "ment, computes their similarity with qiusing the\n",
      "contextualized token vectors. The maximum sim-\n",
      "ilarities are picked for query token qi. Max op-\n",
      "erator is adopted to capture the most important\n",
      "signal (Kim, 2014). This ﬁts in the general lexical\n",
      "match formulation, with hqgiving representation\n",
      "forqi,htgiving representations for all dj=qi, and\n",
      "\u001btcompute dot similarities between query vector\n",
      "with document vectors and max pool the scores.\n",
      "As with classic lexical systems, stokdeﬁned in\n",
      "Equation 5 does not take into account similarities\n",
      "between lexical-different terms, thus faces vocabu-\n",
      "lary mismatch. Many popular LMs (Devlin et al.,\n",
      "2019; Yang et al., 2019; Liu et al., 2019) use a\n",
      "special CLS token to aggregate sequence represen-\n",
      "tation. We project the CLS vectos with Wnc\u0002nlm\n",
      "cls\n",
      "to represent the entire query or document,\n",
      "vq\n",
      "cls=WclsLM(q;CLS) +bcls\n",
      "vd\n",
      "cls=WclsLM(d;CLS) +bcls(6)\n",
      "The similarity between vq\n",
      "clsandvd\n",
      "clsprovides high-\n",
      "level semantic matching and mitigates the issue of\n",
      "vocabulary mismatch. The full form of COIL is:\n",
      "sfull(q;d) =stok(q;d) +vq\n",
      "cls|vd\n",
      "cls (7)\n",
      "In the rest of the paper, we refer to systems with\n",
      "CLS matching COIL-full and without COIL-tok .\n",
      "COIL’s scoring model (Figure 1d) is fully differ-\n",
      "entiable. Following earlier work (Karpukhin et al.,\n",
      "2020), we train COIL with negative log likelihood\n",
      "deﬁned over query q, a positive document d+and aset of negative documents fd\u0000\n",
      "1;d\u0000\n",
      "2;::d\u0000\n",
      "l::gas loss.\n",
      "L=\u0000logexp(s(q;d+))\n",
      "exp(s(q;d+)) +P\n",
      "lexp(s(q;d\u0000\n",
      "l))\n",
      "(8)\n",
      "Following Karpukhin et al. (2020), we use in batch\n",
      "negatives and hard negatives generated by BM25.\n",
      "Details are discussed in implementation, section 4.\n",
      "3.3 Index and Retrieval with COIL\n",
      "COIL pre-computes the document representations\n",
      "and builds up a search index, which is illustrated in\n",
      "Figure 3. Documents in the collection are encoded\n",
      "ofﬂine into token and CLS vectors. Formally, for\n",
      "a unique token tin the vocabulary V, we collect\n",
      "its contextualized vectors from all of its mentions\n",
      "from documents in collection C, building token t’s\n",
      "contextualized inverted list:\n",
      "It=fvd\n",
      "jjdj=t;d2Cg; (9)\n",
      "where vd\n",
      "jis the BERT-based token encoding de-\n",
      "ﬁned in Equation 4. We deﬁne search index to\n",
      "store inverted lists for all tokens in vocabulary,\n",
      "I=fItjt2Vg. For COIL-full, we also build an\n",
      "index for the CLS token Icls=fvd\n",
      "clsjd2Cg.\n",
      "As shown in Figure 3, in this work we im-\n",
      "plement COIL’s by stacking vectors in each in-\n",
      "verted listItinto a matrix Mnt\u0002jIkj, so that sim-\n",
      "ilarity computation that traverses an inverted list\n",
      "and computes vector dot product can be done ef-\n",
      "ﬁciently as one matrix-vector product with opti-\n",
      "mized BLAS (Blackford et al., 2002) routines on\n",
      "CPU or GPU. All vd\n",
      "clsvectors can also be organized\n",
      "in a similar fashion into matrix Mclsand queried\n",
      "with matrix product. The matrix implementation\n",
      "here is an exhaustive approach that involves all vec-\n",
      "tors in an inverted list. As a collection of dense\n",
      "vectors, it is also possible to organize each inverted\n",
      "list as an approximate search index (Johnson et al.,\n",
      "2017; Guo et al., 2019) to further speed up search.\n",
      "When a query qcomes in, we encode every of\n",
      "its token into vectors vq\n",
      "i. The vectors are sent to\n",
      "the subset of COIL inverted lists that corresponds\n",
      "query tokens J=fItjt2qg. where the matrix\n",
      "product described above is carried out. This is\n",
      "efﬁcient asjJj<<jIj, having only a small subset\n",
      "of all inverted lists to be involved in search. For\n",
      "COIL-full, we also use encoded CLS vectors vq\n",
      "cls\n",
      "to query the CLS index to get the CLS matching\n",
      "scores. The scoring over different inverted lists canserve in parallel. The scores are then combined by\n",
      "Equation 5 to rank the documents.\n",
      "Readers can ﬁnd detailed illustration ﬁgures in\n",
      "the Appendix A, for index building and querying,\n",
      "Figure 4 and Figure 5, respectively.\n",
      "3.4 Connection to Other Retrievers\n",
      "Deep LM based Lexical Index Models like\n",
      "DeepCT (Dai and Callan, 2019a, 2020) and\n",
      "DocT5Query (Nogueira and Lin, 2019) alter tft;d\n",
      "in documents with deep LM BERT or T5. This is\n",
      "similar to a COIL-tok with token dimension nt= 1.\n",
      "A single degree of freedom however measures more\n",
      "of a term importance than semantic agreement .\n",
      "Dense Retriever Dense retrievers (Figure 1b)\n",
      "are equivalent to COIL-full’s CLS matching. COIL\n",
      "makes up for the lost token-level interactions in\n",
      "dense retriever with exact matching signals.\n",
      "ColBERT ColBERT (Figure 1c) computes rel-\n",
      "evance by soft matching allquery and document\n",
      "term’s contextualized vectors.\n",
      "s(q;d) =X\n",
      "qi2[cls;q;exp]max\n",
      "dj2[cls;d](vq\n",
      "i|vd\n",
      "j) (10)\n",
      "where interactions happen among query q, docu-\n",
      "mentd,clsand set of query expansion tokens exp.\n",
      "The all-to-all match contrasts COIL that only uses\n",
      "exact match. It requires a dense retrieval over all\n",
      "document tokens’ representations as opposed to\n",
      "COIL which only considers query’s overlapping to-\n",
      "kens, and are therefore much more computationally\n",
      "expensive than COIL.\n",
      "4 Experiment Methodologies\n",
      "Datasets We experiment with two large scale ad\n",
      "hoc retrieval benchmarks from the TREC 2019\n",
      "Deep Learning (DL) shared task: MSMARCO\n",
      "passage (8M English passages of average length\n",
      "around 60 tokens) and MSMARCO document (3M\n",
      "English documents of average length around 900\n",
      "tokens)4. For each, we train models with the\n",
      "MSMARCO Train queries, and record results on\n",
      "MSMARCO Dev queries and TREC DL 2019\n",
      "test queries. We report mainly full-corpus re-\n",
      "trieval results but also include the rerank task on\n",
      "MSMARCO Dev queries where we use neural\n",
      "scores to reorder BM25 retrieval results provided\n",
      "by MSMARO organizers. Ofﬁcial metrics include\n",
      "4Both datasets can be downloaded from https://\n",
      "microsoft.github.io/msmarco/MRR@1K and NDCG@10 on test and MRR@10\n",
      "on MSMARCO Dev. We also report recall for the\n",
      "dev queries following prior work (Dai and Callan,\n",
      "2019a; Nogueira and Lin, 2019).\n",
      "Compared Systems Baselines include 1) tradi-\n",
      "tional exact match system BM25, 2) deep LM aug-\n",
      "mented BM25 systems DeepCT (Dai and Callan,\n",
      "2019a) and DocT5Query (Nogueira and Lin, 2019),\n",
      "3) dense retrievers, and 4) soft all-to-all retriever\n",
      "ColBERT. For DeepCT and DocT5Query, we use\n",
      "the rankings provided by the authors. For dense\n",
      "retrievers, we report two dense retrievers trained\n",
      "with BM25 negatives or with mixed BM25 and\n",
      "random negatives, published in Xiong et al. (2020).\n",
      "However since these systems use a robust version\n",
      "of BERT, RoBERTa (Liu et al., 2019) as the LM\n",
      "and train document retriever also on MSMARCO\n",
      "passage set, we in addition reproduce a third dense\n",
      "retriever, that uses the exact same training setup as\n",
      "COIL. All dense retrievers use 768 dimension em-\n",
      "bedding. For ColBERT, we report its published re-\n",
      "sults (available only on passage collection). BERT\n",
      "reranker is added in the rerank task.\n",
      "We include 2 COIL systems: 1) COIL-tok, the\n",
      "exact token match only system, and 2) COLL-full,\n",
      "the model with both token match and CLS match.\n",
      "Implementation We build our models with Py-\n",
      "torch (Paszke et al., 2019) based on huggingface\n",
      "transformers (Wolf et al., 2019). COIL’s LM is\n",
      "based on BERT’s base variant. COIL systems use\n",
      "token dimension nt= 32 and COIL-full use CLS\n",
      "dimensionnc= 768 as default, leading to 110M\n",
      "parameters. We add a Layer Normalization to CLS\n",
      "vector when useful. All models are trained for 5\n",
      "epochs with AdamW optimizer, a learning rate of\n",
      "3e-6, 0.1 warm-up ratio, and linear learning rate\n",
      "decay, which takes around 12 hours. Hard neg-\n",
      "atives are sampled from top 1000 BM25 results.\n",
      "Each query uses 1 positive and 7 hard negatives;\n",
      "each batch uses 8 queries on MSMARCO passage\n",
      "and 4 on MSMARCO document. Documents are\n",
      "truncated to the ﬁrst 512 tokens to ﬁt in BERT.\n",
      "We conduct validation on randomly selected 512\n",
      "queries from corresponding train set. Latency num-\n",
      "bers are measured on dual Xeon E5-2630 v3 for\n",
      "CPU and RTX 2080 ti for GPU. We implement\n",
      "COIL’s inverted lists as matrices as described in\n",
      "subsection 3.3, using NumPy (Harris et al., 2020)\n",
      "on CPU and Pytorch on GPU. We perform a) a set\n",
      "of matrix products to compute token similaritiesover contextualized inverted lists, b) scatter to map\n",
      "token scores back to documents, and c) sort to rank\n",
      "the documents. Illustration can be found in the\n",
      "appendix, Figure 5.\n",
      "5 Results\n",
      "This section studies the effectiveness of COIL\n",
      "and how vector dimension in COIL affects the\n",
      "effectiveness-efﬁciency tradeoff. We also provide\n",
      "qualitative analysis on contextualized exact match.\n",
      "5.1 Main Results\n",
      "Table 1 reports various systems’ performance on\n",
      "the MARCO passage collection. COIL-tok ex-\n",
      "act lexical match only system signiﬁcantly out-\n",
      "performs all previous lexical retrieval systems.\n",
      "With contextualized term similarities, COIL-tok\n",
      "achieves a MRR of 0.34 compared to BM25’s MRR\n",
      "0.18. DeepCT and DocT5Query, which also use\n",
      "deep LMs like BERT and T5, are able to break the\n",
      "limit of heuristic term frequencies but are still lim-\n",
      "ited by semantic mismatch issues. We see COIL-\n",
      "tok outperforms both systems by a large margin.\n",
      "COIL-tok also ranks top of the candidate list bet-\n",
      "ter than dense retrieves. It prevails in MRR and\n",
      "NDCG while performs on par in recall with the\n",
      "best dense system, indicating that COIL’s token\n",
      "level interaction can improve precision. With the\n",
      "CLS matching added, COIL-full gains the ability\n",
      "to handle mismatched vocabulary and enjoys an-\n",
      "other performance leap, outperforming all dense\n",
      "retrievers.\n",
      "COIL-full achieves a very narrow performance\n",
      "gap to ColBERT. Recall that ColBERT computes\n",
      "all-to-all soft matches between all token pairs. For\n",
      "retrieval, it needs to consider for each query token\n",
      "allmentions of alltokens in the collection (MS-\n",
      "MARCO passage collection has around 500M to-\n",
      "ken mentions). COIL-full is able to capture match-\n",
      "ing patterns as effectively with exact match signals\n",
      "from only query tokens’ mentions and a single CLS\n",
      "matching to bridge the vocabulary gap.\n",
      "We observe a similar pattern in the rerank task.\n",
      "COIL-tok is already able to outperform dense re-\n",
      "triever and COIL-full further adds up to perfor-\n",
      "mance with CLS matching, being on-par with Col-\n",
      "BERT. Meanwhile, previous BERT rerankers have\n",
      "little performance advantage over COIL5. In prac-\n",
      "tice, we found BERT rerankers to be much more\n",
      "5Close performance between COIL and BERT rerankers\n",
      "is partially due to the bottleneck of BM25 candidates.Table 1: MSMARCO passage collection results. Results not applicable are denoted ‘–’ and no available ‘n.a.’.\n",
      "MS MARCO Passage Ranking\n",
      "Dev Rerank Dev Retrieval DL2019 Retrieval\n",
      "Model MRR@10 MRR@10 Recall@1K NDCG@10 MRR@1K\n",
      "Lexical Retriever\n",
      "BM25 – 0.184 0.853 0.506 0.825\n",
      "DeepCT – 0.243 0.909 0.572 0.883\n",
      "DocT5Query – 0.278 0.945 0.642 0.888\n",
      "BM25+BERT reranker 0.347 – – – –\n",
      "Dense Retriever\n",
      "Dense (BM25 neg) n.a. 0.299 0.928 0.600 n.a.\n",
      "Dense (rand + BM25 neg) n.a. 0.311 0.952 0.576 n.a.\n",
      "Dense (our train) 0.312 0.304 0.932 0.635 0.898\n",
      "ColBERT 0.349 0.360 0.968 n.a. n.a.\n",
      "COIL-tok 0.336 0.341 0.949 0.660 0.915\n",
      "COIL-full 0.348 0.355 0.963 0.704 0.924\n",
      "Table 2: MSMARCO document collection results. Results not applicable are denoted ‘–’ and no available ‘n.a.’.\n",
      "MS MARCO Document Ranking\n",
      "Dev Rerank Dev Retrieval DL2019 Retrieval\n",
      "Model MRR@10 MRR@10 Recall@1K NDCG@10 MRR@1K\n",
      "Lexical Retriever\n",
      "BM25 – 0.230 0.886 0.519 0.805\n",
      "DeepCT – 0.320 0.942 0.544 0.891\n",
      "DocT5Query – 0.288 0.926 0.597 0.837\n",
      "BM25+BERT reranker 0.383 – – – –\n",
      "Dense Retriever\n",
      "Dense (BM25 neg) n.a. 0.299 0.928 0.600 n.a.\n",
      "Dense (rand + BM25 neg) n.a. 0.311 0.952 0.576 n.a.\n",
      "Dense (our train) 0.358 0.340 0.883 0.546 0.785\n",
      "COIL-tok 0.381 0.385 0.952 0.626 0.921\n",
      "COIL-full 0.388 0.397 0.962 0.636 0.913\n",
      "expensive, requiring over 2700 ms for reranking\n",
      "compared to around 10ms in the case of COIL.\n",
      "Table 2 reports the results on MSMARCO docu-\n",
      "ment collection. In general, we observe a similar\n",
      "pattern as with the passage case. COIL systems\n",
      "signiﬁcantly outperform both lexical and dense sys-\n",
      "tems in MRR and NDCG and retain a small advan-\n",
      "tage measured in recall. The results suggest that\n",
      "COIL can be applicable to longer documents with\n",
      "a consistent advantage in effectiveness.\n",
      "The results indicate exact lexical match mecha-\n",
      "nism can be greatly improved with the introduction\n",
      "of contextualized representation in COIL. COIL’s\n",
      "token-level match also yields better ﬁne-grained\n",
      "signals than dense retriever’s global match signal.\n",
      "COIL-full further combines the lexical signals with\n",
      "dense CLS match, forming a system that can deal\n",
      "with both vocabulary and semantic mismatch, be-\n",
      "ing as effective as all-to-all system.5.2 Analysis of Dimensionality\n",
      "The second experiment tests how varying COIL’s\n",
      "token dimension ntand CLS dimension ncaffect\n",
      "model effectiveness and efﬁciency. We record re-\n",
      "trieval performance and latency on MARCO pas-\n",
      "sage collection in Table 3.\n",
      "In COIL-full systems, reducing CLS dimension\n",
      "from 768 to 128 leads to a small drop in perfor-\n",
      "mance on the Dev set, indicating that a full 768\n",
      "dimension may not be necessary for COIL. Keep-\n",
      "ing CLS dimension at 128, systems with token\n",
      "dimension 32 and 8 have very small performance\n",
      "difference, suggesting that token-speciﬁc semantic\n",
      "consumes much fewer dimensions. Similar pattern\n",
      "inntis also observed in COIL-tok ( nc= 0).\n",
      "On the DL2019 queries, we observe that reduc-\n",
      "ing dimension actually achieves better MRR. We\n",
      "believe this is due to a regulatory effect, as theTable 3: Performance and latency of COIL systems with different representation dimensions. Results not applica-\n",
      "ble are denoted ‘–’ and no available ‘n.a.’. Here ncdenotes COIL CLS dimension and nttoken vector dimension.\n",
      "*: ColBERT use approximate search and quantization. We exclude I/O time from measurements.\n",
      "Dev Retrieval DL2019 Retrieval Latency/ms\n",
      "Model MRR@10 Recall@1K NDCG@10 MRR CPU GPU\n",
      "BM25 0.184 0.853 0.506 0.825 36 n.a.\n",
      "Dense 0.304 0.932 0.635 0.898 293 32\n",
      "ColBERT 0.360 0.968 n.a. n.a. 458* –\n",
      "COIL\n",
      "ncnt\n",
      "768 32 0.355 0.963 0.704 0.924 380 41\n",
      "128 32 0.350 0.953 0.692 0.956 125 23\n",
      "128 8 0.347 0.956 0.694 0.977 113 21\n",
      "0 32 0.341 0.949 0.660 0.915 67 18\n",
      "0 8 0.336 0.940 0.678 0.953 55 16\n",
      "Table 4: Sample query document pairs with similarity scores produced by COIL. Tokens in examination are colored\n",
      "blue. Numbers in brackets are query-document vector similarities computed with vectors generated by COIL.\n",
      "Query Token COIL Contextualized Exact Match Score Relevance\n",
      "what is a cabinet in govtCabinet [16.28] (government) A cabinet [16.75] is a body of high-\n",
      "ranking state ofﬁcials, typically consisting of the top leaders of the ....+\n",
      "Cabinet [7.23] is 20x60 and top is 28x72. .... I had a 2cm granite counter-\n",
      "top installed with a 10 inch overhang on one side and a 14 inch....-\n",
      "what is priority passPriority Pass [11.61] is an independent airport lounge access program. A\n",
      "membership provides you with access to their network of over 700 ....+\n",
      "Snoqualmie Pass [7.98] is a mountain pass [6.83] that carries Interstate\n",
      "90 through the Cascade Range in the U.S. State of Washington....-\n",
      "what isnjstartNJSTART is [1.25] a self-service online platform that allows vendors to\n",
      "manage forms, certiﬁcations, submit proposals, access training ....+\n",
      "Contract awardees will receive their Blanket P.O. once it is [-0.10] con-\n",
      "verted, and details regarding that process will also be sent...-\n",
      "test queries were labeled differently from the MS-\n",
      "MARCO train/dev queries (Craswell et al., 2020).\n",
      "We also record CPU and GPU search latency\n",
      "in Table 3. Lowering COIL-full’s CLS dimen-\n",
      "sion from 768 to 128 gives a big speedup, making\n",
      "COIL faster than DPR system. Further dropping\n",
      "token dimensions provide some extra speedup. The\n",
      "COIL-tok systems run faster than COIL-full, with a\n",
      "latency of the same order of magnitude as the tradi-\n",
      "tional BM25 system. Importantly, lower dimension\n",
      "COIL systems still retain a performance advantage\n",
      "over dense systems while being much faster. We\n",
      "include ColBERT’s latency reported in the original\n",
      "paper, which was optimized by approximate search\n",
      "and quantization. All COIL systems have lower\n",
      "latency than ColBERT even though our current im-\n",
      "plementation does not use those optimization tech-\n",
      "niques. We however note that approximate search\n",
      "and quantization are applicable to COIL, and leave\n",
      "the study of speeding up COIL to future work.5.3 Case Study\n",
      "COIL differs from all previous embedding-based\n",
      "models in that it does not use a single uniﬁed em-\n",
      "bedding space. Instead, for a speciﬁc token, COIL\n",
      "learns an embedding space to encode and measure\n",
      "the semantic similarity of the token in different\n",
      "contexts. In this section, we show examples where\n",
      "COIL differentiates different senses of a word un-\n",
      "der different contexts. In Table 4, we show how\n",
      "the token similarity scores differ across contexts in\n",
      "relevant and irrelevant query document pairs.\n",
      "The ﬁrst query looks for “cabinet” in the context\n",
      "of “govt” (abbreviation for “government”). The\n",
      "two documents both include query token \"cabinet\"\n",
      "but of a different concept. The ﬁrst one refers to\n",
      "the government cabinet and the second to a case\n",
      "or cupboard. COIL manages to match “cabinet” in\n",
      "the query to “cabinet” in the ﬁrst document with\n",
      "a much higher score. In the second query, \"pass\"\n",
      "in both documents refer to the concept of permis-sion. However, through contextualization, COIL\n",
      "captures the variation of the same concept and as-\n",
      "signs a higher score to “pass” in the ﬁrst document.\n",
      "Stop words like “it”, “a”, and “the” are com-\n",
      "monly removed in classic exact match IR systems\n",
      "as they are not informative on their own. In the\n",
      "third query, on the other hand, we observe that\n",
      "COIL is able to differentiate “is” in an explanatory\n",
      "sentence and “is” in a passive form, assigning the\n",
      "ﬁrst higher score to match query context.\n",
      "All examples here show that COIL can go be-\n",
      "yond matching token surface form and introduce\n",
      "rich context information to estimate matching. Dif-\n",
      "ferences in similarity scores across mentions under\n",
      "different contexts demonstrate how COIL systems\n",
      "gain strength over lexical systems.\n",
      "6 Conclusion and Future Work\n",
      "Exact lexical match systems have been widely used\n",
      "for decades in classical IR systems and prove to be\n",
      "effective and efﬁcient. In this paper, we point out\n",
      "a critical problem, semantic mismatch, that gener-\n",
      "ally limits all IR systems based on surface token\n",
      "for matching. To ﬁx semantic mismatch, we in-\n",
      "troduce contextualized exact match to differentiate\n",
      "the same token in different contexts, providing ef-\n",
      "fective semantic-aware token match signals. We\n",
      "further propose contextualized inverted list (COIL)\n",
      "search index which swaps token statistics in in-\n",
      "verted lists with contextualized vector representa-\n",
      "tions to perform effective search.\n",
      "On two large-scale ad hoc retrieval benchmarks,\n",
      "we ﬁnd COIL substantially improves lexical re-\n",
      "trieval and outperforms state-of-the-art dense re-\n",
      "trieval systems. These results indicate large head-\n",
      "room of the simple-but-efﬁcient exact lexical match\n",
      "scheme. When the introduction of contextualiza-\n",
      "tion handles the issue of semantic mismatch, exact\n",
      "match system gains the capability of modeling com-\n",
      "plicated matching patterns that were not captured\n",
      "by classical systems.\n",
      "V ocabulary mismatch in COIL can also be\n",
      "largely mitigated with a high-level CLS vector\n",
      "matching. The full system performs on par with\n",
      "more expensive and complex all-to-all match re-\n",
      "trievers. The success of the full system also shows\n",
      "that dense retrieval and COIL’s exact token match-\n",
      "ing give complementary effects, with COIL making\n",
      "up dense system’s lost token level matching signals\n",
      "and dense solving the vocabulary mismatch proba-\n",
      "bly for COIL.With our COIL systems showing viable search\n",
      "latency, we believe this paper makes a solid step\n",
      "towards building next-generation index that stores\n",
      "semantics. At the intersection of lexical and neural\n",
      "systems, efﬁcient algorithms proposed for both can\n",
      "push COIL towards real-world systems.References\n",
      "S. Blackford, J. Demmel, J. Dongarra, I. Duff, S. Ham-\n",
      "marling, Greg Henry, M. Héroux, L. Kaufman, An-\n",
      "drew Lumsdaine, A. Petitet, R. Pozo, K. Remington,\n",
      "and C. Whaley. 2002. An updated set of basic linear\n",
      "algebra subprograms (blas). ACM Transactions on\n",
      "Mathematical Software , 28.\n",
      "Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel\n",
      "Campos, and Ellen M V oorhees. 2020. Overview\n",
      "of the trec 2019 deep learning track. arXiv preprint\n",
      "arXiv:2003.07820 .\n",
      "Zhuyun Dai and J. Callan. 2019a. Context-aware sen-\n",
      "tence/passage term importance estimation for ﬁrst\n",
      "stage retrieval. ArXiv , abs/1910.10687.\n",
      "Zhuyun Dai and J. Callan. 2020. Context-aware docu-\n",
      "ment term weighting for ad-hoc search. Proceedings\n",
      "of The Web Conference 2020 .\n",
      "Zhuyun Dai and Jamie Callan. 2019b. Deeper text un-\n",
      "derstanding for IR with contextual neural language\n",
      "modeling. In Proceedings of the 42nd International\n",
      "ACM SIGIR Conference on Research and Develop-\n",
      "ment in Information Retrieval, SIGIR 2019, Paris,\n",
      "France, July 21-25, 2019 , pages 985–988. ACM.\n",
      "J. Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\n",
      "Toutanova. 2019. Bert: Pre-training of deep bidirec-\n",
      "tional transformers for language understanding. In\n",
      "NAACL-HLT .\n",
      "Fernando Diaz, Bhaskar Mitra, and Nick Craswell.\n",
      "2016. Query expansion with locally-trained word\n",
      "embeddings. In Proceedings of the 54th Annual\n",
      "Meeting of the Association for Computational Lin-\n",
      "guistics .\n",
      "Debasis Ganguly, Dwaipayan Roy, Mandar Mitra,\n",
      "and Gareth J. F. Jones. 2015. Word embedding\n",
      "based generalized language model for information\n",
      "retrieval. In Proceedings of the 38th International\n",
      "ACM SIGIR Conference on Research and Develop-\n",
      "ment in Information Retrieval .\n",
      "Luyu Gao, Zhuyun Dai, and Jamie Callan. 2020. Mod-\n",
      "ularized transfomer-based ranking framework. In\n",
      "Proceedings of the 2020 Conference on Empirical\n",
      "Methods in Natural Language Processing, EMNLP\n",
      "2020, Online, November 16-20, 2020 . Association\n",
      "for Computational Linguistics.\n",
      "Luyu Gao, Zhuyun Dai, and Jamie Callan. 2021a. Re-\n",
      "think training of BERT rerankers in multi-stage re-\n",
      "trieval pipeline. In Advances in Information Re-\n",
      "trieval - 43rd European Conference on IR Research,\n",
      "ECIR 2021, Virtual Event, March 28 - April 1, 2021,\n",
      "Proceedings, Part II .\n",
      "Luyu Gao, Zhuyun Dai, Tongfei Chen, Zhen Fan, Ben-\n",
      "jamin Van Durme, and Jamie Callan. 2021b. Com-\n",
      "plement lexical retrieval model with semantic resid-\n",
      "ual embeddings. In Advances in Information Re-\n",
      "trieval - 43rd European Conference on IR Research,ECIR 2021, Virtual Event, March 28 - April 1, 2021,\n",
      "Proceedings, Part I .\n",
      "J. Guo, Y . Fan, Qingyao Ai, and W. Croft. 2016. A\n",
      "deep relevance matching model for ad-hoc retrieval.\n",
      "Proceedings of the 25th ACM International on Con-\n",
      "ference on Information and Knowledge Manage-\n",
      "ment .\n",
      "R. Guo, Philip Y . Sun, E. Lindgren, Quan Geng, David\n",
      "Simcha, Felix Chern, and S. Kumar. 2019. Accel-\n",
      "erating large-scale inference with anisotropic vector\n",
      "quantization. arXiv: Learning .\n",
      "Charles R. Harris, K. Jarrod Millman, Stéfan J\n",
      "van der Walt, Ralf Gommers, Pauli Virtanen, David\n",
      "Cournapeau, Eric Wieser, Julian Taylor, Sebas-\n",
      "tian Berg, Nathaniel J. Smith, Robert Kern, Matti\n",
      "Picus, Stephan Hoyer, Marten H. van Kerkwijk,\n",
      "Matthew Brett, Allan Haldane, Jaime Fernández del\n",
      "Río, Mark Wiebe, Pearu Peterson, Pierre Gérard-\n",
      "Marchant, Kevin Sheppard, Tyler Reddy, Warren\n",
      "Weckesser, Hameer Abbasi, Christoph Gohlke, and\n",
      "Travis E. Oliphant. 2020. Array programming with\n",
      "NumPy. Nature .\n",
      "Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng,\n",
      "Alex Acero, and Larry Heck. 2013. Learning deep\n",
      "structured semantic models for web search using\n",
      "clickthrough data. In Proceedings of the 22nd ACM\n",
      "international conference on Information & Knowl-\n",
      "edge Management .\n",
      "Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux,\n",
      "and J. Weston. 2020. Poly-encoders: Architec-\n",
      "tures and pre-training strategies for fast and accurate\n",
      "multi-sentence scoring. In ICLR .\n",
      "J. Johnson, M. Douze, and H. Jégou. 2017. Billion-\n",
      "scale similarity search with gpus. ArXiv ,\n",
      "abs/1702.08734.\n",
      "V . Karpukhin, Barlas O ˘guz, Sewon Min, Patrick\n",
      "Lewis, Ledell Yu Wu, Sergey Edunov, Danqi\n",
      "Chen, and W. Yih. 2020. Dense passage re-\n",
      "trieval for open-domain question answering. ArXiv ,\n",
      "abs/2004.04906.\n",
      "O. Khattab and M. Zaharia. 2020. Colbert: Efﬁcient\n",
      "and effective passage search via contextualized late\n",
      "interaction over bert. Proceedings of the 43rd Inter-\n",
      "national ACM SIGIR Conference on Research and\n",
      "Development in Information Retrieval .\n",
      "Yoon Kim. 2014. Convolutional neural networks for\n",
      "sentence classiﬁcation. In EMNLP .\n",
      "John Lafferty and Chengxiang Zhai. 2001. Document\n",
      "language models, query models, and risk minimiza-\n",
      "tion for information retrieval. In Proceedings of the\n",
      "24th Annual International ACM SIGIR Conference\n",
      "on Research and Development in Information Re-\n",
      "trieval .Victor Lavrenko and W. Bruce Croft. 2001. Relevance-\n",
      "based language models. In Proceedings of the 24th\n",
      "Annual International ACM SIGIR Conference on Re-\n",
      "search and Development in Information Retrieval .\n",
      "Y . Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar\n",
      "Joshi, Danqi Chen, Omer Levy, M. Lewis, Luke\n",
      "Zettlemoyer, and Veselin Stoyanov. 2019. Roberta:\n",
      "A robustly optimized bert pretraining approach.\n",
      "ArXiv , abs/1907.11692.\n",
      "Yi Luan, Jacob Eisenstein, Kristina Toutanova, and\n",
      "M. Collins. 2020. Sparse, dense, and atten-\n",
      "tional representations for text retrieval. ArXiv ,\n",
      "abs/2005.00181.\n",
      "Sean MacAvaney, F. Nardini, R. Perego, N. Tonellotto,\n",
      "Nazli Goharian, and O. Frieder. 2020. Efﬁcient doc-\n",
      "ument re-ranking for transformers by precomputing\n",
      "term representations. Proceedings of the 43rd Inter-\n",
      "national ACM SIGIR Conference on Research and\n",
      "Development in Information Retrieval .\n",
      "Donald Metzler and W. Bruce Croft. 2005. A markov\n",
      "random ﬁeld model for term dependencies. In SIGIR\n",
      "2005: Proceedings of the 28th Annual International\n",
      "ACM SIGIR Conference on Research and Develop-\n",
      "ment in Information Retrieval .\n",
      "Tomas Mikolov, Ilya Sutskever, Kai Chen, G. S. Cor-\n",
      "rado, and J. Dean. 2013. Distributed representations\n",
      "of words and phrases and their compositionality. In\n",
      "NIPS .\n",
      "Rodrigo Nogueira and Kyunghyun Cho. 2019. Passage\n",
      "re-ranking with bert. ArXiv , abs/1901.04085.\n",
      "Rodrigo Nogueira and Jimmy Lin. 2019. From\n",
      "doc2query to doctttttquery.\n",
      "Adam Paszke, Sam Gross, Francisco Massa, Adam\n",
      "Lerer, James Bradbury, Gregory Chanan, Trevor\n",
      "Killeen, Zeming Lin, Natalia Gimelshein, Luca\n",
      "Antiga, Alban Desmaison, Andreas Kopf, Edward\n",
      "Yang, Zachary DeVito, Martin Raison, Alykhan Te-\n",
      "jani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\n",
      "Junjie Bai, and Soumith Chintala. 2019. Py-\n",
      "torch: An imperative style, high-performance deep\n",
      "learning library. In H. Wallach, H. Larochelle,\n",
      "A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Gar-\n",
      "nett, editors, Advances in Neural Information Pro-\n",
      "cessing Systems 32 . Curran Associates, Inc.\n",
      "Jeffrey Pennington, R. Socher, and Christopher D.\n",
      "Manning. 2014. Glove: Global vectors for word rep-\n",
      "resentation. In EMNLP .\n",
      "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\n",
      "Gardner, Christopher Clark, Kenton Lee, and Luke\n",
      "Zettlemoyer. 2018. Deep contextualized word repre-\n",
      "sentations. ArXiv , abs/1802.05365.\n",
      "Stephen E Robertson and Steve Walker. 1994. Some\n",
      "simple effective approximations to the 2-poissonmodel for probabilistic weighted retrieval. In Pro-\n",
      "ceedings of the 17th Annual International ACM-\n",
      "SIGIR Conference on Research and Development in\n",
      "Information Retrieval .\n",
      "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\n",
      "Uszkoreit, Llion Jones, Aidan N. Gomez, L. Kaiser,\n",
      "and Illia Polosukhin. 2017. Attention is all you need.\n",
      "InNIPS .\n",
      "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien\n",
      "Chaumond, Clement Delangue, Anthony Moi, Pier-\n",
      "ric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-\n",
      "icz, Joe Davison, Sam Shleifer, Patrick von Platen,\n",
      "Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\n",
      "Teven Le Scao, Sylvain Gugger, Mariama Drame,\n",
      "Quentin Lhoest, and Alexander M. Rush. 2019.\n",
      "Huggingface’s transformers: State-of-the-art natural\n",
      "language processing. ArXiv , abs/1910.03771.\n",
      "Chenyan Xiong, Zhuyun Dai, J. Callan, Zhiyuan Liu,\n",
      "and R. Power. 2017. End-to-end neural ad-hoc rank-\n",
      "ing with kernel pooling. Proceedings of the 40th\n",
      "International ACM SIGIR Conference on Research\n",
      "and Development in Information Retrieval .\n",
      "Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,\n",
      "J. Liu, P. Bennett, Junaid Ahmed, and Arnold Over-\n",
      "wijk. 2020. Approximate nearest neighbor negative\n",
      "contrastive learning for dense text retrieval. ArXiv ,\n",
      "abs/2007.00808.\n",
      "Z. Yang, Zihang Dai, Yiming Yang, J. Carbonell,\n",
      "R. Salakhutdinov, and Quoc V . Le. 2019. Xlnet:\n",
      "Generalized autoregressive pretraining for language\n",
      "understanding. In NeurIPS .A Appendix\n",
      "A.1 Index Building Illustration\n",
      "The following ﬁgure demonstrates how the document \"apple pie baked ...\" is indexed by COIL. The\n",
      "document is ﬁrst processed by a ﬁne-tuned deep LM to produce for each token a contextualized vector.\n",
      "The vectors of each term \"apple\" and \"juice\" are collected to the corresponding inverted list index along\n",
      "with the document id for lookup.\n",
      "apple\n",
      "LM\n",
      "uapple\n",
      "v\n",
      "appleDocument #10 - apple pie baked ...\n",
      "10\n",
      "v\n",
      "pie10u\n",
      "vpie baked\n",
      "vpie wbaked\n",
      "v10\n",
      "w\n",
      "baked\n",
      "Figure 4: COIL Index Building of document \"apple pie baked...\"A.2 Search Illustration\n",
      "The following ﬁgure demonstrates how the query \"apple juice\" is processed by COIL. Contextualized\n",
      "vectors of each term \"apple\" and \"juice\" go to the corresponding inverted list index consisting of a lookup\n",
      "id array and a matrix stacked from document term vectors. For each index, a matrix vector product is run\n",
      "to produce an array of scores. Afterwards a max-scatter of scores followed by a sortproduces the ﬁnal\n",
      "ranking. Note for each index, we show only operations for a subset of vectors (3 vectors) in the index\n",
      "matrix.\n",
      "v u776\n",
      "zyx776\n",
      "ScoreIdxzyx ScoreIndexapple\n",
      "v w975\n",
      "rqp975\n",
      "ScoreIdxrqp ScoreIndexjuice\n",
      "SortMatrix V ector Product\n",
      "Max Scatter\n",
      "SortingQuery: apple juice\n",
      "Figure 5: COIL Search of query \"apple juice\".\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# List of PDF files to extract text from\n",
    "pdf_files = [\"Reference1.pdf\", \"Reference2.pdf\", \"Reference3.pdf\", \"Reference4.pdf\", \"Reference5.pdf\"]\n",
    "\n",
    "# Output directory to save the text files\n",
    "output_directory = \"output_text_files\"\n",
    "\n",
    "# Create the output directory if it does not exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Extract text from each PDF file and save it into separate text files\n",
    "for pdf_file in pdf_files:\n",
    "    pdf_path = os.path.abspath(pdf_file)\n",
    "    text_content = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "    # Remove the '.pdf' extension from the file name to use it as the text file name\n",
    "    text_file_name = os.path.splitext(pdf_file)[0] + \".txt\"\n",
    "\n",
    "    # Create the full path to the output text file\n",
    "    output_file_path = os.path.join(output_directory, text_file_name)\n",
    "\n",
    "    # Write the text content to the text file\n",
    "    with open(output_file_path, \"w\", encoding=\"utf-8\") as text_file:\n",
    "        text_file.write(text_content)\n",
    "\n",
    "    # Print the extracted text content for each PDF file\n",
    "    print(f\"Extracted text from '{pdf_file}':\")\n",
    "    print(text_content)\n",
    "    print(\"-----------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec2913a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='output_text_files.zip' target='_blank'>output_text_files.zip</a><br>"
      ],
      "text/plain": [
       "C:\\Users\\Student.MS-02\\output_text_files.zip"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# List of PDF files to extract text from\n",
    "pdf_files = [\"Reference1.pdf\", \"Reference2.pdf\", \"Reference3.pdf\", \"Reference4.pdf\", \"Reference5.pdf\"]\n",
    "\n",
    "# Output directory to save the text files\n",
    "output_directory = \"output_text_files\"\n",
    "\n",
    "# Create the output directory if it does not exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Extract text from each PDF file and save it into separate text files\n",
    "for pdf_file in pdf_files:\n",
    "    pdf_path = os.path.abspath(pdf_file)\n",
    "    text_content = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "    # Remove the '.pdf' extension from the file name to use it as the text file name\n",
    "    text_file_name = os.path.splitext(pdf_file)[0] + \".txt\"\n",
    "\n",
    "    # Create the full path to the output text file\n",
    "    output_file_path = os.path.join(output_directory, text_file_name)\n",
    "\n",
    "    # Write the text content to the text file\n",
    "    with open(output_file_path, \"w\", encoding=\"utf-8\") as text_file:\n",
    "        text_file.write(text_content)\n",
    "\n",
    "# Zip the output_text_files folder\n",
    "import shutil\n",
    "\n",
    "shutil.make_archive(output_directory, 'zip', output_directory)\n",
    "\n",
    "# Display the download link\n",
    "from IPython.display import FileLink\n",
    "FileLink(f\"{output_directory}.zip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6c6b0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Name of the directory to store the extracted text files\n",
    "directory_name = \"output_text_files\"\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "os.makedirs(directory_name, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95858d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Move the extracted text files to the created directory\n",
    "for pdf_file in pdf_files:\n",
    "    text_file_name = os.path.splitext(pdf_file)[0] + \".txt\"\n",
    "    original_file_path = os.path.join(output_directory, text_file_name)\n",
    "    new_file_path = os.path.join(directory_name, text_file_name)\n",
    "    shutil.move(original_file_path, new_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e3a07bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers>=4.10.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32c632e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (2.1.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: boto3 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from transformers) (1.24.28)\n",
      "Requirement already satisfied: requests in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from transformers) (2.29.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: regex in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from transformers) (0.1.99)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: botocore<1.28.0,>=1.27.28 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from boto3->transformers) (1.27.59)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from boto3->transformers) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from boto3->transformers) (0.6.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: six in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\student.ms-02\\appdata\\roaming\\python\\python311\\site-packages (from sacremoses->transformers) (8.1.6)\n",
      "Requirement already satisfied: joblib in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\student.ms-02\\appdata\\roaming\\python\\python311\\site-packages (from tqdm->transformers) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from botocore<1.28.0,>=1.27.28->boto3->transformers) (2.8.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "885b2e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: click in c:\\users\\student.ms-02\\appdata\\roaming\\python\\python311\\site-packages (from nltk) (8.1.6)\n",
      "Requirement already satisfied: joblib in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\student.ms-02\\appdata\\roaming\\python\\python311\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "86ce70cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ask a question about Reference1.txt: In which Reference the MRR@10 Notes is used\n",
      "Answer for Reference1.txt: The up-\n",
      "shot of the above analysis is that retrieval tech-\n",
      "niques based on learned sparse representations\n",
      "should be divided into an expansion model andSparse Representations MRR@10 Notes\n",
      "Term Weighting Expansion\n",
      "(1a) BM25 None 0.184 copied from (Nogueira and Lin, 2019)\n",
      "(1b) BM25 doc2query–T5 0.277 copied from (Nogueira and Lin, 2019)\n",
      "(2a) DeepCT None 0.243 copied from (Dai and Callan, 2019)\n",
      "(2b) DeepCT doc2query–T5 ?\n",
      "Ask a question about Reference2.txt: define Neural Matching Models\n",
      "Answer for Reference2.txt: . . $15.00\n",
      "https://doi.org/10.1145/3397271.3401075\n",
      "0.15 0.20 0.25 0.30 0.35 0.40\n",
      "MRR@10101102103104105Query Latency (ms)\n",
      "BM25doc2queryKNRMDuet\n",
      "DeepCTfT+ConvKNRM\n",
      "docTTTTTqueryBERT-baseBERT-large\n",
      "ColBERT (re-rank)ColBERT (full retrieval)Bag-of-Words (BoW) Model\n",
      "BoW Model with NLU Augmentation\n",
      "Neural Matching Model\n",
      "Deep Language Model\n",
      "ColBERT (ours)Figure 1: Effectiveness (MRR@10) versus Mean Query La-\n",
      "tency (log-scale) for a number of representative ranking\n",
      "models on MS MARCO Ranking [24].\n",
      "Ask a question about Reference3.txt: sequence-to-sequencemodelsare known to be?\n",
      "Answer for Reference3.txt: Doc2Query — the process of expanding the content of a\n",
      "document before indexing using a sequence-to-sequence model — has\n",
      "emerged as a prominent technique for improving the ﬁrst-stage retrieval\n",
      "eﬀectivenessofsearchengines.However,sequence-to-sequencemodelsare\n",
      "known to be prone to “hallucinating” content that is not present in the\n",
      "source text.\n",
      "Ask a question about Reference4.txt: what are the two mainstream paradigms for IR\n",
      "Answer for Reference4.txt: There are two mainstream\n",
      "paradigms for IR: lexical-based sparse retrieval,\n",
      "such as BM25, and embedding-based dense re-\n",
      "trieval (Xiong et al., 2021; Qu et al., 2021).\n",
      "Ask a question about Reference5.txt: use of deep LMs\n",
      "Answer for Reference5.txt: DeepCT and DocT5Query, which also use\n",
      "deep LMs like BERT and T5, are able to break the\n",
      "limit of heuristic term frequencies but are still lim-\n",
      "ited by semantic mismatch issues.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "# List of PDF files to extract text from\n",
    "pdf_files = [\"Reference1.pdf\", \"Reference2.pdf\", \"Reference3.pdf\", \"Reference4.pdf\", \"Reference5.pdf\"]\n",
    "\n",
    "# Output directory to save the text files\n",
    "output_directory = \"output_text_files\"\n",
    "\n",
    "# Create the output directory if it does not exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# ... Code for extracting text from PDF files and saving as text files ...\n",
    "\n",
    "# Loop through the text files and ask questions\n",
    "for pdf_file in pdf_files:\n",
    "    text_file_name = os.path.splitext(pdf_file)[0] + \".txt\"\n",
    "    file_path = os.path.join(output_directory, text_file_name)\n",
    "    \n",
    "    # Read the text content from the file\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as text_file:\n",
    "        text_content = text_file.read()\n",
    "    \n",
    "    # Ask the question\n",
    "    question = input(f\"Ask a question about {text_file_name}: \")\n",
    "    \n",
    "    # Perform sentence splitting without explicitly downloading the tokenizer\n",
    "    sentences = sent_tokenize(text_content)\n",
    "    \n",
    "    # Find the sentence that best matches the question using regular expressions\n",
    "    best_match = None\n",
    "    max_match_score = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Calculate the match score based on the number of overlapping words\n",
    "        match_score = sum(1 for word in re.findall(r'\\w+', question.lower()) if word in sentence.lower())\n",
    "        \n",
    "        # Update the best match if the current sentence has a higher match score\n",
    "        if match_score > max_match_score:\n",
    "            max_match_score = match_score\n",
    "            best_match = sentence\n",
    "    \n",
    "    # Print the answer (best matching sentence)\n",
    "    print(f\"Answer for {text_file_name}: {best_match}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef0f1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXTRACTING TABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "51355fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: camelot-py in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: chardet>=3.0.4 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from camelot-py) (4.0.0)\n",
      "Requirement already satisfied: click>=6.7 in c:\\users\\student.ms-02\\appdata\\roaming\\python\\python311\\site-packages (from camelot-py) (8.1.6)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from camelot-py) (1.24.3)\n",
      "Requirement already satisfied: openpyxl>=2.5.8 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from camelot-py) (3.0.10)\n",
      "Requirement already satisfied: pandas>=0.23.4 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from camelot-py) (1.5.3)\n",
      "Requirement already satisfied: pdfminer.six>=20200726 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from camelot-py) (20221105)\n",
      "Requirement already satisfied: PyPDF2>=1.26.0 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from camelot-py) (3.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\student.ms-02\\appdata\\roaming\\python\\python311\\site-packages (from click>=6.7->camelot-py) (0.4.6)\n",
      "Requirement already satisfied: et_xmlfile in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from openpyxl>=2.5.8->camelot-py) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from pandas>=0.23.4->camelot-py) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from pandas>=0.23.4->camelot-py) (2022.7)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from pdfminer.six>=20200726->camelot-py) (2.0.4)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from pdfminer.six>=20200726->camelot-py) (39.0.1)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six>=20200726->camelot-py) (1.15.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas>=0.23.4->camelot-py) (1.16.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six>=20200726->camelot-py) (2.21)\n"
     ]
    }
   ],
   "source": [
    "!pip install camelot-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a2d9e6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved 'Reference1.pdf' to 'pdf_files_folder'.\n",
      "Moved 'Reference2.pdf' to 'pdf_files_folder'.\n",
      "Moved 'Reference3.pdf' to 'pdf_files_folder'.\n",
      "Moved 'Reference4.pdf' to 'pdf_files_folder'.\n",
      "Moved 'Reference5.pdf' to 'pdf_files_folder'.\n",
      "Files in 'pdf_files_folder': ['Reference1.pdf', 'Reference2.pdf', 'Reference3.pdf', 'Reference4.pdf', 'Reference5.pdf']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# List of PDF files to move into the folder\n",
    "pdf_files = [\"Reference1.pdf\", \"Reference2.pdf\", \"Reference3.pdf\", \"Reference4.pdf\", \"Reference5.pdf\"]\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "folder_name = \"pdf_files_folder\"\n",
    "if not os.path.exists(folder_name):\n",
    "    os.mkdir(folder_name)\n",
    "    print(f\"Folder '{folder_name}' created.\")\n",
    "\n",
    "# Move the PDF files into the folder\n",
    "for pdf_file in pdf_files:\n",
    "    src_path = os.path.join(\".\", pdf_file)  # Assuming the PDF files are in the same location as the script/notebook\n",
    "    dest_path = os.path.join(folder_name, pdf_file)\n",
    "    shutil.move(src_path, dest_path)\n",
    "    print(f\"Moved '{pdf_file}' to '{folder_name}'.\")\n",
    "\n",
    "# Verify if the files are moved correctly\n",
    "files_in_folder = os.listdir(folder_name)\n",
    "print(f\"Files in '{folder_name}': {files_in_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea3939a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "folder_path = \"pdf_files_folder\"\n",
    "os.startfile(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "462e34d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "folder_path = \"pdf_files_folder\"\n",
    "os.system(f\"open {folder_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f7bb59a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "folder_path = \"pdf_files_folder\"\n",
    "os.system(f\"xdg-open {folder_path}\")\n",
    "\n",
    "#C:\\Users\\Student.MS-02\\pdf_files_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "be66139e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2 pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ea9e312d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b0d3ca23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tabula-py in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: pandas>=0.25.3 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from tabula-py) (1.5.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from tabula-py) (1.24.3)\n",
      "Requirement already satisfied: distro in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from tabula-py) (1.8.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from pandas>=0.25.3->tabula-py) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from pandas>=0.25.3->tabula-py) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas>=0.25.3->tabula-py) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tabula-py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7bb507cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables extracted from Reference1.pdf:\n",
      "Table 1:\n",
      "                                           Unnamed: 0  \\\n",
      "35                                                  1   \n",
      "36    We present a novel conceptual framework for un-   \n",
      "37  derstanding recent developments in information...   \n",
      "38  trieval that organizes techniques along two di...   \n",
      "39  sions. The first dimension establishes the con...   \n",
      "40    between sparse and dense vector representations   \n",
      "41                        for queries and documents.1   \n",
      "42  sion establishes the contrast between unsuperv...   \n",
      "43  and learned (supervised) representations. Figu...   \n",
      "44                         illustrates our framework.   \n",
      "46    by DPR (Karpukhin et al., 2020) and ANCE (Xiong   \n",
      "47    et al., 2021), but also encompassing many other   \n",
      "48                                         techniques   \n",
      "49  2020; Qu et al., 2021; Hofstätter et al., 2021...   \n",
      "50                        1 2Consistent with parlance   \n",
      "51  “document” throughout this paper in a generic ...   \n",
      "52  the unit of retrieved text. To be more precise...   \n",
      "53          are in fact focused on passage retrieval.   \n",
      "\n",
      "             arXiv:2106.14807v1  [cs.IR]  28 Jun 2021  \n",
      "35  Introduction this way, it is technically accur...  \n",
      "36  is represented by a sparse vector where each d...  \n",
      "37  sion corresponds to a unique term in the vocab...  \n",
      "38  and the scoring function assigns a weight to e...  \n",
      "39   mension. As with dense retrieval, query–document  \n",
      "40            scores are computed via inner products.  \n",
      "41  The second dimen- What about learned sparse re...  \n",
      "42  prominent recent example of this in the litera...  \n",
      "43       is DeepCT (Dai and Callan, 2019), which uses  \n",
      "44  a transformer to learn term weights based on a...  \n",
      "46     from the MS MARCO passage ranking test collec-  \n",
      "47  tion.2 DeepCT has an interesting “quirk”: in t...  \n",
      "48  (Gao et al., 2021b; Hofstätter et al., it only...  \n",
      "49  of term weights, but still relies on the remai...  \n",
      "50  in information retrieval, we use Learning spar...  \n",
      "51  The earliest example we are aware of is Wilbur...  \n",
      "52  attempted to learn global term weights using T...  \n",
      "53           the idea likely dates back even further.  \n",
      "Table 2:\n",
      "      parts of the BM25 scoring function via the gen-  \\\n",
      "0     eration of pseudo-documents. This approach also   \n",
      "1    has a weakness: it only assigns weights to terms   \n",
      "2     that are already present in the document, which   \n",
      "4   tant limitation that is addressed by the use o...   \n",
      "5   representations, which are capable of capturin...   \n",
      "6                                     mantic matches.   \n",
      "7      These two issues were resolved by the recently   \n",
      "8    proposed DeepImpact model (Mallia et al., 2021),   \n",
      "9   which also belongs in the family of learned sp...   \n",
      "10   representations. DeepImpact brought together two   \n",
      "11  key ideas: the use of document expansion to iden-   \n",
      "12   tify dimensions in the sparse vector that should   \n",
      "13   have non-zero weights and a term weighting model   \n",
      "14  based on a pairwise loss between relevant and ...   \n",
      "15  relevant texts with respect to a query. Expansion   \n",
      "16    terms were identified by doc2query–T5 (Nogueira   \n",
      "17   and Lin, 2019), a sequence-to-sequence model for   \n",
      "18  document expansion that predicts queries for w...   \n",
      "19     a text would be relevant. Since the DeepImpact   \n",
      "20  scoring model directly predicts term weights that   \n",
      "21   are then quantized, it would be more accurate to   \n",
      "22   call these weights learned impacts, since query–   \n",
      "23   document scores are simply the sum of weights of   \n",
      "24  document terms that are found in the query. Ca...   \n",
      "25  these impact scores draws an explicit connecti...   \n",
      "26  a thread of research in information retrieval ...   \n",
      "29       The recently proposed COIL architecture (Gao   \n",
      "30  et al., 2021a) presents an interesting case fo...   \n",
      "31    conceptual framework. Where does it belong? The   \n",
      "32     authors themselves describe COIL as “a new ex-   \n",
      "33  act lexical match retrieval architecture armed...   \n",
      "34     deep LM representations”. COIL produces repre-   \n",
      "35   sentations for each document token that are then   \n",
      "36   directly stored in the inverted index, where the   \n",
      "37   term frequency usually goes in an inverted list.   \n",
      "38     Although COIL is perhaps best described as the   \n",
      "39    intellectual descendant of ColBERT (Khattab and   \n",
      "40  Zaharia, 2020), another way to think about it ...   \n",
      "41  our conceptual framework is that instead of as...   \n",
      "42  ing scalar weights to terms in a query, the “s...   \n",
      "43   model assigns each term a vector “weight”. Query   \n",
      "44     evaluation in COIL involves accumulating inner   \n",
      "45                products instead of scalar weights.   \n",
      "46        Our conceptual framework highlights a final   \n",
      "47  class of techniques: unsupervised dense represen-   \n",
      "48  tations. While there is little work in this sp...   \n",
      "49  late, it does describe techniques such as LSI ...   \n",
      "50   wester et al., 1990; Atreya and Elkan, 2010) and   \n",
      "\n",
      "    LDA (Wei and Croft, 2006), which have been previ-  \n",
      "0   ously explored. Thus, all quadrants in our pro...  \n",
      "1       conceptual framework are populated with known  \n",
      "2                       examples from the literature.  \n",
      "4                         2 Comments and Observations  \n",
      "5    Based on this framework, we can make a number of  \n",
      "6   interesting observations that highlight obviou...  \n",
      "7   steps in the development of retrieval techniques.  \n",
      "8                              We discuss as follows:  \n",
      "9   Choice of bases. Retrieval techniques using le...  \n",
      "10  dense representations and learned sparse repre...  \n",
      "11  tations present an interesting contrast. Nearl...  \n",
      "12  recent proposals take advantage of transformer...  \n",
      "13  that aspect of the design is not a salient dif...  \n",
      "14  The critical contrast is the basis of the vect...  \n",
      "15  resentations: In sparse approaches, the basis ...  \n",
      "16  vector space remains fixed to the corpus vocab...  \n",
      "17      and thus techniques such as DeepCT, COIL, and  \n",
      "18     DeepImpact can be understood as term weighting  \n",
      "19    models. In dense approaches, the model is given  \n",
      "20     the freedom to choose a new basis derived from  \n",
      "21  transformer representations. This change in basis  \n",
      "22   allows the encoder to represent the “meaning” of  \n",
      "23  texts in relatively small fixed-width vectors ...  \n",
      "24  pared to sparse vectors that may have millions of  \n",
      "25   dimensions). This leads us to the next important  \n",
      "26                                       observation:  \n",
      "29   some form of expansion, learned sparse represen-  \n",
      "30  tations remain limited to (better) exact matching  \n",
      "31       between queries and documents. The nature of  \n",
      "32  sparse representations means that it is imprac...  \n",
      "33   to consider non-zero weights for all elements in  \n",
      "34  the vector (i.e., the vocabulary space). Thus,...  \n",
      "35  ment expansion serves the critical role of pro...  \n",
      "36  a set of candidate terms that should receive non-  \n",
      "37  zero weights; since the number of candidate terms  \n",
      "38  is small compared to the vocabulary size, the re-  \n",
      "39  sulting vector remains sparse. Without expansion,  \n",
      "40  learned sparse representations cannot address the  \n",
      "41  vocabulary mismatch problem (Furnas et al., 19...  \n",
      "42    because document terms not present in the query  \n",
      "43  cannot contribute any score. For DeepImpact, this  \n",
      "44     expansion is performed by doc2query–T5, but in  \n",
      "45  principle we can imagine other methods also. This  \n",
      "46        leads us to the next important observation:  \n",
      "47     Relating DeepCT, DeepImpact, and COIL. The up-  \n",
      "48  shot of the above analysis is that retrieval t...  \n",
      "49     niques based on learned sparse representations  \n",
      "50      should be divided into an expansion model and  \n",
      "Table 3:\n",
      "                               Sparse Representations  \\\n",
      "36                              which we report next.   \n",
      "42     shown in Table 2 on the development queries of   \n",
      "50  BM25 baseline, and row (1b) provides the effec...   \n",
      "\n",
      "                        MRR@10                              Notes Unnamed: 0  \n",
      "36                     uments,  as in row (2b); to our knowledge,       this  \n",
      "42           the contributions              of each component, we      could  \n",
      "50  is a dense–sparse hybrid).                        Through the    lens of  \n",
      "Table 4:\n",
      "      bined with doc2query–T5. Using source code pro-  \\\n",
      "0      vided by the authors,3 we trained such a model   \n",
      "1     from scratch, using the same hyperparameters as   \n",
      "2   the authors. This variant leads to a nearly tw...   \n",
      "3        gain in effectiveness, as shown in row (2f).   \n",
      "4   In another interesting extension, if we reduce...   \n",
      "5    token dimension of COIL to one, the model degen-   \n",
      "6    erates into producing scalar weights, which then   \n",
      "7     becomes directly comparable to DeepCT, row (2a)   \n",
      "8   and the “no-expansion” variant of DeepImpact, row   \n",
      "9   (2c). These comparisons isolate the effects of...   \n",
      "10  ent term weighting models. We dub this variant of   \n",
      "11    COIL “uniCOIL”, on top of which we can also add   \n",
      "12     doc2query–T5, which produces a fair comparison   \n",
      "13  to DeepImpact, row (2d). The original formulation   \n",
      "14    of COIL, even with a token dimension of one, is   \n",
      "15  not directly amenable to retrieval using inverted   \n",
      "16    indexes because weights can be negative. To ad-   \n",
      "17     dress this issue, we added a ReLU operation on   \n",
      "18  the output term weights of the base COIL model to   \n",
      "19  force the model to generate non-negative weights.   \n",
      "20    Once again, we retrained the model from scratch   \n",
      "21     using the same hyperparameters provided by the   \n",
      "22    authors. When encoding the corpus, we quantized   \n",
      "23  these weights into 8 bits to obtain impact sco...   \n",
      "24  query weights are similarly quantized. After t...   \n",
      "25  modifications, uniCOIL is directly compatible ...   \n",
      "26  inverted indexes. Our experimental results are...   \n",
      "27  ported with the Anserini toolkit (Yang et al.,...   \n",
      "29  It is no surprise that uniCOIL without doc2query–   \n",
      "53      learned impact weights, beating DeepImpact by   \n",
      "54                                 around two points.   \n",
      "67                      https://github.com/luyug/COIL   \n",
      "\n",
      "        that ColBERT (Khattab and Zaharia, 2020) uses  \n",
      "0      the more expressive MaxSim operator to compare  \n",
      "1   query and document representations; all other ...  \n",
      "2                          niques use inner products.  \n",
      "3   The final block of Table 2 presents the result...  \n",
      "4    dense–sparse hybrids. Lin et al. (2021) reported  \n",
      "5       the results of dense–sparse hybrids when TCT-  \n",
      "6         ColBERTv2, row (3f), is combined with BM25,  \n",
      "7      row (1a), and doc2query–T5, row (1b). To this,  \n",
      "8       we added fusion with DeepImpact, uniCOIL, and  \n",
      "9   COIL-tok (d = 32). For a fair comparison, we fol-  \n",
      "10   lowed the same technique for combining dense and  \n",
      "11  sparse results as Lin et al. (2021), which is ...  \n",
      "12  et al. (2021). For each query q, we used the c...  \n",
      "13   sponding dense and sparse techniques to retrieve  \n",
      "14   top-1k documents. The final fusion score of each  \n",
      "15  document is calculated by sdense+α ·ssparse. S...  \n",
      "16  the range of the two different scores are quit...  \n",
      "17  ent, we first normalized the scores into range...  \n",
      "18   The α was tuned in the range(0, 2) with a simple  \n",
      "19    line search on a subset of the MS MARCO passage  \n",
      "20                                      training set.  \n",
      "21        With these hybrid combinations, we are able  \n",
      "22  to achieve, to our knowledge, the highest repo...  \n",
      "23    scores on the MS MARCO passage ranking task for  \n",
      "24  single-stage techniques (i.e., no reranking). ...  \n",
      "25  that, as before, uniCOIL is compatible with stan-  \n",
      "26  dard inverted indexes, unlike COIL-tok, which re-  \n",
      "27                      quires custom infrastructure.  \n",
      "29                                       4 Next Steps  \n",
      "53       An important point to make here is that neu-  \n",
      "54  ral networks, particularly transformers, have not  \n",
      "67   indexes (e.g., integer coding techniques to com-  \n",
      "Table 5:\n",
      "  press inverted lists) and efficient query evaluation Indexing by  latent  \\\n",
      "3  our uniCOIL experiments is only 1.3 GB, com- G...                Thomas   \n",
      "\n",
      "  semantic  analysis. Journal  of  \n",
      "3       K.  Landauer,   Louis  M.  \n",
      "Table 6:\n",
      "      Antonio Mallia,   Omar  Khattab, Torsten Suel,  and\n",
      "2  nual International    ACM  SIGIR Conference    on  Re-\n",
      "8      Ren, Wayne Xin  Zhao,     Daxiang Dong,   Hua  Wu,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Got stderr: Jul 27, 2023 4:03:34 PM org.apache.fontbox.ttf.CmapSubtable processSubtype14\r\n",
      "WARNING: Format 14 cmap table is not supported and will be ignored\r\n",
      "Jul 27, 2023 4:03:34 PM org.apache.pdfbox.pdmodel.font.PDSimpleFont toUnicode\r\n",
      "WARNING: No Unicode mapping for summationdisplay.1 (213) in font CYBZPM+txexs\r\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables extracted from Reference2.pdf:\n",
      "Table 1:\n",
      "   Session 1A: NeuIR and Semantic Matching SIGIR ’20, July 25–30, 2020, Virtual Event, China\n",
      "0   ColBERT: Efficient and Effective Passage Searc...                                       \n",
      "1           Contextualized Late Interaction over BERT                                       \n",
      "2                          Omar Khattab Matei Zaharia                                       \n",
      "3             Stanford University Stanford University                                       \n",
      "4         okhattab@stanford.edu matei@cs.stanford.edu                                       \n",
      "..                                                ...                                       \n",
      "65  SIGIR ’20, July 25–30, 2020, Virtual Event, China                                       \n",
      "66  1© 2020 Copyright held by the owner/author(s)....                                       \n",
      "67  ACM ISBN 978-1-4503-8016-4/20/07. . . $15.00 2...                                       \n",
      "68  https://doi.org/10.1145/3397271.3401075 in-sea...                                       \n",
      "69                                                 39                                       \n",
      "\n",
      "[70 rows x 1 columns]\n",
      "Table 2: Empty table\n",
      "Table 3: Empty table\n",
      "Table 4:\n",
      "   Session 1A: NeuIR and Semantic Matching SIGIR ’20, July 25–30, 2020, Virtual Event, China\n",
      "0   Even though ColBERT’s late-interaction framewo...                                       \n",
      "1   plied to awide variety of architectures (e.g.,...                                       \n",
      "2   ers, etc.), we choose to focus this work on bi...                                       \n",
      "3   based encoders (i.e., BERT) owing to their sta...                                       \n",
      "4   ness yet very high computational cost. at the ...                                       \n",
      "..                                                ...                                       \n",
      "57  of tokens N , we pad it with BERT’s special [m...                                       \n",
      "58                                       j ∈[ |E |] j                                       \n",
      "59  to length N q (otherwise, we truncate it to th...                                       \n",
      "60  This padded sequence of input tokens is then p...                                       \n",
      "61                                                 42                                       \n",
      "\n",
      "[62 rows x 1 columns]\n",
      "Table 5:\n",
      "   Session 1A: NeuIR and Semantic Matching SIGIR ’20, July 25–30, 2020, Virtual Event, China\n",
      "0   ColBERT is differentiable end-to-end. We fine-...                                       \n",
      "1   encoders and train from scratch the additional...                                       \n",
      "2   linear layer and the [Q] and [D] markers’ embe...                                       \n",
      "3   Adam [16] optimizer. Notice that our interacti...                                       \n",
      "4   no trainable parameters. Given a triple ⟨ q, d...                                       \n",
      "..                                                ...                                       \n",
      "56  Given a query q, we compute its bag of context...                                       \n",
      "57  dings Eq(Equation 1) and, concurrently, gather...                                       \n",
      "58  sentations into a 3-dimensional tensor D consi...                                       \n",
      "59  4The public BERT implementations we saw simply...                                       \n",
      "60                                                 43                                       \n",
      "\n",
      "[61 rows x 1 columns]\n",
      "Table 6: Empty table\n",
      "Table 7:\n",
      "              Session 1A: NeuIR and Semantic Matching  \\\n",
      "0                   Method MRR@10 (Dev) MRR@10 (Eval)   \n",
      "1                           BM25 (official) 16.7 16.5   \n",
      "2                                      KNRM 19.8 19.8   \n",
      "3                                      Duet 24.3 24.5   \n",
      "4                         fastText+ConvKNRM 29.0 27.7   \n",
      "5                                BERTbase [25] 34.7 -   \n",
      "6                      BERTbase (our training) 36.0 -   \n",
      "7                            BERTlarge [25] 36.5 35.9   \n",
      "8                   ColBERT (over BERTbase) 34.9 34.9   \n",
      "11            Method MRR@10 (Dev) MRR@10 (Local Eval)   \n",
      "12                             BM25 (official) 16.7 -   \n",
      "13                          BM25 (Anserini) 18.7 19.5   \n",
      "14                                doc2query 21.5 22.8   \n",
      "15                                      DeepCT 24.3 -   \n",
      "16                            docTTTTTquery 27.7 28.4   \n",
      "17                      ColBERTL2 (re-rank) 34.8 36.4   \n",
      "18                   ColBERTL2 (end-to-end) 36.0 36.7   \n",
      "32  subsumes the entire computation from gathering...   \n",
      "43  16% increase in MRR@10. As described in §1, th...   \n",
      "\n",
      "    SIGIR ’20, July 25–30, 2020, Virtual Event, China  \n",
      "0                 Re-ranking Latency (ms) FLOPs/query  \n",
      "1                                                 - -  \n",
      "2                                     3 592M (0.085×)  \n",
      "3                                       22 159B (23×)  \n",
      "4                                        28 78B (11×)  \n",
      "5                                10,700 97T (13,900×)  \n",
      "6                                10,700 97T (13,900×)  \n",
      "7                               32,900 340T (48,600×)  \n",
      "8                                          61 7B (1×)  \n",
      "11      Latency (ms) Recall@50 Recall@200 Recall@1000  \n",
      "12                                         - - - 81.4  \n",
      "13                                  62 59.2 73.8 85.7  \n",
      "14                                  85 64.4 77.9 89.1  \n",
      "15                     62 (est.) 69 [2] 82 [2] 91 [2]  \n",
      "16                                  87 75.6 86.9 94.7  \n",
      "17                                   - 75.3 80.5 81.4  \n",
      "18                                 458 82.9 92.3 96.8  \n",
      "32  While highly competitive in retrieval quality,...  \n",
      "43  Diving deeper into the quality–cost tradeoff b...  \n",
      "Table 8: Empty table\n",
      "Table 9: Empty table\n",
      "Table 10:\n",
      "   Session 1A: NeuIR and Semantic Matching SIGIR ’20, July 25–30, 2020, Virtual Event, China\n",
      "0   Shifting our attention to ColBERT’s end-to-end...                                       \n",
      "1   tiveness, we see its major gains in MRR@10 ove...                                       \n",
      "2   to-end models. In fact, using ColBERT in the e...                                       \n",
      "3                             +length-based bucketing                                       \n",
      "4   perior in terms of MRR@10 to re-ranking with t...                                       \n",
      "..                                                ...                                       \n",
      "62  NSF under CAREER grant CNS-1651570. Any opinio...                                       \n",
      "63  4.5 Indexing Throughput & Footprint and conclu...                                       \n",
      "64  Lastly, we examine the indexing throughput and...                                       \n",
      "65  of ColBERT. Figure 6 reports indexing throughp...                                       \n",
      "66                                                 47                                       \n",
      "\n",
      "[66 rows x 1 columns]\n",
      "Table 11: Empty table\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Got stderr: Jul 27, 2023 4:03:36 PM org.apache.pdfbox.pdmodel.font.PDSimpleFont toUnicode\r\n",
      "WARNING: No Unicode mapping for mapsto (55) in font XCALMJ+CMSY10\r\n",
      "Jul 27, 2023 4:03:36 PM org.apache.pdfbox.pdmodel.font.PDSimpleFont toUnicode\r\n",
      "WARNING: No Unicode mapping for prime (48) in font NSJEOL+CMSY7\r\n",
      "Jul 27, 2023 4:03:36 PM org.apache.pdfbox.pdmodel.font.PDSimpleFont toUnicode\r\n",
      "WARNING: No Unicode mapping for braceleftBig (110) in font EJMRHD+CMEX10\r\n",
      "Jul 27, 2023 4:03:36 PM org.apache.pdfbox.pdmodel.font.PDSimpleFont toUnicode\r\n",
      "WARNING: No Unicode mapping for parenleftbig (0) in font EJMRHD+CMEX10\r\n",
      "Jul 27, 2023 4:03:36 PM org.apache.pdfbox.pdmodel.font.PDSimpleFont toUnicode\r\n",
      "WARNING: No Unicode mapping for braceleftbig (8) in font EJMRHD+CMEX10\r\n",
      "Jul 27, 2023 4:03:36 PM org.apache.pdfbox.pdmodel.font.PDSimpleFont toUnicode\r\n",
      "WARNING: No Unicode mapping for bracerightbig (9) in font EJMRHD+CMEX10\r\n",
      "Jul 27, 2023 4:03:36 PM org.apache.pdfbox.pdmodel.font.PDSimpleFont toUnicode\r\n",
      "WARNING: No Unicode mapping for parenrightbig (1) in font EJMRHD+CMEX10\r\n",
      "Jul 27, 2023 4:03:36 PM org.apache.pdfbox.pdmodel.font.PDSimpleFont toUnicode\r\n",
      "WARNING: No Unicode mapping for bracerightBig (111) in font EJMRHD+CMEX10\r\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables extracted from Reference3.pdf:\n",
      "Table 1:\n",
      "   Unnamed: 0 arXiv:2301.03266v3  [cs.IR]  27 Feb 2023\n",
      "20          1                             Introduction\n",
      "Table 2:\n",
      "                                  2 Gospodinov et al.\n",
      "0   Original Passage: Barley (Hordeum vulgare L.),...\n",
      "1   member of the grass family, is a major cereal ...\n",
      "2   was one of the first cultivated grains and is ...\n",
      "3   widely. Barley grain is a staple in Tibetan cu...\n",
      "4   was eaten widely by peasants in Medieval Europ...\n",
      "5   ley has also been used as animal fodder, as a ...\n",
      "6   of fermentable material for beer and certain d...\n",
      "7   beverages, and as a component of various healt...\n",
      "8   Fig. 1. Example passage from MS MARCO and gene...\n",
      "9   Doc2Query model. The relevance of each query t...\n",
      "10  thors on a scale of 0–3 using the TREC Deep Le...\n",
      "11  Based on this observation, we hypothesise that...\n",
      "12  Doc2Query would improve if hallucinated querie...\n",
      "13  conduct experiments where we apply a new filte...\n",
      "14  poor queries prior to indexing. Given that thi...\n",
      "15  call the approach Doc2Query-- (Doc2Query-minus...\n",
      "16  a new model for this task, we identify that re...\n",
      "17  this purpose: they estimate how relevant a pas...\n",
      "18  explore filtering strategies that make use of ...\n",
      "19  Through experimentation on the MS MARCO datase...\n",
      "20  tering approach can improve the retrieval effe...\n",
      "21  Doc2Query-- by up to 16%; less can indeed be m...\n",
      "22  urally reduces the index size, lowering storag...\n",
      "23  costs. Finally, we conduct an exploration of t...\n",
      "24  by the filtering process and conclude that the...\n",
      "25  up for the additional time spent generating mo...\n",
      "26  a positive impact on the environmental costs o...\n",
      "27  retrieval effectiveness can be achieved with o...\n",
      "28  tational cost when indexing. To facilitate las...\n",
      "29  reproduction efforts [36], we release the code...\n",
      "30  summary, we contribute a technique to improve ...\n",
      "31  of Doc2Query by filtering out queries that do ...\n",
      "32                                     2 Related Work\n",
      "33  The classical lexical mismatch problem is a ke...\n",
      "34  documents that do not contain the query terms ...\n",
      "35  literature, various approaches have addressed ...\n",
      "36  ing stemming, query expansion models (e.g. Roc...\n",
      "37  document expansion [9,30,35]. Classically, que...\n",
      "38  popular, as they avoid the costs associated wi...\n",
      "39  for each document needed for document expansio...\n",
      "40  may result in reduced performance [11], as que...\n",
      "41  necessary evidence to understand the context o...\n",
      "Table 3:\n",
      "                     Doc2Query--: When Less is More 3\n",
      "0   The application of latent representations of q...\n",
      "1   as using latent semantic indexing [8] allow re...\n",
      "2   by lexical signals. More recently, transformer...\n",
      "3   BERT [6]) have resulted in representations of ...\n",
      "4   meaning of words are accounted for. In particu...\n",
      "5   and documents are represented in embeddings sp...\n",
      "6   by Approximate Nearest Neighbour (ANN) data st...\n",
      "7   when using ANN, retrieval can still be ineffic...\n",
      "8   Others have explored approaches for augmenting...\n",
      "9   additional terms that may be relevant. In this...\n",
      "10  which uses a sequence-to-sequence model that m...\n",
      "11  it might be able to answer. By appending these...\n",
      "12  ment’s content before indexing, the document i...\n",
      "13  user queries when using a model like BM25. An ...\n",
      "14  expansion, proposed by MacAvaney et al. [19] a...\n",
      "15  models (e.g., [10,39,40]), uses the built-in M...\n",
      "16  mechanism. MLM expansion generates individual ...\n",
      "17  ment as a bag of words (rather than as a seque...\n",
      "18  also prone to hallucination,2 the bag-of-words...\n",
      "19  that individual expansion tokens may not have ...\n",
      "20  tering effectively. We therefore focus only on...\n",
      "21  the exploration of MLM expansion for future work.\n",
      "22                                      3 Doc2Query--\n",
      "23  Doc2Query-- consists of two phases: a generati...\n",
      "24  In the generation phase, a Doc2Query model gen...\n",
      "25  each document might be able to answer. However...\n",
      "26  all of the queries are necessarily relevant to...\n",
      "27  problem, Doc2Query-- then proceeds to a filter...\n",
      "28  for eliminating the generated queries that are...\n",
      "29  ument. Because hallucinated queries contain de...\n",
      "30  text (by definition), we argue that hallucinat...\n",
      "31  trieval than non-hallucinated ones. Filtering ...\n",
      "32  the most relevant p proportion of generated qu...\n",
      "33  retained queries are then concatenated to thei...\n",
      "34  to indexing, as per the existing Doc2Query app...\n",
      "35  More formally, consider an expansion function ...\n",
      "36                                                  n\n",
      "37  queries: e : D → Q . In Doc2Query, each docume...\n",
      "38  nated with their expansion queries, forming a ...\n",
      "39  d ∈ D}, which is then indexed by a retrieval s...\n",
      "40  mechanism that uses a relevance model that map...\n",
      "41  real-valued relevance score s : Q×D→ R (with l...\n",
      "42  2 For instance, we find that SPLADE [10] gener...\n",
      "43  terms for the passage in Figure 1 in the top 2...\n",
      "Table 4:\n",
      "                    4 Gospodinov et al.  \\\n",
      "2                                     {   \n",
      "3                        D′ = Concat d,   \n",
      "37              3 ir-datasets [21] IDs:   \n",
      "38          msmarco-passage/eval/small,   \n",
      "40  msmarco-passage/trec-dl-2020/judged   \n",
      "41                remove stopwords were   \n",
      "42         5was also tuned for filtered   \n",
      "43                                    6   \n",
      "44        castorini/monot5-base-msmarco   \n",
      "\n",
      "                                           Unnamed: 0  \n",
      "2                                             }( { })  \n",
      "3              q | q ∈ e(d) ∧ s(q, d) ≥ t | d ∈ D (1)  \n",
      "37  msmarco-passage/dev/small, msmarco-passage/dev/2,  \n",
      "38               msmarco-passage/trec-dl-2019/judged,  \n",
      "40                       BM25’s k1, b, and whether to  \n",
      "41  tuned for all systems; the filtering percentag...  \n",
      "42         systems. crystina-z/monoELECTRA_LCE_nneg31  \n",
      "43                                                  7  \n",
      "44               castorini/tct_colbert-v2-hnp-msmarco  \n",
      "Table 5:\n",
      "                     Doc2Query--: When Less is More 5\n",
      "0   Table 1. Effectiveness and efficiency measurem...\n",
      "1   Significant differences between Doc2Query and ...\n",
      "2   for Dev, Dev2, DL’19 and DL’20 are indicated w...\n",
      "3   marked with † are taken from the corresponding...\n",
      "4                               RR@10 nDCG@10 ms/q GB\n",
      "5          System Dev Dev2 Eval DL’19 DL’20 MRT Index\n",
      "6          †BM25 0.185 0.182 0.186 0.499 0.479 5 0.71\n",
      "7   †Doc2Query (n = 40) 0.277 0.265 0.272 0.626 0....\n",
      "8   w/ ELECTRA Filter (30%) *0.316 *0.310 - 0.667 ...\n",
      "9   w/ MonoT5 Filter (40%) *0.308 *0.298 0.306 0.6...\n",
      "10  w/ TCT Filter (50%) *0.287 *0.280 - 0.640 0.59...\n",
      "11  Doc2Query (n = 80) 0.279 0.267 - 0.627 0.605 3...\n",
      "12  w/ ELECTRA Filter (30%) *0.323 *0.316 0.325 0....\n",
      "13  w/ MonoT5 Filter (40%) *0.311 *0.298 - 0.665 0...\n",
      "14  w/ TCT Filter (50%) *0.293 *0.283 - 0.642 0.58...\n",
      "15                                          5 Results\n",
      "16  We first explore RQ1: whether relevance filter...\n",
      "17  Doc2Query models. Table 1 compares the effecti...\n",
      "18  ious filters. We observe that all the filters ...\n",
      "19  effectiveness on the Dev and Dev2 datasets at ...\n",
      "20  observe a large boost in performance on the Ev...\n",
      "21  ences in DL’19 and DL’20 appear to be consider...\n",
      "22     differences are not statistically significant.\n",
      "23  Digging a little deeper, Figure 2 shows the re...\n",
      "24  with various numbers of generated queries (in ...\n",
      "25  ing performance when filtering using the top-p...\n",
      "26  solid blue). We observe that performing releva...\n",
      "27  improves the retrieval effectiveness. For inst...\n",
      "28  sion queries at n = 80, performance is increas...\n",
      "29                                       improvement.\n",
      "30  In aggregate, results from Table 1 and Figure ...\n",
      "31  filtering can significantly improve the retrie...\n",
      "32  various scoring models, numbers of generated q...\n",
      "33  Next, we explore the trade-offs in terms of ef...\n",
      "34  when using Doc2Query--. Table 1 includes the m...\n",
      "35  sizes for each of the settings. As expected, f...\n",
      "36  fewer terms are stored. For the best-performin...\n",
      "37  8 Significance cannot be determined due to the...\n",
      "38  due to restrictions on the number of submissio...\n",
      "39  to submit two runs. The first aims to be a fai...\n",
      "40  Eval result, using the same number of generate...\n",
      "41  scoring. The second is our overall best-perfor...\n",
      "42                       at n = 80 generated queries.\n",
      "Table 6: Empty table\n",
      "Table 7:\n",
      "                     Doc2Query--: When Less is More 7\n",
      "0   filtering, the filtered results consistently y...\n",
      "1   tiveness. As the computational budget increase...\n",
      "2   Doc2Query and Doc2Query--, from 4% at 34 hours...\n",
      "3   From the opposite perspective, Doc2Query consu...\n",
      "4   time than Doc2Query-- to achieve similar effec...\n",
      "5   vs. n = 5 with ELECTRA filter). Since the effe...\n",
      "6   out between n = 40 and n = 80 (as seen in Figu...\n",
      "7   massive amount of additional compute to reach ...\n",
      "8   at n ≥ 10, if that effectiveness is achievable...\n",
      "9   if a deployment is targeting a certain level o...\n",
      "10  compute budget), Doc2Query-- is also preferabl...\n",
      "11  These results collectively answer RQ2: Doc2Que...\n",
      "12  ness at lower query-time costs, even when cont...\n",
      "13                            required at index time.\n",
      "14                                      6 Conclusions\n",
      "15  This work demonstrated that there are untapped...\n",
      "16  language for document expansion. Specifically,...\n",
      "17  is a new approach for improving the effectiven...\n",
      "18  model by filtering out the least relevant quer...\n",
      "19  provement in retrieval effectiveness can be ac...\n",
      "20  size by 33% and mean query execution time by 23%.\n",
      "21  The technique of filtering text generated from...\n",
      "22  evance scoring is ripe for future work. For in...\n",
      "23  potentially apply to approaches that generate ...\n",
      "24  training data [2], or natural language respons...\n",
      "25  are potentially affected by hallucinated conte...\n",
      "26  explore approaches for relevance filtering ove...\n",
      "27  pansion [19], rather than sequence-to-sequence...\n",
      "28                                   Acknowledgements\n",
      "29  Sean MacAvaney and Craig Macdonald acknowledge...\n",
      "30  Closed-Loop Data Science for Complex, Computat...\n",
      "31                                           alytics.\n",
      "32                                         References\n",
      "33  1. Amati, G., Van Rijsbergen, C.J.: Probabilis...\n",
      "34  based on measuring the divergence from randomn...\n",
      "35                                             (2002)\n",
      "36  2. Bonifacio, L., Abonizio, H., Fadaee, M., No...\n",
      "37  generation for information retrieval. In: Proc...\n",
      "Table 8:\n",
      "                                  8 Gospodinov et al.\n",
      "0   3. Dai, Z., Callan, J.: Deeper text understand...\n",
      "1     guage modeling. In: Proceedings of SIGIR (2019)\n",
      "2   4. Dai, Z., Callan, J.: Context-aware document...\n",
      "3            Proceedings of The Web Conference (2020)\n",
      "4   5. Das, R., Dhuliawala, S., Zaheer, M., McCall...\n",
      "5   interaction for scalable open-domain question ...\n",
      "6                                              (2019)\n",
      "7   6. Devlin, J., Chang, M.W., Lee, K., Toutanova...\n",
      "8   bidirectional transformers for language unders...\n",
      "9                                          HLT (2019)\n",
      "10  7. Ding, S., Suel, T.: Faster top-k document r...\n",
      "11                        Proceedings of SIGIR (2011)\n",
      "12  8. Dumais, S.T., Furnas, G.W., Landauer, T.K.,...\n",
      "13  latent semantic analysis to improve access to ...\n",
      "14                               of SIGCHI CHI (1988)\n",
      "15  9. Efron, M., Organisciak, P., Fenlon, K.: Imp...\n",
      "16  document expansion. In: Proceedings of SIGIR (...\n",
      "17  10. Formal, T., Piwowarski, B., Clinchant, S.:...\n",
      "18  model for first stage ranking. In: Proceedings...\n",
      "19  11. He, B., Ounis, I.: Studying query expansio...\n",
      "20                                             (2009)\n",
      "21  12. Jaleel, N.A., Allan, J., Croft, W.B., Diaz...\n",
      "22  Wade, C.: Umass at TREC 2004: Novelty and HARD...\n",
      "23  13. Johnson, J., Douze, M., Jegou, H.: Billion...\n",
      "24              Transactions on Big Data 7(03) (2021)\n",
      "25  14. Khattab, O., Zaharia, M.: ColBERT: Efficie...\n",
      "26  contextualized late interaction over BERT. In:...\n",
      "27  15. Lin, J., Ma, X., Mackenzie, J., Mallia, A....\n",
      "28  ranking models for text retrieval applications...\n",
      "29  16. Lin, S.C., Yang, J.H., Lin, J.: In-batch n...\n",
      "30  tightly-coupled teachers for dense retrieval. ...\n",
      "31  17. MacAvaney, S., Macdonald, C.: A Python int...\n",
      "32                                       SIGIR (2022)\n",
      "33  18. MacAvaney, S., Macdonald, C., Ounis, I.: S...\n",
      "34                     In: Proceedings of ECIR (2022)\n",
      "35  19. MacAvaney, S., Nardini, F.M., Perego, R., ...\n",
      "36  O.: Expansion via prediction of importance wit...\n",
      "37                                    of SIGIR (2020)\n",
      "38  20. MacAvaney, S., Tonellotto, N., Macdonald, ...\n",
      "39              graph. In: Proceedings of CIKM (2022)\n",
      "40  21. MacAvaney, S., Yates, A., Feldman, S., Dow...\n",
      "41  Simplified data wrangling with ir_datasets. In...\n",
      "42  22. Macdonald, C., Tonellotto, N.: Declarative...\n",
      "43  trieval using PyTerrier. In: Proceedings of IC...\n",
      "44  23. Mallia, A., Khattab, O., Suel, T., Tonello...\n",
      "45  inverted indexes. In: Proceedings of SIGIR (2021)\n",
      "46  24. Mallia, A., Siedlaczek, M., Mackenzie, J.,...\n",
      "47  search for academia. In: Proceedings of OSIRRC...\n",
      "48  25. Maynez, J., Narayan, S., Bohnet, B., McDon...\n",
      "49  in abstractive summarization. In: Proceedings ...\n",
      "Table 9:\n",
      "                     Doc2Query--: When Less is More 9\n",
      "0   26. Nguyen, T., Rosenberg, M., Song, X., Gao, ...\n",
      "1   L.: MS MARCO: A human generated machine readin...\n",
      "2                     Proceedings of CoCo@NIPS (2016)\n",
      "3   27. Nogueira, R., Cho, K.: Passage re-ranking ...\n",
      "4                                              (2019)\n",
      "5   28. Nogueira, R., Lin, J.: From doc2query to d...\n",
      "6   29. Nogueira, R., Yang, W., Lin, J.J., Cho, K....\n",
      "7                   tion. ArXiv abs/1904.08375 (2019)\n",
      "8   30. Pickens, J., Cooper, M., Golovchinsky, G.:...\n",
      "9           expansion. In: Proceedings of CIKM (2010)\n",
      "10  31. Pradeep, R., Liu, Y., Zhang, X., Li, Y., Y...\n",
      "11  stone: A bag of tricks for further improving c...\n",
      "12                ing. In: Proceedings of ECIR (2022)\n",
      "13  32. Pradeep, R., Nogueira, R., Lin, J.: The ex...\n",
      "14  ranking with pretrained sequence-to-sequence m...\n",
      "15                                             (2021)\n",
      "16  33. Raffel, C., Shazeer, N., Roberts, A., Lee,...\n",
      "17  Li, W., Liu, P.J., et al.: Exploring the limit...\n",
      "18  text-to-text transformer. J. Mach. Learn. Res....\n",
      "19  34. Scells, H., Zhuang, S., Zuccon, G.: Reduce...\n",
      "20  trieval research. In: Proceedings of SIGIR (2022)\n",
      "21  35. Tao, T., Wang, X., Mei, Q., Zhai, C.: Lang...\n",
      "22  document expansion. In: Proceedings of HLT-NAA...\n",
      "23  36. Wang, X., MacAvaney, S., Macdonald, C., Ou...\n",
      "24  ducibility and replicability of TCT-ColBERT. I...\n",
      "25  37. Xiong, L., Xiong, C., Li, Y., Tang, K.F., ...\n",
      "26  wijk, A.: Approximate nearest neighbor negativ...\n",
      "27          retrieval. In: Proceedings of ICLR (2021)\n",
      "28  38. Yu, S.Y., Liu, J., Yang, J., Xiong, C., Be...\n",
      "29  generative conversational query rewriting. In:...\n",
      "30  39. Zhao, T., Lu, X., Lee, K.: SPARTA: Efficie...\n",
      "31  sparse transformer matching retrieval. arXiv a...\n",
      "32  40. Zhuang, S., Zuccon, G.: TILDE: Term indepe...\n",
      "33        re-ranking. In: Proceedings of SIGIR (2021)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Got stderr: Jul 27, 2023 4:03:37 PM org.apache.pdfbox.pdmodel.font.PDSimpleFont toUnicode\r\n",
      "WARNING: No Unicode mapping for prime (48) in font LAVCNC+CMSY8\r\n",
      "Jul 27, 2023 4:03:37 PM org.apache.pdfbox.pdmodel.font.PDSimpleFont toUnicode\r\n",
      "WARNING: No Unicode mapping for summationtext (80) in font LBWBDW+CMEX10\r\n",
      "Jul 27, 2023 4:03:37 PM org.apache.pdfbox.pdmodel.font.PDSimpleFont toUnicode\r\n",
      "WARNING: No Unicode mapping for a23 (55) in font YCQRWY+Dingbats\r\n",
      "Jul 27, 2023 4:03:37 PM org.apache.pdfbox.pdmodel.font.PDSimpleFont toUnicode\r\n",
      "WARNING: No Unicode mapping for a19 (51) in font YCQRWY+Dingbats\r\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables extracted from Reference4.pdf:\n",
      "Table 1:\n",
      "   Unnamed: 0                               Unnamed: 1  \\\n",
      "35       sued                                   query.   \n",
      "39       such                                       as   \n",
      "40    trieval   (Xiong et al., 2021; Qu et al., 2021).   \n",
      "46        and                                   Croft,   \n",
      "51     ments.    However, query expansion methods like   \n",
      "52        RM3  (Lavrenko and Croft, 2001; Lv and Zhai,   \n",
      "\n",
      "             arXiv:2303.07678v1  [cs.IR]  14 Mar 2023  \n",
      "35  It is a core component in modern information r...  \n",
      "39  BM25, and embedding-based dense re- TREC DL 20...  \n",
      "40    Al- documents are generated by prompting an im-  \n",
      "46  2001) is a long-standing technique track. Stro...  \n",
      "51  encoder based re-ranker. Experiments in zero-shot  \n",
      "52      OOD settings demonstrate that our method out-  \n",
      "Table 2:\n",
      "   performs strong baselines on most datasets. Fur-     Unnamed: 0 Unnamed: 1  \\\n",
      "38  to generate a pseudo-document d′ as depicted in  a coefficient         to   \n",
      "41      and k labeled pairs randomly sampled from a     Comparison       with   \n",
      "\n",
      "                  Unnamed: 2 Unnamed: 3  \n",
      "38  balance the distillation   loss and  \n",
      "41          Pseudo-relevance   Feedback  \n",
      "Table 3:\n",
      "                                 Unnamed: 0         Unnamed: 1  \\\n",
      "3                                      BM25                  7   \n",
      "4                               + query2doc                  7   \n",
      "5                                BM25 + RM3                  7   \n",
      "6             docT5query (Nogueira and Lin)                  3   \n",
      "8                 ANCE (Xiong et al., 2021)                  3   \n",
      "9                   HyDE (Gao et al., 2022)                  7   \n",
      "10                 DPRbert-base (our impl.)                  3   \n",
      "11                              + query2doc                  3   \n",
      "13            RocketQAv2 (Ren et al., 2021)                  3   \n",
      "14                 AR2 (Zhang et al., 2021)                  3   \n",
      "15               SimLM (Wang et al., 2022a)                  3   \n",
      "16                              + query2doc                  3   \n",
      "17         E5base + KD (Wang et al., 2022b)                  3   \n",
      "18                              + query2doc                  3   \n",
      "25  we utilize the MS-MARCO passage ranking              (Cam-   \n",
      "26          pos et al., 2016), TREC DL 2019  (Craswell et al.,   \n",
      "\n",
      "                  MS MARCO dev TREC DL 19 TREC DL 20  \n",
      "3                         18.4 58.5 85.7 51.2∗ 47.7∗  \n",
      "4     21.4+3.0 65.3+6.8 91.8+6.1 66.2+15.0 62.9+15.2  \n",
      "5                           15.8 56.7 86.4 52.2 47.4  \n",
      "6                              27.7 75.6 94.7 64.2 -  \n",
      "8                              33.0 - 95.9 64.5 64.6  \n",
      "9                                    - - - 61.3 57.9  \n",
      "10                          33.7 80.5 95.9 64.7 64.1  \n",
      "11      35.1+1.4 82.6+2.1 97.2+1.3 68.7+4.0 67.1+3.0  \n",
      "13                                38.8 86.2 98.1 - -  \n",
      "14                                39.5 87.8 98.6 - -  \n",
      "15                          41.1 87.8 98.7 71.4 69.7  \n",
      "16      41.5+0.4 88.0+0.2 98.8+0.1 72.9+1.5 71.6+1.9  \n",
      "17                          40.7 87.6 98.6 74.3 70.7  \n",
      "18      41.5+0.8 88.1+0.5 98.7+0.1 74.9+0.6 72.5+1.8  \n",
      "25    ment expansion method docT5query achieves bet-  \n",
      "26  ter numbers on the MS-MARCO dev set, it requires  \n",
      "Table 4: Empty table\n",
      "Table 5: Empty table\n",
      "Table 6:\n",
      "               query who killed nicholas ii of russia\n",
      "0   LLM generation Nicholas II of Russia, the last...\n",
      "1   July 17th, 1918, along with his wife Alexandra...\n",
      "2   ordered by Vladimir Lenin, the leader of the B...\n",
      "3   out by a firing squad of Bolshevik troops, and...\n",
      "4   Groundtruth (1868-1918). Nicholas II was the l...\n",
      "..                                                ...\n",
      "65  Large Language Models (LLMs) such as GPT-3 que...\n",
      "66  (Brown et al., 2020), PaLM (Chowdhery et al., ...\n",
      "67  2022), and LLaMA (Touvron et al., 2023) are pr...\n",
      "68  trained on trillions of tokens with billions o...\n",
      "69  eters, exhibiting unparalleled generalization ...\n",
      "\n",
      "[70 rows x 1 columns]\n",
      "Table 7:\n",
      "                                   Limitations           Unnamed: 0  \\\n",
      "3                                       BM25 -                 16ms   \n",
      "4                          + query2doc >2000ms                177ms   \n",
      "27               benchmarking results in Table    6. Real-world de-   \n",
      "40             Henning Wachsmuth, et al. 2022.             Overview   \n",
      "46          tion Forum for European Languages,                pages   \n",
      "50           Stefan Riezler. 2016. A full-text          learning to   \n",
      "51  dataset for medical information retrieval.             In Euro-   \n",
      "52              pean Conference on Information     Retrieval, pages   \n",
      "56             Subbiah, Jared Kaplan, Prafulla     Dhariwal, Arvind   \n",
      "58                   Askell, Sandhini Agarwal,  Ariel Herbert-Voss,   \n",
      "59             Gretchen Krueger, Tom Henighan,         Rewon Child,   \n",
      "60           Aditya Ramesh, Daniel M. Ziegler,              Jeffrey   \n",
      "61                 Clemens Winter, Christopher    Hesse, Mark Chen,   \n",
      "63              Chess, Jack Clark, Christopher          Berner, Sam   \n",
      "68               tion Processing Systems 2020,        NeurIPS 2020,   \n",
      "72             Xia Song, Jianfeng Gao, Saurabh       Tiwary, Rangan   \n",
      "\n",
      "       Majumder, Li Deng, and Bhaskar Mitra. 2016. Ms  \n",
      "3   Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,  \n",
      "4         Maarten Bosma, Gaurav Mishra, Adam Roberts,  \n",
      "27    Jason Wei, Kathleen S. Meier-Hellstern, Douglas  \n",
      "40                                                 of  \n",
      "46                                               311–  \n",
      "50  rank sion model for first stage ranking. Proce...  \n",
      "51     the 44th International ACM SIGIR Conference on  \n",
      "52  Research and Development in Information Retrie...  \n",
      "56   ings of the 2021 Conference on Empirical Methods  \n",
      "58   Online and Punta Cana, Dominican Republic. Asso-  \n",
      "59             ciation for Computational Linguistics.  \n",
      "60                                                Wu,  \n",
      "61        Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie  \n",
      "63   Mc- out relevance labels. ArXiv, abs/2212.10496.  \n",
      "68  De- A test collection for entity search. In Pr...  \n",
      "72                                    1265–1268. ACM.  \n",
      "Table 8:\n",
      "   Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and\n",
      "0   Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, a...                                                    \n",
      "1   Wen-tau Yih. 2020. Dense passage retrieval for...                                                    \n",
      "2   open-domain question answering. In Proceedings of                                                    \n",
      "4   ural Language Processing (EMNLP), pages 6769– ...                                                    \n",
      "5   6781, Online. Association for Computational Li...                                                    \n",
      "..                                                ...                                                    \n",
      "68  cal Methods in Natural Language Processing, pa...                                                    \n",
      "69    2825–2835, Online and Punta Cana, Dominican Re-                                                    \n",
      "70  public. Association for Computational Linguist...                                                    \n",
      "72  J. J. Rocchio. 1971. Relevance feedback in inf...                                                    \n",
      "73               retrieval. preprint, abs/2110.03611.                                                    \n",
      "\n",
      "[69 rows x 1 columns]\n",
      "Table 9: Empty table\n",
      "Table 10:\n",
      "   Unnamed: 0      Write a passage that answers the given query:\n",
      "28     output  games and served as the basis for Pokemon Red ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Got stderr: Jul 27, 2023 4:03:38 PM org.apache.pdfbox.pdmodel.font.PDSimpleFont toUnicode\r\n",
      "WARNING: No Unicode mapping for summationdisplay (88) in font IXNPPI+CMEX10\r\n",
      "Jul 27, 2023 4:03:38 PM org.apache.pdfbox.pdmodel.font.PDSimpleFont toUnicode\r\n",
      "WARNING: No Unicode mapping for intercal (124) in font EXPVTB+MSAM10\r\n",
      "Jul 27, 2023 4:03:38 PM org.apache.pdfbox.pdmodel.font.PDSimpleFont toUnicode\r\n",
      "WARNING: No Unicode mapping for summationtext (80) in font IXNPPI+CMEX10\r\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables extracted from Reference5.pdf:\n",
      "Table 1:\n",
      "       Unnamed: 0           arXiv:2104.07186v1  [cs.IR]  15 Apr 2021\n",
      "32              1  Introduction to handle semantic mismatch, whic...\n",
      "37  lexical match  2 between query and document terms.cal matchin...\n",
      "51            ent      terms, e.g. cat v.s. kitty, for the same con-\n",
      "61         match,          that the same term can refer to different\n",
      "Table 2: Empty table\n",
      "Table 3: Empty table\n",
      "Table 4: Empty table\n",
      "Table 5: Empty table\n",
      "Table 6:\n",
      "                MRR@1K and NDCG@10 on test and MRR@10  \\\n",
      "0       on MSMARCO Dev. We also report recall for the   \n",
      "1   dev queries following prior work (Dai and Callan,   \n",
      "2                     2019a; Nogueira and Lin, 2019).   \n",
      "3        Compared Systems Baselines include 1) tradi-   \n",
      "16         with BM25 negatives or with mixed BM25 and   \n",
      "17  random negatives, published in Xiong et al. (2...   \n",
      "18   However since these systems use a robust version   \n",
      "19      of BERT, RoBERTa (Liu et al., 2019) as the LM   \n",
      "20       and train document retriever also on MSMARCO   \n",
      "21  passage set, we in addition reproduce a third ...   \n",
      "22  retriever, that uses the exact same training s...   \n",
      "23   COIL. All dense retrievers use 768 dimension em-   \n",
      "24  bedding. For ColBERT, we report its published re-   \n",
      "25  sults (available only on passage collection). ...   \n",
      "26              reranker is added in the rerank task.   \n",
      "34        Implementation We build our models with Py-   \n",
      "35   torch (Paszke et al., 2019) based on huggingface   \n",
      "36     transformers (Wolf et al., 2019). COIL’s LM is   \n",
      "37     based on BERT’s base variant. COIL systems use   \n",
      "38      token dimension nt = 32 and COIL-full use CLS   \n",
      "39     dimension nc = 768 as default, leading to 110M   \n",
      "40    parameters. We add a Layer Normalization to CLS   \n",
      "41   vector when useful. All models are trained for 5   \n",
      "42    epochs with AdamW optimizer, a learning rate of   \n",
      "43  3e-6, 0.1 warm-up ratio, and linear learning rate   \n",
      "44      decay, which takes around 12 hours. Hard neg-   \n",
      "45     atives are sampled from top 1000 BM25 results.   \n",
      "46   Each query uses 1 positive and 7 hard negatives;   \n",
      "47       each batch uses 8 queries on MSMARCO passage   \n",
      "48           and 4 on MSMARCO document. Documents are   \n",
      "49  truncated to the first 512 tokens to fit in BERT.   \n",
      "50     We conduct validation on randomly selected 512   \n",
      "51  queries from corresponding train set. Latency ...   \n",
      "52      bers are measured on dual Xeon E5-2630 v3 for   \n",
      "53          CPU and RTX 2080 ti for GPU. We implement   \n",
      "55  COIL’s inverted lists as matrices as described in   \n",
      "56  subsection 3.3, using NumPy (Harris et al., 2020)   \n",
      "57     on CPU and Pytorch on GPU. We perform a) a set   \n",
      "58   of matrix products to compute token similarities   \n",
      "\n",
      "   over contextualized inverted lists, b) scatter to map  \n",
      "0   token scores back to documents, and c) sort to...     \n",
      "1     the documents. Illustration can be found in the     \n",
      "2                                 appendix, Figure 5.     \n",
      "3                                           5 Results     \n",
      "16    Table 1 reports various systems’ performance on     \n",
      "17         the MARCO passage collection. COIL-tok ex-     \n",
      "18   act lexical match only system significantly out-     \n",
      "19   performs all previous lexical retrieval systems.     \n",
      "20    With contextualized term similarities, COIL-tok     \n",
      "21      achieves a MRR of 0.34 compared to BM25’s MRR     \n",
      "22        0.18. DeepCT and DocT5Query, which also use     \n",
      "23   deep LMs like BERT and T5, are able to break the     \n",
      "24  limit of heuristic term frequencies but are st...     \n",
      "25     ited by semantic mismatch issues. We see COIL-     \n",
      "26    tok outperforms both systems by a large margin.     \n",
      "34  level interaction can improve precision. With the     \n",
      "35    CLS matching added, COIL-full gains the ability     \n",
      "36     to handle mismatched vocabulary and enjoys an-     \n",
      "37    other performance leap, outperforming all dense     \n",
      "38                                        retrievers.     \n",
      "39       COIL-full achieves a very narrow performance     \n",
      "40       gap to ColBERT. Recall that ColBERT computes     \n",
      "41  all-to-all soft matches between all token pair...     \n",
      "42  retrieval, it needs to consider for each query...     \n",
      "43  all mentions of all tokens in the collection (MS-     \n",
      "44       MARCO passage collection has around 500M to-     \n",
      "45  ken mentions). COIL-full is able to capture ma...     \n",
      "46  ing patterns as effectively with exact match s...     \n",
      "47  from only query tokens’ mentions and a single CLS     \n",
      "48             matching to bridge the vocabulary gap.     \n",
      "49   We observe a similar pattern in the rerank task.     \n",
      "50   COIL-tok is already able to outperform dense re-     \n",
      "51   triever and COIL-full further adds up to perfor-     \n",
      "52    mance with CLS matching, being on-par with Col-     \n",
      "53      BERT. Meanwhile, previous BERT rerankers have     \n",
      "55  little performance advantage over COIL . In prac-     \n",
      "56      tice, we found BERT rerankers to be much more     \n",
      "57  5Close performance between COIL and BERT reran...     \n",
      "58  is partially due to the bottleneck of BM25 can...     \n",
      "Table 7:\n",
      "  Dev Rerank\\rMRR@10          Dev Retrieval\\rMRR@10Recall@1K\n",
      "0     –\\r–\\r–\\r0.347  0.1840.853\\r0.2430.909\\r0.2780.945\\r––\n",
      "1  n.a.\\rn.a.\\r0.312      0.2990.928\\r0.3110.952\\r0.3040.932\n",
      "2              0.349                              0.3600.968\n",
      "3       0.336\\r0.348                  0.3410.949\\r0.3550.963\n",
      "Table 8:\n",
      "  Dev Rerank\\rMRR@10          Dev Retrieval\\rMRR@10Recall@1K\n",
      "0     –\\r–\\r–\\r0.383  0.2300.886\\r0.3200.942\\r0.2880.926\\r––\n",
      "1  n.a.\\rn.a.\\r0.358      0.2990.928\\r0.3110.952\\r0.3400.883\n",
      "2       0.381\\r0.388                  0.3850.952\\r0.3970.962\n",
      "Table 9:\n",
      "                      Dev Retrieval\\rMRR@10Recall@1K  \\\n",
      "0                 0.1840.853\\r0.3040.932\\r0.3600.968   \n",
      "1  0.3550.963\\r0.3500.953\\r0.3470.956\\r0.3410.949...   \n",
      "\n",
      "                        DL2019 Retrieval\\rNDCG@10MRR  \n",
      "0                   0.5060.825\\r0.6350.898\\rn.a.n.a.  \n",
      "1  0.7040.924\\r0.6920.956\\r0.6940.977\\r0.6600.915...  \n",
      "Table 10:\n",
      "               COIL Contextualized Exact Match Score\n",
      "0  Cabinet[16.28]\\r(government)Acabinet[16.75]\\ri...\n",
      "1  Cabinet [7.23] is 20x60 and top is 28x72. .......\n",
      "2  Priority Pass [11.61] is an independent airpor...\n",
      "3  Snoqualmie Pass [7.98] is a mountain pass [6.8...\n",
      "4  NJSTART is [1.25] a self-service online platfo...\n",
      "5  Contract awardees will receive their Blanket P...\n",
      "Table 11: Empty table\n",
      "Table 12:\n",
      "   References ECIR 2021, Virtual Event, March 28 - April 1, 2021,\n",
      "0                                Proceedings, Part I.            \n",
      "1   S. Blackford, J. Demmel, J. Dongarra, I. Duff,...            \n",
      "2     marling, Greg Henry, M. Héroux, L. Kaufman, An-            \n",
      "3   J. Guo, Y. Fan, Qingyao Ai, and W. Croft. 2016. A            \n",
      "4   drew Lumsdaine, A. Petitet, R. Pozo, K. Reming...            \n",
      "..                                                ...            \n",
      "68  Luyu Gao, Zhuyun Dai, Tongfei Chen, Zhen Fan, ...            \n",
      "69  jamin Van Durme, and Jamie Callan. 2021b. Com-...            \n",
      "70  plement lexical retrieval model with semantic ...            \n",
      "71  ual embeddings. In Advances in Information Re-...            \n",
      "72  trieval - 43rd European Conference on IR Resea...            \n",
      "\n",
      "[72 rows x 1 columns]\n",
      "Table 13:\n",
      "   Victor Lavrenko and W. Bruce Croft. 2001. Relevance-  \\\n",
      "0   based language models. In Proceedings of the 24th     \n",
      "3   Y. Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar     \n",
      "12                                    abs/2005.00181.     \n",
      "14  Sean MacAvaney, F. Nardini, R. Perego, N. Tone...     \n",
      "34                                              NIPS.     \n",
      "\n",
      "   model for probabilistic weighted retrieval. In Pro- Unnamed: 0  \n",
      "0           ceedings of the 17th Annual International        ACM-  \n",
      "3          Ashish Vaswani, Noam Shazeer, Niki Parmar,       Jakob  \n",
      "12       Clara Ma, Yacine Jernite, Julien Plu, Canwen         Xu,  \n",
      "14             Quentin Lhoest, and Alexander M. Rush.       2019.  \n",
      "34            R. Salakhutdinov, and Quoc V. Le. 2019.      Xlnet:  \n",
      "Table 14: Empty table\n",
      "Table 15: Empty table\n",
      "Table 16: Empty table\n",
      "Table 17: Empty table\n",
      "Table 18: Empty table\n",
      "Table 19: Empty table\n",
      "Table 20: Empty table\n",
      "Table 21: Empty table\n",
      "Table 22: Empty table\n",
      "Table 23: Empty table\n",
      "Table 24: Empty table\n",
      "Table 25: Empty table\n",
      "Table 26: Empty table\n",
      "Table 27: Empty table\n",
      "Table 28: Empty table\n",
      "Table 29: Empty table\n",
      "Table 30: Empty table\n",
      "Table 31: Empty table\n",
      "Table 32: Empty table\n",
      "Table 33: Empty table\n",
      "Table 34: Empty table\n",
      "Table 35: Empty table\n",
      "Table 36: Empty table\n",
      "Table 37: Empty table\n"
     ]
    }
   ],
   "source": [
    "############## 6\n",
    "import os\n",
    "import tabula\n",
    "import pandas as pd\n",
    "\n",
    "def extract_tables_from_pdf(pdf_path):\n",
    "    tables = tabula.read_pdf(pdf_path, pages=\"all\", multiple_tables=True, area=(0, 0, 1000, 1000))\n",
    "    return tables\n",
    "\n",
    "def clean_table(table):\n",
    "    # Remove rows with all NaN values\n",
    "    cleaned_table = table.dropna(how=\"all\")\n",
    "    # Remove rows with more than 50% NaN values\n",
    "    cleaned_table = cleaned_table.dropna(thresh=cleaned_table.shape[1]*0.5)\n",
    "    # Remove rows containing \"ranking: NaN\" or similar patterns\n",
    "    cleaned_table = cleaned_table[~cleaned_table.astype(str).apply(lambda x: x.str.contains(r'ranking:|NaN|nan', case=False)).any(axis=1)]\n",
    "    return cleaned_table\n",
    "\n",
    "# Folder path containing the PDF files\n",
    "pdf_folder = \"pdf_files_folder\"\n",
    "\n",
    "# List of PDF files to extract tables from\n",
    "pdf_files = [\"Reference1.pdf\", \"Reference2.pdf\", \"Reference3.pdf\", \"Reference4.pdf\", \"Reference5.pdf\"]\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    pdf_path = os.path.join(pdf_folder, pdf_file)\n",
    "    tables = extract_tables_from_pdf(pdf_path)\n",
    "    if not tables:\n",
    "        print(f\"No tables found in {pdf_file}\")\n",
    "    else:\n",
    "        print(f\"Tables extracted from {pdf_file}:\")\n",
    "        for i, table in enumerate(tables, 1):\n",
    "            cleaned_table = clean_table(table)\n",
    "            if not cleaned_table.empty:\n",
    "                print(f\"Table {i}:\")\n",
    "                print(cleaned_table)\n",
    "            else:\n",
    "                print(f\"Table {i}: Empty table\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ec79d3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: camelot-py[cv] in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: chardet>=3.0.4 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from camelot-py[cv]) (4.0.0)\n",
      "Requirement already satisfied: click>=6.7 in c:\\users\\student.ms-02\\appdata\\roaming\\python\\python311\\site-packages (from camelot-py[cv]) (8.1.6)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from camelot-py[cv]) (1.24.3)\n",
      "Requirement already satisfied: openpyxl>=2.5.8 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from camelot-py[cv]) (3.0.10)\n",
      "Requirement already satisfied: pandas>=0.23.4 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from camelot-py[cv]) (1.5.3)\n",
      "Requirement already satisfied: pdfminer.six>=20200726 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from camelot-py[cv]) (20221105)\n",
      "Requirement already satisfied: PyPDF2>=1.26.0 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from camelot-py[cv]) (3.0.1)\n",
      "Requirement already satisfied: opencv-python>=3.4.2.17 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from camelot-py[cv]) (4.8.0.74)\n",
      "Requirement already satisfied: colorama in c:\\users\\student.ms-02\\appdata\\roaming\\python\\python311\\site-packages (from click>=6.7->camelot-py[cv]) (0.4.6)\n",
      "Requirement already satisfied: et_xmlfile in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from openpyxl>=2.5.8->camelot-py[cv]) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from pandas>=0.23.4->camelot-py[cv]) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from pandas>=0.23.4->camelot-py[cv]) (2022.7)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from pdfminer.six>=20200726->camelot-py[cv]) (2.0.4)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from pdfminer.six>=20200726->camelot-py[cv]) (39.0.1)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six>=20200726->camelot-py[cv]) (1.15.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas>=0.23.4->camelot-py[cv]) (1.16.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six>=20200726->camelot-py[cv]) (2.21)\n"
     ]
    }
   ],
   "source": [
    "!pip install camelot-py[cv]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cbf35445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tabula-py in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: pandas>=0.25.3 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from tabula-py) (1.5.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from tabula-py) (1.24.3)\n",
      "Requirement already satisfied: distro in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from tabula-py) (1.8.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from pandas>=0.25.3->tabula-py) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from pandas>=0.25.3->tabula-py) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\student.ms-02\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas>=0.25.3->tabula-py) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tabula-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c3c03bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 1 from Reference1.pdf saved as output_folder\\Reference1.pdf_table_1.csv\n",
      "Table 2 from Reference1.pdf saved as output_folder\\Reference1.pdf_table_2.csv\n",
      "Table 3 from Reference1.pdf saved as output_folder\\Reference1.pdf_table_3.csv\n",
      "Table 4 from Reference1.pdf saved as output_folder\\Reference1.pdf_table_4.csv\n",
      "Table 5 from Reference1.pdf saved as output_folder\\Reference1.pdf_table_5.csv\n",
      "Table 6 from Reference1.pdf saved as output_folder\\Reference1.pdf_table_6.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Got stderr: Jul 27, 2023 4:04:09 PM org.apache.fontbox.ttf.CmapSubtable processSubtype14\r\n",
      "WARNING: Format 14 cmap table is not supported and will be ignored\r\n",
      "Jul 27, 2023 4:04:10 PM org.apache.fontbox.ttf.CmapSubtable processSubtype14\r\n",
      "WARNING: Format 14 cmap table is not supported and will be ignored\r\n",
      "Jul 27, 2023 4:04:10 PM org.apache.pdfbox.pdmodel.font.PDSimpleFont toUnicode\r\n",
      "WARNING: No Unicode mapping for summationdisplay.1 (213) in font CYBZPM+txexs\r\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 1 from Reference2.pdf saved as output_folder\\Reference2.pdf_table_1.csv\n",
      "Table 2 from Reference2.pdf saved as output_folder\\Reference2.pdf_table_2.csv\n",
      "Table 3 from Reference2.pdf saved as output_folder\\Reference2.pdf_table_3.csv\n",
      "Table 4 from Reference2.pdf saved as output_folder\\Reference2.pdf_table_4.csv\n",
      "Table 5 from Reference2.pdf saved as output_folder\\Reference2.pdf_table_5.csv\n",
      "Table 6 from Reference2.pdf saved as output_folder\\Reference2.pdf_table_6.csv\n",
      "Table 7 from Reference2.pdf saved as output_folder\\Reference2.pdf_table_7.csv\n",
      "Table 8 from Reference2.pdf saved as output_folder\\Reference2.pdf_table_8.csv\n",
      "Table 9 from Reference2.pdf saved as output_folder\\Reference2.pdf_table_9.csv\n",
      "Table 10 from Reference2.pdf saved as output_folder\\Reference2.pdf_table_10.csv\n",
      "Table 11 from Reference2.pdf saved as output_folder\\Reference2.pdf_table_11.csv\n",
      "Table 12 from Reference2.pdf saved as output_folder\\Reference2.pdf_table_12.csv\n",
      "Table 13 from Reference2.pdf saved as output_folder\\Reference2.pdf_table_13.csv\n",
      "Table 14 from Reference2.pdf saved as output_folder\\Reference2.pdf_table_14.csv\n",
      "Table 15 from Reference2.pdf saved as output_folder\\Reference2.pdf_table_15.csv\n",
      "Table 16 from Reference2.pdf saved as output_folder\\Reference2.pdf_table_16.csv\n",
      "Table 17 from Reference2.pdf saved as output_folder\\Reference2.pdf_table_17.csv\n",
      "Table 18 from Reference2.pdf saved as output_folder\\Reference2.pdf_table_18.csv\n",
      "Table 19 from Reference2.pdf saved as output_folder\\Reference2.pdf_table_19.csv\n",
      "Table 20 from Reference2.pdf saved as output_folder\\Reference2.pdf_table_20.csv\n",
      "Table 21 from Reference2.pdf saved as output_folder\\Reference2.pdf_table_21.csv\n",
      "Table 22 from Reference2.pdf saved as output_folder\\Reference2.pdf_table_22.csv\n",
      "Table 23 from Reference2.pdf saved as output_folder\\Reference2.pdf_table_23.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Got stderr: Jul 27, 2023 4:04:13 PM org.apache.pdfbox.pdmodel.font.PDSimpleFont toUnicode\r\n",
      "WARNING: No Unicode mapping for mapsto (55) in font XCALMJ+CMSY10\r\n",
      "Jul 27, 2023 4:04:13 PM org.apache.pdfbox.pdmodel.font.PDSimpleFont toUnicode\r\n",
      "WARNING: No Unicode mapping for prime (48) in font NSJEOL+CMSY7\r\n",
      "Jul 27, 2023 4:04:13 PM org.apache.pdfbox.pdmodel.font.PDSimpleFont toUnicode\r\n",
      "WARNING: No Unicode mapping for braceleftBig (110) in font EJMRHD+CMEX10\r\n",
      "Jul 27, 2023 4:04:13 PM org.apache.pdfbox.pdmodel.font.PDSimpleFont toUnicode\r\n",
      "WARNING: No Unicode mapping for parenleftbig (0) in font EJMRHD+CMEX10\r\n",
      "Jul 27, 2023 4:04:13 PM org.apache.pdfbox.pdmodel.font.PDSimpleFont toUnicode\r\n",
      "WARNING: No Unicode mapping for braceleftbig (8) in font EJMRHD+CMEX10\r\n",
      "Jul 27, 2023 4:04:13 PM org.apache.pdfbox.pdmodel.font.PDSimpleFont toUnicode\r\n",
      "WARNING: No Unicode mapping for bracerightbig (9) in font EJMRHD+CMEX10\r\n",
      "Jul 27, 2023 4:04:13 PM org.apache.pdfbox.pdmodel.font.PDSimpleFont toUnicode\r\n",
      "WARNING: No Unicode mapping for parenrightbig (1) in font EJMRHD+CMEX10\r\n",
      "Jul 27, 2023 4:04:13 PM org.apache.pdfbox.pdmodel.font.PDSimpleFont toUnicode\r\n",
      "WARNING: No Unicode mapping for bracerightBig (111) in font EJMRHD+CMEX10\r\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 1 from Reference3.pdf saved as output_folder\\Reference3.pdf_table_1.csv\n",
      "Table 2 from Reference3.pdf saved as output_folder\\Reference3.pdf_table_2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Got stderr: Jul 27, 2023 4:04:15 PM org.apache.pdfbox.pdmodel.font.PDSimpleFont toUnicode\r\n",
      "WARNING: No Unicode mapping for prime (48) in font LAVCNC+CMSY8\r\n",
      "Jul 27, 2023 4:04:15 PM org.apache.pdfbox.pdmodel.font.PDSimpleFont toUnicode\r\n",
      "WARNING: No Unicode mapping for summationtext (80) in font LBWBDW+CMEX10\r\n",
      "Jul 27, 2023 4:04:15 PM org.apache.pdfbox.pdmodel.font.PDSimpleFont toUnicode\r\n",
      "WARNING: No Unicode mapping for a23 (55) in font YCQRWY+Dingbats\r\n",
      "Jul 27, 2023 4:04:15 PM org.apache.pdfbox.pdmodel.font.PDSimpleFont toUnicode\r\n",
      "WARNING: No Unicode mapping for a19 (51) in font YCQRWY+Dingbats\r\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 1 from Reference4.pdf saved as output_folder\\Reference4.pdf_table_1.csv\n",
      "Table 2 from Reference4.pdf saved as output_folder\\Reference4.pdf_table_2.csv\n",
      "Table 3 from Reference4.pdf saved as output_folder\\Reference4.pdf_table_3.csv\n",
      "Table 4 from Reference4.pdf saved as output_folder\\Reference4.pdf_table_4.csv\n",
      "Table 5 from Reference4.pdf saved as output_folder\\Reference4.pdf_table_5.csv\n",
      "Table 6 from Reference4.pdf saved as output_folder\\Reference4.pdf_table_6.csv\n",
      "Table 7 from Reference4.pdf saved as output_folder\\Reference4.pdf_table_7.csv\n",
      "Table 8 from Reference4.pdf saved as output_folder\\Reference4.pdf_table_8.csv\n",
      "Table 9 from Reference4.pdf saved as output_folder\\Reference4.pdf_table_9.csv\n",
      "Table 10 from Reference4.pdf saved as output_folder\\Reference4.pdf_table_10.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Got stderr: Jul 27, 2023 4:04:18 PM org.apache.pdfbox.pdmodel.font.PDSimpleFont toUnicode\r\n",
      "WARNING: No Unicode mapping for summationdisplay (88) in font IXNPPI+CMEX10\r\n",
      "Jul 27, 2023 4:04:18 PM org.apache.pdfbox.pdmodel.font.PDSimpleFont toUnicode\r\n",
      "WARNING: No Unicode mapping for intercal (124) in font EXPVTB+MSAM10\r\n",
      "Jul 27, 2023 4:04:18 PM org.apache.pdfbox.pdmodel.font.PDSimpleFont toUnicode\r\n",
      "WARNING: No Unicode mapping for summationtext (80) in font IXNPPI+CMEX10\r\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 1 from Reference5.pdf saved as output_folder\\Reference5.pdf_table_1.csv\n",
      "Table 2 from Reference5.pdf saved as output_folder\\Reference5.pdf_table_2.csv\n",
      "Table 3 from Reference5.pdf saved as output_folder\\Reference5.pdf_table_3.csv\n",
      "Table 4 from Reference5.pdf saved as output_folder\\Reference5.pdf_table_4.csv\n",
      "Table 5 from Reference5.pdf saved as output_folder\\Reference5.pdf_table_5.csv\n",
      "Table 6 from Reference5.pdf saved as output_folder\\Reference5.pdf_table_6.csv\n",
      "Table 7 from Reference5.pdf saved as output_folder\\Reference5.pdf_table_7.csv\n",
      "Table 8 from Reference5.pdf saved as output_folder\\Reference5.pdf_table_8.csv\n",
      "Table 9 from Reference5.pdf saved as output_folder\\Reference5.pdf_table_9.csv\n",
      "Table 10 from Reference5.pdf saved as output_folder\\Reference5.pdf_table_10.csv\n",
      "Table 11 from Reference5.pdf saved as output_folder\\Reference5.pdf_table_11.csv\n",
      "Table 12 from Reference5.pdf saved as output_folder\\Reference5.pdf_table_12.csv\n",
      "Table 13 from Reference5.pdf saved as output_folder\\Reference5.pdf_table_13.csv\n",
      "Table 14 from Reference5.pdf saved as output_folder\\Reference5.pdf_table_14.csv\n",
      "Table 15 from Reference5.pdf saved as output_folder\\Reference5.pdf_table_15.csv\n",
      "Table 16 from Reference5.pdf saved as output_folder\\Reference5.pdf_table_16.csv\n",
      "Table 17 from Reference5.pdf saved as output_folder\\Reference5.pdf_table_17.csv\n",
      "Table 18 from Reference5.pdf saved as output_folder\\Reference5.pdf_table_18.csv\n",
      "Table 19 from Reference5.pdf saved as output_folder\\Reference5.pdf_table_19.csv\n",
      "Table 20 from Reference5.pdf saved as output_folder\\Reference5.pdf_table_20.csv\n",
      "Table 21 from Reference5.pdf saved as output_folder\\Reference5.pdf_table_21.csv\n",
      "Table 22 from Reference5.pdf saved as output_folder\\Reference5.pdf_table_22.csv\n",
      "Table 23 from Reference5.pdf saved as output_folder\\Reference5.pdf_table_23.csv\n",
      "Table 24 from Reference5.pdf saved as output_folder\\Reference5.pdf_table_24.csv\n",
      "Table 25 from Reference5.pdf saved as output_folder\\Reference5.pdf_table_25.csv\n",
      "Table 26 from Reference5.pdf saved as output_folder\\Reference5.pdf_table_26.csv\n",
      "Table 27 from Reference5.pdf saved as output_folder\\Reference5.pdf_table_27.csv\n",
      "Table 28 from Reference5.pdf saved as output_folder\\Reference5.pdf_table_28.csv\n",
      "Table 29 from Reference5.pdf saved as output_folder\\Reference5.pdf_table_29.csv\n",
      "Table 30 from Reference5.pdf saved as output_folder\\Reference5.pdf_table_30.csv\n",
      "Table 31 from Reference5.pdf saved as output_folder\\Reference5.pdf_table_31.csv\n",
      "Table 32 from Reference5.pdf saved as output_folder\\Reference5.pdf_table_32.csv\n",
      "Table 33 from Reference5.pdf saved as output_folder\\Reference5.pdf_table_33.csv\n",
      "Table 34 from Reference5.pdf saved as output_folder\\Reference5.pdf_table_34.csv\n",
      "Table 35 from Reference5.pdf saved as output_folder\\Reference5.pdf_table_35.csv\n",
      "Table 36 from Reference5.pdf saved as output_folder\\Reference5.pdf_table_36.csv\n",
      "Table 37 from Reference5.pdf saved as output_folder\\Reference5.pdf_table_37.csv\n"
     ]
    }
   ],
   "source": [
    "# to make this as a seperate table file \n",
    "import os\n",
    "import pandas as pd\n",
    "import tabula\n",
    "\n",
    "def extract_tables_from_pdf(pdf_path):\n",
    "    try:\n",
    "        tables = tabula.read_pdf(pdf_path, pages=\"all\", multiple_tables=True)\n",
    "        return tables\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting tables from {pdf_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Assuming you have the pdf_files_folder and the PDF files in it\n",
    "pdf_folder = \"pdf_files_folder\"\n",
    "pdf_files = os.listdir(pdf_folder)\n",
    "output_folder = \"output_folder\"  # Change this to your desired folder name\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    pdf_path = os.path.join(pdf_folder, pdf_file)\n",
    "\n",
    "    tables = extract_tables_from_pdf(pdf_path)\n",
    "\n",
    "    if not tables:\n",
    "        print(f\"No tables found in {pdf_file}\")\n",
    "    else:\n",
    "        # Save each table as a separate CSV file\n",
    "        for idx, df in enumerate(tables, 1):\n",
    "            table_filename = f\"{pdf_file}_table_{idx}.csv\"\n",
    "            table_filepath = os.path.join(output_folder, table_filename)\n",
    "            df.to_csv(table_filepath, index=False)\n",
    "            print(f\"Table {idx} from {pdf_file} saved as {table_filepath}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d061357a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install camelot-py[cv] pandas ipywidgets\n",
    "#jupyter notebook\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
